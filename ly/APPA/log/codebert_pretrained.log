nohup: ignoring input
训练集: 1515
测试集: 377
加载further pretrained模型成功
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 1 ==============
Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.698
[ 2 / 64 ] loss: 0.694
[ 3 / 64 ] loss: 0.694
[ 4 / 64 ] loss: 0.693
[ 5 / 64 ] loss: 0.679
[ 6 / 64 ] loss: 0.681
[ 7 / 64 ] loss: 0.679
[ 8 / 64 ] loss: 0.681
[ 9 / 64 ] loss: 0.666
[ 10 / 64 ] loss: 0.681
[ 11 / 64 ] loss: 0.662
[ 12 / 64 ] loss: 0.689
[ 13 / 64 ] loss: 0.681
[ 14 / 64 ] loss: 0.660
[ 15 / 64 ] loss: 0.641
[ 16 / 64 ] loss: 0.747
[ 17 / 64 ] loss: 0.648
[ 18 / 64 ] loss: 0.652
[ 19 / 64 ] loss: 0.613
[ 20 / 64 ] loss: 0.652
[ 21 / 64 ] loss: 0.543
[ 22 / 64 ] loss: 0.646
[ 23 / 64 ] loss: 0.567
[ 24 / 64 ] loss: 0.637
[ 25 / 64 ] loss: 0.604
[ 26 / 64 ] loss: 0.588
[ 27 / 64 ] loss: 0.544
[ 28 / 64 ] loss: 0.605
[ 29 / 64 ] loss: 0.547
[ 30 / 64 ] loss: 0.575
[ 31 / 64 ] loss: 0.565
[ 32 / 64 ] loss: 0.631
[ 33 / 64 ] loss: 0.623
[ 34 / 64 ] loss: 0.567
[ 35 / 64 ] loss: 0.625
[ 36 / 64 ] loss: 0.514
[ 37 / 64 ] loss: 0.677
[ 38 / 64 ] loss: 0.486
[ 39 / 64 ] loss: 0.525
[ 40 / 64 ] loss: 0.608
[ 41 / 64 ] loss: 0.639
[ 42 / 64 ] loss: 0.628
[ 43 / 64 ] loss: 0.618
[ 44 / 64 ] loss: 0.511
[ 45 / 64 ] loss: 0.690
[ 46 / 64 ] loss: 0.580
[ 47 / 64 ] loss: 0.682
[ 48 / 64 ] loss: 0.631
[ 49 / 64 ] loss: 0.520
[ 50 / 64 ] loss: 0.520
[ 51 / 64 ] loss: 0.650
[ 52 / 64 ] loss: 0.639
[ 53 / 64 ] loss: 0.614
[ 54 / 64 ] loss: 0.565
[ 55 / 64 ] loss: 0.585
[ 56 / 64 ] loss: 0.526
[ 57 / 64 ] loss: 0.563
[ 58 / 64 ] loss: 0.545
[ 59 / 64 ] loss: 0.494
[ 60 / 64 ] loss: 0.536
[ 61 / 64 ] loss: 0.726
[ 62 / 64 ] loss: 0.548
[ 63 / 64 ] loss: 0.605
[ 64 / 64 ] loss: 0.309
0.6108191581442952
Accuracy: 0.705570 -- Precision: 0.712000 -- Recall: 0.820276 -- F1: 0.762313 -- AUC: 0.758698
保存模型参数
========= epoch: 2 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.566
[ 2 / 64 ] loss: 0.627
[ 3 / 64 ] loss: 0.682
[ 4 / 64 ] loss: 0.483
[ 5 / 64 ] loss: 0.513
[ 6 / 64 ] loss: 0.465
[ 7 / 64 ] loss: 0.529
[ 8 / 64 ] loss: 0.487
[ 9 / 64 ] loss: 0.595
[ 10 / 64 ] loss: 0.822
[ 11 / 64 ] loss: 0.804
[ 12 / 64 ] loss: 0.620
[ 13 / 64 ] loss: 0.442
[ 14 / 64 ] loss: 0.684
[ 15 / 64 ] loss: 0.584
[ 16 / 64 ] loss: 0.680
[ 17 / 64 ] loss: 0.645
[ 18 / 64 ] loss: 0.656
[ 19 / 64 ] loss: 0.471
[ 20 / 64 ] loss: 0.480
[ 21 / 64 ] loss: 0.500
[ 22 / 64 ] loss: 0.555
[ 23 / 64 ] loss: 0.444
[ 24 / 64 ] loss: 0.516
[ 25 / 64 ] loss: 0.587
[ 26 / 64 ] loss: 0.545
[ 27 / 64 ] loss: 0.532
[ 28 / 64 ] loss: 0.485
[ 29 / 64 ] loss: 0.633
[ 30 / 64 ] loss: 0.484
[ 31 / 64 ] loss: 0.710
[ 32 / 64 ] loss: 0.501
[ 33 / 64 ] loss: 0.541
[ 34 / 64 ] loss: 0.611
[ 35 / 64 ] loss: 0.552
[ 36 / 64 ] loss: 0.547
[ 37 / 64 ] loss: 0.431
[ 38 / 64 ] loss: 0.457
[ 39 / 64 ] loss: 0.502
[ 40 / 64 ] loss: 0.550
[ 41 / 64 ] loss: 0.498
[ 42 / 64 ] loss: 0.507
[ 43 / 64 ] loss: 0.544
[ 44 / 64 ] loss: 0.440
[ 45 / 64 ] loss: 0.534
[ 46 / 64 ] loss: 0.467
[ 47 / 64 ] loss: 0.412
[ 48 / 64 ] loss: 0.479
[ 49 / 64 ] loss: 0.410
[ 50 / 64 ] loss: 0.562
[ 51 / 64 ] loss: 0.518
[ 52 / 64 ] loss: 0.360
[ 53 / 64 ] loss: 0.549
[ 54 / 64 ] loss: 0.321
[ 55 / 64 ] loss: 0.485
[ 56 / 64 ] loss: 0.410
[ 57 / 64 ] loss: 0.636
[ 58 / 64 ] loss: 0.652
[ 59 / 64 ] loss: 0.365
[ 60 / 64 ] loss: 0.503
[ 61 / 64 ] loss: 0.476
[ 62 / 64 ] loss: 0.531
[ 63 / 64 ] loss: 0.673
[ 64 / 64 ] loss: 0.714
0.5400413074530661
Accuracy: 0.750663 -- Precision: 0.794258 -- Recall: 0.764977 -- F1: 0.779343 -- AUC: 0.815351
保存模型参数
========= epoch: 3 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.633
[ 2 / 64 ] loss: 0.404
[ 3 / 64 ] loss: 0.507
[ 4 / 64 ] loss: 0.732
[ 5 / 64 ] loss: 0.590
[ 6 / 64 ] loss: 0.614
[ 7 / 64 ] loss: 0.546
[ 8 / 64 ] loss: 0.435
[ 9 / 64 ] loss: 0.603
[ 10 / 64 ] loss: 0.341
[ 11 / 64 ] loss: 0.505
[ 12 / 64 ] loss: 0.542
[ 13 / 64 ] loss: 0.466
[ 14 / 64 ] loss: 0.528
[ 15 / 64 ] loss: 0.483
[ 16 / 64 ] loss: 0.428
[ 17 / 64 ] loss: 0.463
[ 18 / 64 ] loss: 0.695
[ 19 / 64 ] loss: 0.498
[ 20 / 64 ] loss: 0.510
[ 21 / 64 ] loss: 0.636
[ 22 / 64 ] loss: 0.597
[ 23 / 64 ] loss: 0.262
[ 24 / 64 ] loss: 0.550
[ 25 / 64 ] loss: 0.431
[ 26 / 64 ] loss: 0.470
[ 27 / 64 ] loss: 0.447
[ 28 / 64 ] loss: 0.389
[ 29 / 64 ] loss: 0.534
[ 30 / 64 ] loss: 0.426
[ 31 / 64 ] loss: 0.472
[ 32 / 64 ] loss: 0.363
[ 33 / 64 ] loss: 0.331
[ 34 / 64 ] loss: 0.516
[ 35 / 64 ] loss: 0.316
[ 36 / 64 ] loss: 0.595
[ 37 / 64 ] loss: 0.551
[ 38 / 64 ] loss: 0.372
[ 39 / 64 ] loss: 0.463
[ 40 / 64 ] loss: 0.554
[ 41 / 64 ] loss: 0.307
[ 42 / 64 ] loss: 0.420
[ 43 / 64 ] loss: 0.375
[ 44 / 64 ] loss: 0.622
[ 45 / 64 ] loss: 0.459
[ 46 / 64 ] loss: 0.479
[ 47 / 64 ] loss: 0.326
[ 48 / 64 ] loss: 0.439
[ 49 / 64 ] loss: 0.568
[ 50 / 64 ] loss: 0.511
[ 51 / 64 ] loss: 0.279
[ 52 / 64 ] loss: 0.577
[ 53 / 64 ] loss: 0.481
[ 54 / 64 ] loss: 0.269
[ 55 / 64 ] loss: 0.462
[ 56 / 64 ] loss: 0.480
[ 57 / 64 ] loss: 0.635
[ 58 / 64 ] loss: 0.478
[ 59 / 64 ] loss: 0.378
[ 60 / 64 ] loss: 0.582
[ 61 / 64 ] loss: 0.578
[ 62 / 64 ] loss: 0.629
[ 63 / 64 ] loss: 0.721
[ 64 / 64 ] loss: 1.107
0.49938274221494794
Accuracy: 0.766578 -- Precision: 0.738007 -- Recall: 0.921659 -- F1: 0.819672 -- AUC: 0.774107
保存模型参数
========= epoch: 4 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.596
[ 2 / 64 ] loss: 0.523
[ 3 / 64 ] loss: 0.439
[ 4 / 64 ] loss: 0.461
[ 5 / 64 ] loss: 0.473
[ 6 / 64 ] loss: 0.420
[ 7 / 64 ] loss: 0.464
[ 8 / 64 ] loss: 0.436
[ 9 / 64 ] loss: 0.418
[ 10 / 64 ] loss: 0.742
[ 11 / 64 ] loss: 0.405
[ 12 / 64 ] loss: 0.385
[ 13 / 64 ] loss: 0.528
[ 14 / 64 ] loss: 0.308
[ 15 / 64 ] loss: 0.399
[ 16 / 64 ] loss: 0.358
[ 17 / 64 ] loss: 0.470
[ 18 / 64 ] loss: 0.339
[ 19 / 64 ] loss: 0.352
[ 20 / 64 ] loss: 0.403
[ 21 / 64 ] loss: 0.453
[ 22 / 64 ] loss: 0.370
[ 23 / 64 ] loss: 0.519
[ 24 / 64 ] loss: 0.461
[ 25 / 64 ] loss: 0.277
[ 26 / 64 ] loss: 0.516
[ 27 / 64 ] loss: 0.642
[ 28 / 64 ] loss: 0.421
[ 29 / 64 ] loss: 0.436
[ 30 / 64 ] loss: 0.683
[ 31 / 64 ] loss: 0.432
[ 32 / 64 ] loss: 0.341
[ 33 / 64 ] loss: 0.290
[ 34 / 64 ] loss: 0.766
[ 35 / 64 ] loss: 0.364
[ 36 / 64 ] loss: 0.657
[ 37 / 64 ] loss: 0.844
[ 38 / 64 ] loss: 0.741
[ 39 / 64 ] loss: 0.588
[ 40 / 64 ] loss: 0.563
[ 41 / 64 ] loss: 0.261
[ 42 / 64 ] loss: 0.556
[ 43 / 64 ] loss: 0.554
[ 44 / 64 ] loss: 0.247
[ 45 / 64 ] loss: 0.432
[ 46 / 64 ] loss: 0.537
[ 47 / 64 ] loss: 0.430
[ 48 / 64 ] loss: 0.360
[ 49 / 64 ] loss: 0.407
[ 50 / 64 ] loss: 0.402
[ 51 / 64 ] loss: 0.345
[ 52 / 64 ] loss: 0.419
[ 53 / 64 ] loss: 0.428
[ 54 / 64 ] loss: 0.644
[ 55 / 64 ] loss: 0.397
[ 56 / 64 ] loss: 0.585
[ 57 / 64 ] loss: 0.486
[ 58 / 64 ] loss: 0.463
[ 59 / 64 ] loss: 0.401
[ 60 / 64 ] loss: 0.619
[ 61 / 64 ] loss: 0.614
[ 62 / 64 ] loss: 0.466
[ 63 / 64 ] loss: 0.416
[ 64 / 64 ] loss: 0.130
0.46691761212423444
Accuracy: 0.798408 -- Precision: 0.792531 -- Recall: 0.880184 -- F1: 0.834061 -- AUC: 0.854551
保存模型参数
========= epoch: 5 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.472
[ 2 / 64 ] loss: 0.380
[ 3 / 64 ] loss: 0.309
[ 4 / 64 ] loss: 0.372
[ 5 / 64 ] loss: 0.592
[ 6 / 64 ] loss: 0.469
[ 7 / 64 ] loss: 0.558
[ 8 / 64 ] loss: 0.260
[ 9 / 64 ] loss: 0.404
[ 10 / 64 ] loss: 0.555
[ 11 / 64 ] loss: 0.552
[ 12 / 64 ] loss: 0.585
[ 13 / 64 ] loss: 0.407
[ 14 / 64 ] loss: 0.401
[ 15 / 64 ] loss: 0.461
[ 16 / 64 ] loss: 0.396
[ 17 / 64 ] loss: 0.385
[ 18 / 64 ] loss: 0.282
[ 19 / 64 ] loss: 0.609
[ 20 / 64 ] loss: 0.834
[ 21 / 64 ] loss: 0.356
[ 22 / 64 ] loss: 0.231
[ 23 / 64 ] loss: 0.589
[ 24 / 64 ] loss: 0.246
[ 25 / 64 ] loss: 0.460
[ 26 / 64 ] loss: 0.444
[ 27 / 64 ] loss: 0.421
[ 28 / 64 ] loss: 0.509
[ 29 / 64 ] loss: 0.340
[ 30 / 64 ] loss: 0.382
[ 31 / 64 ] loss: 0.498
[ 32 / 64 ] loss: 0.344
[ 33 / 64 ] loss: 0.520
[ 34 / 64 ] loss: 0.536
[ 35 / 64 ] loss: 0.371
[ 36 / 64 ] loss: 0.325
[ 37 / 64 ] loss: 0.476
[ 38 / 64 ] loss: 0.516
[ 39 / 64 ] loss: 0.445
[ 40 / 64 ] loss: 0.435
[ 41 / 64 ] loss: 0.359
[ 42 / 64 ] loss: 0.470
[ 43 / 64 ] loss: 0.421
[ 44 / 64 ] loss: 0.442
[ 45 / 64 ] loss: 0.400
[ 46 / 64 ] loss: 0.515
[ 47 / 64 ] loss: 0.460
[ 48 / 64 ] loss: 0.444
[ 49 / 64 ] loss: 0.555
[ 50 / 64 ] loss: 0.305
[ 51 / 64 ] loss: 0.359
[ 52 / 64 ] loss: 0.496
[ 53 / 64 ] loss: 0.248
[ 54 / 64 ] loss: 0.498
[ 55 / 64 ] loss: 0.298
[ 56 / 64 ] loss: 0.370
[ 57 / 64 ] loss: 0.381
[ 58 / 64 ] loss: 0.346
[ 59 / 64 ] loss: 0.251
[ 60 / 64 ] loss: 0.315
[ 61 / 64 ] loss: 0.493
[ 62 / 64 ] loss: 0.553
[ 63 / 64 ] loss: 0.281
[ 64 / 64 ] loss: 0.598
0.4310284957755357
Accuracy: 0.795756 -- Precision: 0.775591 -- Recall: 0.907834 -- F1: 0.836518 -- AUC: 0.849194
========= epoch: 6 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.323
[ 2 / 64 ] loss: 0.326
[ 3 / 64 ] loss: 0.517
[ 4 / 64 ] loss: 0.499
[ 5 / 64 ] loss: 0.574
[ 6 / 64 ] loss: 0.685
[ 7 / 64 ] loss: 0.621
[ 8 / 64 ] loss: 0.440
[ 9 / 64 ] loss: 0.565
[ 10 / 64 ] loss: 0.997
[ 11 / 64 ] loss: 0.790
[ 12 / 64 ] loss: 0.425
[ 13 / 64 ] loss: 0.646
[ 14 / 64 ] loss: 0.691
[ 15 / 64 ] loss: 0.830
[ 16 / 64 ] loss: 0.725
[ 17 / 64 ] loss: 0.483
[ 18 / 64 ] loss: 0.716
[ 19 / 64 ] loss: 0.498
[ 20 / 64 ] loss: 0.656
[ 21 / 64 ] loss: 0.527
[ 22 / 64 ] loss: 0.342
[ 23 / 64 ] loss: 0.242
[ 24 / 64 ] loss: 0.674
[ 25 / 64 ] loss: 0.506
[ 26 / 64 ] loss: 0.571
[ 27 / 64 ] loss: 0.484
[ 28 / 64 ] loss: 0.352
[ 29 / 64 ] loss: 0.314
[ 30 / 64 ] loss: 0.426
[ 31 / 64 ] loss: 0.290
[ 32 / 64 ] loss: 0.455
[ 33 / 64 ] loss: 0.395
[ 34 / 64 ] loss: 0.423
[ 35 / 64 ] loss: 0.441
[ 36 / 64 ] loss: 0.420
[ 37 / 64 ] loss: 0.317
[ 38 / 64 ] loss: 0.220
[ 39 / 64 ] loss: 0.560
[ 40 / 64 ] loss: 0.405
[ 41 / 64 ] loss: 0.564
[ 42 / 64 ] loss: 0.283
[ 43 / 64 ] loss: 0.441
[ 44 / 64 ] loss: 0.452
[ 45 / 64 ] loss: 0.458
[ 46 / 64 ] loss: 0.425
[ 47 / 64 ] loss: 0.231
[ 48 / 64 ] loss: 0.291
[ 49 / 64 ] loss: 0.364
[ 50 / 64 ] loss: 0.411
[ 51 / 64 ] loss: 0.239
[ 52 / 64 ] loss: 0.255
[ 53 / 64 ] loss: 0.416
[ 54 / 64 ] loss: 0.491
[ 55 / 64 ] loss: 0.506
[ 56 / 64 ] loss: 0.668
[ 57 / 64 ] loss: 0.258
[ 58 / 64 ] loss: 0.469
[ 59 / 64 ] loss: 0.496
[ 60 / 64 ] loss: 0.596
[ 61 / 64 ] loss: 0.405
[ 62 / 64 ] loss: 0.357
[ 63 / 64 ] loss: 0.455
[ 64 / 64 ] loss: 0.155
0.46966030821204185
Accuracy: 0.806366 -- Precision: 0.818584 -- Recall: 0.852535 -- F1: 0.835214 -- AUC: 0.845334
保存模型参数
========= epoch: 7 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.615
[ 2 / 64 ] loss: 0.281
[ 3 / 64 ] loss: 0.367
[ 4 / 64 ] loss: 0.559
[ 5 / 64 ] loss: 0.271
[ 6 / 64 ] loss: 0.342
[ 7 / 64 ] loss: 0.418
[ 8 / 64 ] loss: 0.639
[ 9 / 64 ] loss: 0.296
[ 10 / 64 ] loss: 0.723
[ 11 / 64 ] loss: 0.387
[ 12 / 64 ] loss: 0.464
[ 13 / 64 ] loss: 0.525
[ 14 / 64 ] loss: 0.385
[ 15 / 64 ] loss: 0.288
[ 16 / 64 ] loss: 0.149
[ 17 / 64 ] loss: 0.178
[ 18 / 64 ] loss: 0.228
[ 19 / 64 ] loss: 0.319
[ 20 / 64 ] loss: 0.451
[ 21 / 64 ] loss: 0.576
[ 22 / 64 ] loss: 0.618
[ 23 / 64 ] loss: 0.434
[ 24 / 64 ] loss: 0.452
[ 25 / 64 ] loss: 0.485
[ 26 / 64 ] loss: 0.402
[ 27 / 64 ] loss: 0.442
[ 28 / 64 ] loss: 0.406
[ 29 / 64 ] loss: 0.391
[ 30 / 64 ] loss: 0.444
[ 31 / 64 ] loss: 0.403
[ 32 / 64 ] loss: 0.466
[ 33 / 64 ] loss: 0.261
[ 34 / 64 ] loss: 0.534
[ 35 / 64 ] loss: 0.509
[ 36 / 64 ] loss: 0.504
[ 37 / 64 ] loss: 0.284
[ 38 / 64 ] loss: 0.417
[ 39 / 64 ] loss: 0.325
[ 40 / 64 ] loss: 0.359
[ 41 / 64 ] loss: 0.659
[ 42 / 64 ] loss: 0.379
[ 43 / 64 ] loss: 0.300
[ 44 / 64 ] loss: 0.387
[ 45 / 64 ] loss: 0.531
[ 46 / 64 ] loss: 0.571
[ 47 / 64 ] loss: 0.333
[ 48 / 64 ] loss: 0.478
[ 49 / 64 ] loss: 0.500
[ 50 / 64 ] loss: 0.573
[ 51 / 64 ] loss: 0.370
[ 52 / 64 ] loss: 0.702
[ 53 / 64 ] loss: 0.449
[ 54 / 64 ] loss: 0.635
[ 55 / 64 ] loss: 0.278
[ 56 / 64 ] loss: 0.416
[ 57 / 64 ] loss: 0.533
[ 58 / 64 ] loss: 0.434
[ 59 / 64 ] loss: 0.542
[ 60 / 64 ] loss: 0.423
[ 61 / 64 ] loss: 0.491
[ 62 / 64 ] loss: 0.524
[ 63 / 64 ] loss: 0.457
[ 64 / 64 ] loss: 0.213
0.433990536024794
Accuracy: 0.827586 -- Precision: 0.808943 -- Recall: 0.917051 -- F1: 0.859611 -- AUC: 0.883036
保存模型参数
========= epoch: 8 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.648
[ 2 / 64 ] loss: 0.350
[ 3 / 64 ] loss: 0.383
[ 4 / 64 ] loss: 0.432
[ 5 / 64 ] loss: 0.410
[ 6 / 64 ] loss: 0.383
[ 7 / 64 ] loss: 0.347
[ 8 / 64 ] loss: 0.473
[ 9 / 64 ] loss: 0.367
[ 10 / 64 ] loss: 0.528
[ 11 / 64 ] loss: 0.359
[ 12 / 64 ] loss: 0.292
[ 13 / 64 ] loss: 0.528
[ 14 / 64 ] loss: 0.379
[ 15 / 64 ] loss: 0.310
[ 16 / 64 ] loss: 0.450
[ 17 / 64 ] loss: 0.667
[ 18 / 64 ] loss: 0.499
[ 19 / 64 ] loss: 0.335
[ 20 / 64 ] loss: 0.344
[ 21 / 64 ] loss: 0.453
[ 22 / 64 ] loss: 0.565
[ 23 / 64 ] loss: 0.431
[ 24 / 64 ] loss: 0.299
[ 25 / 64 ] loss: 0.269
[ 26 / 64 ] loss: 0.484
[ 27 / 64 ] loss: 0.603
[ 28 / 64 ] loss: 0.275
[ 29 / 64 ] loss: 0.468
[ 30 / 64 ] loss: 0.220
[ 31 / 64 ] loss: 0.257
[ 32 / 64 ] loss: 0.604
[ 33 / 64 ] loss: 0.241
[ 34 / 64 ] loss: 0.417
[ 35 / 64 ] loss: 0.315
[ 36 / 64 ] loss: 0.268
[ 37 / 64 ] loss: 0.508
[ 38 / 64 ] loss: 0.265
[ 39 / 64 ] loss: 0.212
[ 40 / 64 ] loss: 0.534
[ 41 / 64 ] loss: 0.267
[ 42 / 64 ] loss: 0.643
[ 43 / 64 ] loss: 0.282
[ 44 / 64 ] loss: 0.373
[ 45 / 64 ] loss: 0.230
[ 46 / 64 ] loss: 0.627
[ 47 / 64 ] loss: 0.278
[ 48 / 64 ] loss: 0.427
[ 49 / 64 ] loss: 0.237
[ 50 / 64 ] loss: 0.224
[ 51 / 64 ] loss: 0.482
[ 52 / 64 ] loss: 0.420
[ 53 / 64 ] loss: 0.543
[ 54 / 64 ] loss: 0.242
[ 55 / 64 ] loss: 0.330
[ 56 / 64 ] loss: 0.446
[ 57 / 64 ] loss: 0.505
[ 58 / 64 ] loss: 0.229
[ 59 / 64 ] loss: 0.188
[ 60 / 64 ] loss: 0.429
[ 61 / 64 ] loss: 0.332
[ 62 / 64 ] loss: 0.255
[ 63 / 64 ] loss: 0.312
[ 64 / 64 ] loss: 0.117
0.3842880558222532
Accuracy: 0.838196 -- Precision: 0.807087 -- Recall: 0.944700 -- F1: 0.870488 -- AUC: 0.863393
保存模型参数
========= epoch: 9 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.405
[ 2 / 64 ] loss: 0.485
[ 3 / 64 ] loss: 0.608
[ 4 / 64 ] loss: 0.367
[ 5 / 64 ] loss: 0.296
[ 6 / 64 ] loss: 0.487
[ 7 / 64 ] loss: 0.661
[ 8 / 64 ] loss: 0.621
[ 9 / 64 ] loss: 0.314
[ 10 / 64 ] loss: 0.418
[ 11 / 64 ] loss: 0.470
[ 12 / 64 ] loss: 0.562
[ 13 / 64 ] loss: 0.451
[ 14 / 64 ] loss: 0.296
[ 15 / 64 ] loss: 0.569
[ 16 / 64 ] loss: 0.342
[ 17 / 64 ] loss: 0.404
[ 18 / 64 ] loss: 0.292
[ 19 / 64 ] loss: 0.451
[ 20 / 64 ] loss: 0.652
[ 21 / 64 ] loss: 0.461
[ 22 / 64 ] loss: 0.311
[ 23 / 64 ] loss: 0.252
[ 24 / 64 ] loss: 0.312
[ 25 / 64 ] loss: 0.315
[ 26 / 64 ] loss: 0.445
[ 27 / 64 ] loss: 0.308
[ 28 / 64 ] loss: 0.336
[ 29 / 64 ] loss: 0.305
[ 30 / 64 ] loss: 0.475
[ 31 / 64 ] loss: 0.436
[ 32 / 64 ] loss: 0.455
[ 33 / 64 ] loss: 0.345
[ 34 / 64 ] loss: 0.330
[ 35 / 64 ] loss: 0.460
[ 36 / 64 ] loss: 0.239
[ 37 / 64 ] loss: 0.382
[ 38 / 64 ] loss: 0.189
[ 39 / 64 ] loss: 0.398
[ 40 / 64 ] loss: 0.290
[ 41 / 64 ] loss: 0.373
[ 42 / 64 ] loss: 0.164
[ 43 / 64 ] loss: 0.315
[ 44 / 64 ] loss: 0.214
[ 45 / 64 ] loss: 0.379
[ 46 / 64 ] loss: 0.594
[ 47 / 64 ] loss: 0.192
[ 48 / 64 ] loss: 0.262
[ 49 / 64 ] loss: 0.258
[ 50 / 64 ] loss: 0.480
[ 51 / 64 ] loss: 0.439
[ 52 / 64 ] loss: 0.262
[ 53 / 64 ] loss: 0.229
[ 54 / 64 ] loss: 0.163
[ 55 / 64 ] loss: 0.132
[ 56 / 64 ] loss: 0.566
[ 57 / 64 ] loss: 0.361
[ 58 / 64 ] loss: 0.290
[ 59 / 64 ] loss: 0.267
[ 60 / 64 ] loss: 0.114
[ 61 / 64 ] loss: 0.441
[ 62 / 64 ] loss: 0.469
[ 63 / 64 ] loss: 0.271
[ 64 / 64 ] loss: 0.106
0.367750657023862
Accuracy: 0.843501 -- Precision: 0.826446 -- Recall: 0.921659 -- F1: 0.871460 -- AUC: 0.873790
保存模型参数
========= epoch: 10 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.356
[ 2 / 64 ] loss: 0.325
[ 3 / 64 ] loss: 0.189
[ 4 / 64 ] loss: 0.551
[ 5 / 64 ] loss: 0.280
[ 6 / 64 ] loss: 0.399
[ 7 / 64 ] loss: 0.211
[ 8 / 64 ] loss: 0.116
[ 9 / 64 ] loss: 0.210
[ 10 / 64 ] loss: 0.176
[ 11 / 64 ] loss: 0.346
[ 12 / 64 ] loss: 0.602
[ 13 / 64 ] loss: 0.458
[ 14 / 64 ] loss: 0.211
[ 15 / 64 ] loss: 0.478
[ 16 / 64 ] loss: 0.132
[ 17 / 64 ] loss: 0.155
[ 18 / 64 ] loss: 0.642
[ 19 / 64 ] loss: 0.273
[ 20 / 64 ] loss: 0.451
[ 21 / 64 ] loss: 0.354
[ 22 / 64 ] loss: 0.280
[ 23 / 64 ] loss: 0.342
[ 24 / 64 ] loss: 0.404
[ 25 / 64 ] loss: 0.329
[ 26 / 64 ] loss: 0.210
[ 27 / 64 ] loss: 0.409
[ 28 / 64 ] loss: 0.370
[ 29 / 64 ] loss: 0.406
[ 30 / 64 ] loss: 0.577
[ 31 / 64 ] loss: 0.396
[ 32 / 64 ] loss: 0.392
[ 33 / 64 ] loss: 0.447
[ 34 / 64 ] loss: 0.535
[ 35 / 64 ] loss: 0.437
[ 36 / 64 ] loss: 0.234
[ 37 / 64 ] loss: 0.482
[ 38 / 64 ] loss: 0.386
[ 39 / 64 ] loss: 0.522
[ 40 / 64 ] loss: 0.327
[ 41 / 64 ] loss: 0.356
[ 42 / 64 ] loss: 0.333
[ 43 / 64 ] loss: 0.314
[ 44 / 64 ] loss: 0.261
[ 45 / 64 ] loss: 0.362
[ 46 / 64 ] loss: 0.333
[ 47 / 64 ] loss: 0.435
[ 48 / 64 ] loss: 0.544
[ 49 / 64 ] loss: 0.227
[ 50 / 64 ] loss: 0.374
[ 51 / 64 ] loss: 0.295
[ 52 / 64 ] loss: 0.410
[ 53 / 64 ] loss: 0.273
[ 54 / 64 ] loss: 0.246
[ 55 / 64 ] loss: 0.249
[ 56 / 64 ] loss: 0.543
[ 57 / 64 ] loss: 0.211
[ 58 / 64 ] loss: 0.215
[ 59 / 64 ] loss: 0.231
[ 60 / 64 ] loss: 0.351
[ 61 / 64 ] loss: 0.201
[ 62 / 64 ] loss: 0.221
[ 63 / 64 ] loss: 0.425
[ 64 / 64 ] loss: 0.080
0.34204287140164524
Accuracy: 0.838196 -- Precision: 0.804688 -- Recall: 0.949309 -- F1: 0.871036 -- AUC: 0.890567
========= epoch: 11 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.369
[ 2 / 64 ] loss: 0.145
[ 3 / 64 ] loss: 0.225
[ 4 / 64 ] loss: 0.400
[ 5 / 64 ] loss: 0.333
[ 6 / 64 ] loss: 0.325
[ 7 / 64 ] loss: 0.335
[ 8 / 64 ] loss: 0.485
[ 9 / 64 ] loss: 0.382
[ 10 / 64 ] loss: 0.245
[ 11 / 64 ] loss: 0.313
[ 12 / 64 ] loss: 0.215
[ 13 / 64 ] loss: 0.342
[ 14 / 64 ] loss: 0.306
[ 15 / 64 ] loss: 0.212
[ 16 / 64 ] loss: 0.169
[ 17 / 64 ] loss: 0.450
[ 18 / 64 ] loss: 0.383
[ 19 / 64 ] loss: 0.558
[ 20 / 64 ] loss: 0.311
[ 21 / 64 ] loss: 0.285
[ 22 / 64 ] loss: 0.456
[ 23 / 64 ] loss: 0.315
[ 24 / 64 ] loss: 0.259
[ 25 / 64 ] loss: 0.139
[ 26 / 64 ] loss: 0.222
[ 27 / 64 ] loss: 0.312
[ 28 / 64 ] loss: 0.554
[ 29 / 64 ] loss: 0.293
[ 30 / 64 ] loss: 0.582
[ 31 / 64 ] loss: 0.206
[ 32 / 64 ] loss: 0.094
[ 33 / 64 ] loss: 0.475
[ 34 / 64 ] loss: 0.435
[ 35 / 64 ] loss: 0.402
[ 36 / 64 ] loss: 0.280
[ 37 / 64 ] loss: 0.414
[ 38 / 64 ] loss: 0.469
[ 39 / 64 ] loss: 0.138
[ 40 / 64 ] loss: 0.351
[ 41 / 64 ] loss: 0.222
[ 42 / 64 ] loss: 0.239
[ 43 / 64 ] loss: 0.140
[ 44 / 64 ] loss: 0.324
[ 45 / 64 ] loss: 0.220
[ 46 / 64 ] loss: 0.319
[ 47 / 64 ] loss: 0.559
[ 48 / 64 ] loss: 0.252
[ 49 / 64 ] loss: 0.390
[ 50 / 64 ] loss: 0.221
[ 51 / 64 ] loss: 0.371
[ 52 / 64 ] loss: 0.308
[ 53 / 64 ] loss: 0.623
[ 54 / 64 ] loss: 0.447
[ 55 / 64 ] loss: 0.680
[ 56 / 64 ] loss: 0.366
[ 57 / 64 ] loss: 0.273
[ 58 / 64 ] loss: 0.324
[ 59 / 64 ] loss: 0.337
[ 60 / 64 ] loss: 0.226
[ 61 / 64 ] loss: 0.438
[ 62 / 64 ] loss: 0.483
[ 63 / 64 ] loss: 0.325
[ 64 / 64 ] loss: 1.048
0.34870018518995494
Accuracy: 0.816976 -- Precision: 0.852381 -- Recall: 0.824885 -- F1: 0.838407 -- AUC: 0.861780
========= epoch: 12 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.579
[ 2 / 64 ] loss: 0.085
[ 3 / 64 ] loss: 0.883
[ 4 / 64 ] loss: 0.442
[ 5 / 64 ] loss: 0.418
[ 6 / 64 ] loss: 0.220
[ 7 / 64 ] loss: 0.357
[ 8 / 64 ] loss: 0.247
[ 9 / 64 ] loss: 0.314
[ 10 / 64 ] loss: 0.310
[ 11 / 64 ] loss: 0.305
[ 12 / 64 ] loss: 0.338
[ 13 / 64 ] loss: 0.315
[ 14 / 64 ] loss: 0.288
[ 15 / 64 ] loss: 0.353
[ 16 / 64 ] loss: 0.494
[ 17 / 64 ] loss: 0.452
[ 18 / 64 ] loss: 0.488
[ 19 / 64 ] loss: 0.357
[ 20 / 64 ] loss: 0.349
[ 21 / 64 ] loss: 0.368
[ 22 / 64 ] loss: 0.375
[ 23 / 64 ] loss: 0.344
[ 24 / 64 ] loss: 0.283
[ 25 / 64 ] loss: 0.307
[ 26 / 64 ] loss: 0.454
[ 27 / 64 ] loss: 0.109
[ 28 / 64 ] loss: 0.490
[ 29 / 64 ] loss: 0.473
[ 30 / 64 ] loss: 0.332
[ 31 / 64 ] loss: 0.281
[ 32 / 64 ] loss: 0.407
[ 33 / 64 ] loss: 0.341
[ 34 / 64 ] loss: 0.240
[ 35 / 64 ] loss: 0.246
[ 36 / 64 ] loss: 0.271
[ 37 / 64 ] loss: 0.631
[ 38 / 64 ] loss: 0.301
[ 39 / 64 ] loss: 0.315
[ 40 / 64 ] loss: 0.474
[ 41 / 64 ] loss: 0.326
[ 42 / 64 ] loss: 0.142
[ 43 / 64 ] loss: 0.296
[ 44 / 64 ] loss: 0.311
[ 45 / 64 ] loss: 0.339
[ 46 / 64 ] loss: 0.407
[ 47 / 64 ] loss: 0.178
[ 48 / 64 ] loss: 0.386
[ 49 / 64 ] loss: 0.260
[ 50 / 64 ] loss: 0.377
[ 51 / 64 ] loss: 0.515
[ 52 / 64 ] loss: 0.365
[ 53 / 64 ] loss: 0.340
[ 54 / 64 ] loss: 0.312
[ 55 / 64 ] loss: 0.331
[ 56 / 64 ] loss: 0.197
[ 57 / 64 ] loss: 0.135
[ 58 / 64 ] loss: 0.567
[ 59 / 64 ] loss: 0.402
[ 60 / 64 ] loss: 0.397
[ 61 / 64 ] loss: 0.319
[ 62 / 64 ] loss: 0.422
[ 63 / 64 ] loss: 0.395
[ 64 / 64 ] loss: 0.065
0.35030494281090796
Accuracy: 0.832891 -- Precision: 0.820833 -- Recall: 0.907834 -- F1: 0.862144 -- AUC: 0.879522
========= epoch: 13 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.207
[ 2 / 64 ] loss: 0.186
[ 3 / 64 ] loss: 0.344
[ 4 / 64 ] loss: 0.383
[ 5 / 64 ] loss: 0.317
[ 6 / 64 ] loss: 0.074
[ 7 / 64 ] loss: 0.399
[ 8 / 64 ] loss: 0.153
[ 9 / 64 ] loss: 0.407
[ 10 / 64 ] loss: 0.746
[ 11 / 64 ] loss: 0.500
[ 12 / 64 ] loss: 0.187
[ 13 / 64 ] loss: 0.170
[ 14 / 64 ] loss: 0.419
[ 15 / 64 ] loss: 0.556
[ 16 / 64 ] loss: 0.082
[ 17 / 64 ] loss: 0.405
[ 18 / 64 ] loss: 0.332
[ 19 / 64 ] loss: 0.354
[ 20 / 64 ] loss: 0.202
[ 21 / 64 ] loss: 0.528
[ 22 / 64 ] loss: 0.318
[ 23 / 64 ] loss: 0.574
[ 24 / 64 ] loss: 0.260
[ 25 / 64 ] loss: 0.369
[ 26 / 64 ] loss: 0.185
[ 27 / 64 ] loss: 0.403
[ 28 / 64 ] loss: 0.401
[ 29 / 64 ] loss: 0.207
[ 30 / 64 ] loss: 0.447
[ 31 / 64 ] loss: 0.435
[ 32 / 64 ] loss: 0.136
[ 33 / 64 ] loss: 0.223
[ 34 / 64 ] loss: 0.185
[ 35 / 64 ] loss: 0.233
[ 36 / 64 ] loss: 0.243
[ 37 / 64 ] loss: 0.591
[ 38 / 64 ] loss: 0.531
[ 39 / 64 ] loss: 0.395
[ 40 / 64 ] loss: 0.235
[ 41 / 64 ] loss: 0.226
[ 42 / 64 ] loss: 0.207
[ 43 / 64 ] loss: 0.132
[ 44 / 64 ] loss: 0.555
[ 45 / 64 ] loss: 0.443
[ 46 / 64 ] loss: 0.248
[ 47 / 64 ] loss: 0.270
[ 48 / 64 ] loss: 0.265
[ 49 / 64 ] loss: 0.458
[ 50 / 64 ] loss: 0.408
[ 51 / 64 ] loss: 0.372
[ 52 / 64 ] loss: 0.577
[ 53 / 64 ] loss: 0.515
[ 54 / 64 ] loss: 0.517
[ 55 / 64 ] loss: 0.206
[ 56 / 64 ] loss: 0.423
[ 57 / 64 ] loss: 0.184
[ 58 / 64 ] loss: 0.418
[ 59 / 64 ] loss: 0.162
[ 60 / 64 ] loss: 0.195
[ 61 / 64 ] loss: 0.366
[ 62 / 64 ] loss: 0.339
[ 63 / 64 ] loss: 0.195
[ 64 / 64 ] loss: 0.076
0.3293974190019071
Accuracy: 0.814324 -- Precision: 0.888889 -- Recall: 0.774194 -- F1: 0.827586 -- AUC: 0.882258
========= epoch: 14 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.604
[ 2 / 64 ] loss: 0.321
[ 3 / 64 ] loss: 0.238
[ 4 / 64 ] loss: 0.298
[ 5 / 64 ] loss: 0.238
[ 6 / 64 ] loss: 0.103
[ 7 / 64 ] loss: 0.356
[ 8 / 64 ] loss: 0.335
[ 9 / 64 ] loss: 0.176
[ 10 / 64 ] loss: 0.192
[ 11 / 64 ] loss: 0.350
[ 12 / 64 ] loss: 0.273
[ 13 / 64 ] loss: 0.391
[ 14 / 64 ] loss: 0.196
[ 15 / 64 ] loss: 0.484
[ 16 / 64 ] loss: 0.343
[ 17 / 64 ] loss: 0.140
[ 18 / 64 ] loss: 0.393
[ 19 / 64 ] loss: 0.379
[ 20 / 64 ] loss: 0.248
[ 21 / 64 ] loss: 0.122
[ 22 / 64 ] loss: 0.213
[ 23 / 64 ] loss: 0.083
[ 24 / 64 ] loss: 0.254
[ 25 / 64 ] loss: 0.235
[ 26 / 64 ] loss: 0.324
[ 27 / 64 ] loss: 0.556
[ 28 / 64 ] loss: 0.462
[ 29 / 64 ] loss: 0.294
[ 30 / 64 ] loss: 0.310
[ 31 / 64 ] loss: 0.149
[ 32 / 64 ] loss: 0.141
[ 33 / 64 ] loss: 0.436
[ 34 / 64 ] loss: 0.344
[ 35 / 64 ] loss: 0.590
[ 36 / 64 ] loss: 0.418
[ 37 / 64 ] loss: 0.421
[ 38 / 64 ] loss: 0.455
[ 39 / 64 ] loss: 0.199
[ 40 / 64 ] loss: 0.556
[ 41 / 64 ] loss: 0.385
[ 42 / 64 ] loss: 0.265
[ 43 / 64 ] loss: 0.360
[ 44 / 64 ] loss: 0.474
[ 45 / 64 ] loss: 0.368
[ 46 / 64 ] loss: 0.374
[ 47 / 64 ] loss: 0.242
[ 48 / 64 ] loss: 0.228
[ 49 / 64 ] loss: 0.167
[ 50 / 64 ] loss: 0.282
[ 51 / 64 ] loss: 0.338
[ 52 / 64 ] loss: 0.404
[ 53 / 64 ] loss: 0.289
[ 54 / 64 ] loss: 0.348
[ 55 / 64 ] loss: 0.415
[ 56 / 64 ] loss: 0.201
[ 57 / 64 ] loss: 0.297
[ 58 / 64 ] loss: 0.330
[ 59 / 64 ] loss: 0.215
[ 60 / 64 ] loss: 0.400
[ 61 / 64 ] loss: 0.417
[ 62 / 64 ] loss: 0.450
[ 63 / 64 ] loss: 0.304
[ 64 / 64 ] loss: 0.794
0.32761209772434086
Accuracy: 0.856764 -- Precision: 0.838174 -- Recall: 0.930876 -- F1: 0.882096 -- AUC: 0.901066
保存模型参数
========= epoch: 15 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.354
[ 2 / 64 ] loss: 0.129
[ 3 / 64 ] loss: 0.402
[ 4 / 64 ] loss: 0.449
[ 5 / 64 ] loss: 0.243
[ 6 / 64 ] loss: 0.364
[ 7 / 64 ] loss: 0.341
[ 8 / 64 ] loss: 0.336
[ 9 / 64 ] loss: 0.465
[ 10 / 64 ] loss: 0.381
[ 11 / 64 ] loss: 0.215
[ 12 / 64 ] loss: 0.303
[ 13 / 64 ] loss: 0.145
[ 14 / 64 ] loss: 0.344
[ 15 / 64 ] loss: 0.149
[ 16 / 64 ] loss: 0.412
[ 17 / 64 ] loss: 0.263
[ 18 / 64 ] loss: 0.328
[ 19 / 64 ] loss: 0.318
[ 20 / 64 ] loss: 0.107
[ 21 / 64 ] loss: 0.162
[ 22 / 64 ] loss: 0.585
[ 23 / 64 ] loss: 0.150
[ 24 / 64 ] loss: 0.318
[ 25 / 64 ] loss: 0.157
[ 26 / 64 ] loss: 0.232
[ 27 / 64 ] loss: 0.291
[ 28 / 64 ] loss: 0.398
[ 29 / 64 ] loss: 0.354
[ 30 / 64 ] loss: 0.384
[ 31 / 64 ] loss: 0.116
[ 32 / 64 ] loss: 0.159
[ 33 / 64 ] loss: 0.357
[ 34 / 64 ] loss: 0.193
[ 35 / 64 ] loss: 0.088
[ 36 / 64 ] loss: 0.991
[ 37 / 64 ] loss: 0.118
[ 38 / 64 ] loss: 0.271
[ 39 / 64 ] loss: 0.237
[ 40 / 64 ] loss: 0.239
[ 41 / 64 ] loss: 0.434
[ 42 / 64 ] loss: 0.207
[ 43 / 64 ] loss: 0.245
[ 44 / 64 ] loss: 0.363
[ 45 / 64 ] loss: 0.338
[ 46 / 64 ] loss: 0.186
[ 47 / 64 ] loss: 0.274
[ 48 / 64 ] loss: 0.240
[ 49 / 64 ] loss: 0.461
[ 50 / 64 ] loss: 0.302
[ 51 / 64 ] loss: 0.377
[ 52 / 64 ] loss: 0.258
[ 53 / 64 ] loss: 0.524
[ 54 / 64 ] loss: 0.245
[ 55 / 64 ] loss: 0.192
[ 56 / 64 ] loss: 0.225
[ 57 / 64 ] loss: 0.278
[ 58 / 64 ] loss: 0.398
[ 59 / 64 ] loss: 0.324
[ 60 / 64 ] loss: 0.124
[ 61 / 64 ] loss: 0.324
[ 62 / 64 ] loss: 0.334
[ 63 / 64 ] loss: 0.119
[ 64 / 64 ] loss: 1.028
0.30695982556790113
Accuracy: 0.854111 -- Precision: 0.861607 -- Recall: 0.889401 -- F1: 0.875283 -- AUC: 0.896774
========= epoch: 16 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.205
[ 2 / 64 ] loss: 0.117
[ 3 / 64 ] loss: 0.116
[ 4 / 64 ] loss: 0.288
[ 5 / 64 ] loss: 0.223
[ 6 / 64 ] loss: 0.345
[ 7 / 64 ] loss: 0.307
[ 8 / 64 ] loss: 0.395
[ 9 / 64 ] loss: 0.525
[ 10 / 64 ] loss: 0.098
[ 11 / 64 ] loss: 0.428
[ 12 / 64 ] loss: 0.533
[ 13 / 64 ] loss: 0.182
[ 14 / 64 ] loss: 0.390
[ 15 / 64 ] loss: 0.526
[ 16 / 64 ] loss: 0.371
[ 17 / 64 ] loss: 0.391
[ 18 / 64 ] loss: 0.326
[ 19 / 64 ] loss: 0.292
[ 20 / 64 ] loss: 0.290
[ 21 / 64 ] loss: 0.346
[ 22 / 64 ] loss: 0.582
[ 23 / 64 ] loss: 0.327
[ 24 / 64 ] loss: 0.360
[ 25 / 64 ] loss: 0.532
[ 26 / 64 ] loss: 0.283
[ 27 / 64 ] loss: 0.492
[ 28 / 64 ] loss: 0.490
[ 29 / 64 ] loss: 0.270
[ 30 / 64 ] loss: 0.400
[ 31 / 64 ] loss: 0.400
[ 32 / 64 ] loss: 0.143
[ 33 / 64 ] loss: 0.280
[ 34 / 64 ] loss: 0.389
[ 35 / 64 ] loss: 0.207
[ 36 / 64 ] loss: 0.284
[ 37 / 64 ] loss: 0.364
[ 38 / 64 ] loss: 0.494
[ 39 / 64 ] loss: 0.357
[ 40 / 64 ] loss: 0.273
[ 41 / 64 ] loss: 0.335
[ 42 / 64 ] loss: 0.243
[ 43 / 64 ] loss: 0.476
[ 44 / 64 ] loss: 0.397
[ 45 / 64 ] loss: 0.418
[ 46 / 64 ] loss: 0.413
[ 47 / 64 ] loss: 0.390
[ 48 / 64 ] loss: 0.271
[ 49 / 64 ] loss: 0.413
[ 50 / 64 ] loss: 0.278
[ 51 / 64 ] loss: 0.146
[ 52 / 64 ] loss: 0.311
[ 53 / 64 ] loss: 0.129
[ 54 / 64 ] loss: 0.334
[ 55 / 64 ] loss: 0.164
[ 56 / 64 ] loss: 0.162
[ 57 / 64 ] loss: 0.379
[ 58 / 64 ] loss: 0.373
[ 59 / 64 ] loss: 0.136
[ 60 / 64 ] loss: 0.484
[ 61 / 64 ] loss: 0.180
[ 62 / 64 ] loss: 0.281
[ 63 / 64 ] loss: 0.077
[ 64 / 64 ] loss: 0.082
0.3201912280637771
Accuracy: 0.838196 -- Precision: 0.793233 -- Recall: 0.972350 -- F1: 0.873706 -- AUC: 0.898733
========= epoch: 17 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.606
[ 2 / 64 ] loss: 0.401
[ 3 / 64 ] loss: 0.193
[ 4 / 64 ] loss: 0.227
[ 5 / 64 ] loss: 0.175
[ 6 / 64 ] loss: 0.242
[ 7 / 64 ] loss: 0.105
[ 8 / 64 ] loss: 0.175
[ 9 / 64 ] loss: 0.384
[ 10 / 64 ] loss: 0.352
[ 11 / 64 ] loss: 0.288
[ 12 / 64 ] loss: 0.326
[ 13 / 64 ] loss: 0.562
[ 14 / 64 ] loss: 0.261
[ 15 / 64 ] loss: 0.173
[ 16 / 64 ] loss: 0.269
[ 17 / 64 ] loss: 0.297
[ 18 / 64 ] loss: 0.380
[ 19 / 64 ] loss: 0.432
[ 20 / 64 ] loss: 0.609
[ 21 / 64 ] loss: 0.314
[ 22 / 64 ] loss: 0.215
[ 23 / 64 ] loss: 0.183
[ 24 / 64 ] loss: 0.272
[ 25 / 64 ] loss: 0.227
[ 26 / 64 ] loss: 0.258
[ 27 / 64 ] loss: 0.356
[ 28 / 64 ] loss: 0.397
[ 29 / 64 ] loss: 0.588
[ 30 / 64 ] loss: 0.124
[ 31 / 64 ] loss: 0.421
[ 32 / 64 ] loss: 0.265
[ 33 / 64 ] loss: 0.224
[ 34 / 64 ] loss: 0.411
[ 35 / 64 ] loss: 0.204
[ 36 / 64 ] loss: 0.199
[ 37 / 64 ] loss: 0.393
[ 38 / 64 ] loss: 0.328
[ 39 / 64 ] loss: 0.307
[ 40 / 64 ] loss: 0.270
[ 41 / 64 ] loss: 0.356
[ 42 / 64 ] loss: 0.211
[ 43 / 64 ] loss: 0.338
[ 44 / 64 ] loss: 0.245
[ 45 / 64 ] loss: 0.456
[ 46 / 64 ] loss: 0.304
[ 47 / 64 ] loss: 0.370
[ 48 / 64 ] loss: 0.552
[ 49 / 64 ] loss: 0.288
[ 50 / 64 ] loss: 0.224
[ 51 / 64 ] loss: 0.325
[ 52 / 64 ] loss: 0.528
[ 53 / 64 ] loss: 0.287
[ 54 / 64 ] loss: 0.398
[ 55 / 64 ] loss: 0.506
[ 56 / 64 ] loss: 0.280
[ 57 / 64 ] loss: 0.297
[ 58 / 64 ] loss: 0.183
[ 59 / 64 ] loss: 0.216
[ 60 / 64 ] loss: 0.189
[ 61 / 64 ] loss: 0.506
[ 62 / 64 ] loss: 0.255
[ 63 / 64 ] loss: 0.366
[ 64 / 64 ] loss: 0.636
0.3238669449929148
Accuracy: 0.824934 -- Precision: 0.868293 -- Recall: 0.820276 -- F1: 0.843602 -- AUC: 0.879868
========= epoch: 18 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.246
[ 2 / 64 ] loss: 0.245
[ 3 / 64 ] loss: 0.292
[ 4 / 64 ] loss: 0.471
[ 5 / 64 ] loss: 0.382
[ 6 / 64 ] loss: 0.376
[ 7 / 64 ] loss: 0.231
[ 8 / 64 ] loss: 0.292
[ 9 / 64 ] loss: 0.445
[ 10 / 64 ] loss: 0.345
[ 11 / 64 ] loss: 0.273
[ 12 / 64 ] loss: 0.348
[ 13 / 64 ] loss: 0.299
[ 14 / 64 ] loss: 0.325
[ 15 / 64 ] loss: 0.365
[ 16 / 64 ] loss: 0.282
[ 17 / 64 ] loss: 0.373
[ 18 / 64 ] loss: 0.205
[ 19 / 64 ] loss: 0.370
[ 20 / 64 ] loss: 0.157
[ 21 / 64 ] loss: 0.473
[ 22 / 64 ] loss: 0.321
[ 23 / 64 ] loss: 0.570
[ 24 / 64 ] loss: 0.375
[ 25 / 64 ] loss: 0.348
[ 26 / 64 ] loss: 0.259
[ 27 / 64 ] loss: 0.483
[ 28 / 64 ] loss: 0.231
[ 29 / 64 ] loss: 0.341
[ 30 / 64 ] loss: 0.273
[ 31 / 64 ] loss: 0.360
[ 32 / 64 ] loss: 0.094
[ 33 / 64 ] loss: 0.190
[ 34 / 64 ] loss: 0.230
[ 35 / 64 ] loss: 0.461
[ 36 / 64 ] loss: 0.269
[ 37 / 64 ] loss: 0.537
[ 38 / 64 ] loss: 0.569
[ 39 / 64 ] loss: 0.258
[ 40 / 64 ] loss: 0.383
[ 41 / 64 ] loss: 0.410
[ 42 / 64 ] loss: 0.466
[ 43 / 64 ] loss: 0.504
[ 44 / 64 ] loss: 0.128
[ 45 / 64 ] loss: 0.295
[ 46 / 64 ] loss: 0.125
[ 47 / 64 ] loss: 0.184
[ 48 / 64 ] loss: 0.450
[ 49 / 64 ] loss: 0.429
[ 50 / 64 ] loss: 0.293
[ 51 / 64 ] loss: 0.234
[ 52 / 64 ] loss: 0.462
[ 53 / 64 ] loss: 0.357
[ 54 / 64 ] loss: 0.339
[ 55 / 64 ] loss: 0.238
[ 56 / 64 ] loss: 0.280
[ 57 / 64 ] loss: 0.399
[ 58 / 64 ] loss: 0.359
[ 59 / 64 ] loss: 0.265
[ 60 / 64 ] loss: 0.577
[ 61 / 64 ] loss: 0.405
[ 62 / 64 ] loss: 0.322
[ 63 / 64 ] loss: 0.115
[ 64 / 64 ] loss: 0.098
0.3294489091495052
Accuracy: 0.824934 -- Precision: 0.784906 -- Recall: 0.958525 -- F1: 0.863071 -- AUC: 0.901584
========= epoch: 19 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.265
[ 2 / 64 ] loss: 0.343
[ 3 / 64 ] loss: 0.412
[ 4 / 64 ] loss: 0.269
[ 5 / 64 ] loss: 0.261
[ 6 / 64 ] loss: 0.442
[ 7 / 64 ] loss: 0.293
[ 8 / 64 ] loss: 0.206
[ 9 / 64 ] loss: 0.398
[ 10 / 64 ] loss: 0.380
[ 11 / 64 ] loss: 0.321
[ 12 / 64 ] loss: 0.307
[ 13 / 64 ] loss: 0.158
[ 14 / 64 ] loss: 0.153
[ 15 / 64 ] loss: 0.211
[ 16 / 64 ] loss: 0.228
[ 17 / 64 ] loss: 0.438
[ 18 / 64 ] loss: 0.132
[ 19 / 64 ] loss: 0.267
[ 20 / 64 ] loss: 0.503
[ 21 / 64 ] loss: 0.225
[ 22 / 64 ] loss: 0.420
[ 23 / 64 ] loss: 0.296
[ 24 / 64 ] loss: 0.356
[ 25 / 64 ] loss: 0.546
[ 26 / 64 ] loss: 0.290
[ 27 / 64 ] loss: 0.376
[ 28 / 64 ] loss: 0.109
[ 29 / 64 ] loss: 0.245
[ 30 / 64 ] loss: 0.465
[ 31 / 64 ] loss: 0.243
[ 32 / 64 ] loss: 0.192
[ 33 / 64 ] loss: 0.087
[ 34 / 64 ] loss: 0.769
[ 35 / 64 ] loss: 0.284
[ 36 / 64 ] loss: 0.269
[ 37 / 64 ] loss: 0.235
[ 38 / 64 ] loss: 0.608
[ 39 / 64 ] loss: 0.279
[ 40 / 64 ] loss: 0.302
[ 41 / 64 ] loss: 0.258
[ 42 / 64 ] loss: 0.460
[ 43 / 64 ] loss: 0.419
[ 44 / 64 ] loss: 0.152
[ 45 / 64 ] loss: 0.250
[ 46 / 64 ] loss: 0.319
[ 47 / 64 ] loss: 0.238
[ 48 / 64 ] loss: 0.169
[ 49 / 64 ] loss: 0.325
[ 50 / 64 ] loss: 0.474
[ 51 / 64 ] loss: 0.277
[ 52 / 64 ] loss: 0.238
[ 53 / 64 ] loss: 0.343
[ 54 / 64 ] loss: 0.351
[ 55 / 64 ] loss: 0.191
[ 56 / 64 ] loss: 0.348
[ 57 / 64 ] loss: 0.109
[ 58 / 64 ] loss: 0.275
[ 59 / 64 ] loss: 0.170
[ 60 / 64 ] loss: 0.213
[ 61 / 64 ] loss: 0.082
[ 62 / 64 ] loss: 0.196
[ 63 / 64 ] loss: 0.285
[ 64 / 64 ] loss: 0.081
0.29388156591448933
Accuracy: 0.840849 -- Precision: 0.861751 -- Recall: 0.861751 -- F1: 0.861751 -- AUC: 0.887903
========= epoch: 20 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.433
[ 2 / 64 ] loss: 0.303
[ 3 / 64 ] loss: 0.297
[ 4 / 64 ] loss: 0.497
[ 5 / 64 ] loss: 0.282
[ 6 / 64 ] loss: 0.333
[ 7 / 64 ] loss: 0.136
[ 8 / 64 ] loss: 0.223
[ 9 / 64 ] loss: 0.391
[ 10 / 64 ] loss: 0.124
[ 11 / 64 ] loss: 0.300
[ 12 / 64 ] loss: 0.359
[ 13 / 64 ] loss: 0.380
[ 14 / 64 ] loss: 0.312
[ 15 / 64 ] loss: 0.297
[ 16 / 64 ] loss: 0.334
[ 17 / 64 ] loss: 0.236
[ 18 / 64 ] loss: 0.508
[ 19 / 64 ] loss: 0.367
[ 20 / 64 ] loss: 0.287
[ 21 / 64 ] loss: 0.182
[ 22 / 64 ] loss: 0.099
[ 23 / 64 ] loss: 0.208
[ 24 / 64 ] loss: 0.316
[ 25 / 64 ] loss: 0.296
[ 26 / 64 ] loss: 0.180
[ 27 / 64 ] loss: 0.130
[ 28 / 64 ] loss: 0.421
[ 29 / 64 ] loss: 0.113
[ 30 / 64 ] loss: 0.186
[ 31 / 64 ] loss: 0.346
[ 32 / 64 ] loss: 0.251
[ 33 / 64 ] loss: 0.426
[ 34 / 64 ] loss: 0.153
[ 35 / 64 ] loss: 0.115
[ 36 / 64 ] loss: 0.256
[ 37 / 64 ] loss: 0.457
[ 38 / 64 ] loss: 0.364
[ 39 / 64 ] loss: 0.202
[ 40 / 64 ] loss: 0.151
[ 41 / 64 ] loss: 0.159
[ 42 / 64 ] loss: 0.325
[ 43 / 64 ] loss: 0.200
[ 44 / 64 ] loss: 0.070
[ 45 / 64 ] loss: 0.404
[ 46 / 64 ] loss: 0.068
[ 47 / 64 ] loss: 0.401
[ 48 / 64 ] loss: 0.216
[ 49 / 64 ] loss: 0.173
[ 50 / 64 ] loss: 0.056
[ 51 / 64 ] loss: 0.075
[ 52 / 64 ] loss: 0.411
[ 53 / 64 ] loss: 0.195
[ 54 / 64 ] loss: 0.068
[ 55 / 64 ] loss: 0.362
[ 56 / 64 ] loss: 0.584
[ 57 / 64 ] loss: 0.261
[ 58 / 64 ] loss: 0.193
[ 59 / 64 ] loss: 0.613
[ 60 / 64 ] loss: 0.231
[ 61 / 64 ] loss: 0.156
[ 62 / 64 ] loss: 0.410
[ 63 / 64 ] loss: 0.332
[ 64 / 64 ] loss: 0.073
0.27007589396089315
Accuracy: 0.840849 -- Precision: 0.848889 -- Recall: 0.880184 -- F1: 0.864253 -- AUC: 0.886492
========= epoch: 21 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.212
[ 2 / 64 ] loss: 0.419
[ 3 / 64 ] loss: 0.281
[ 4 / 64 ] loss: 0.179
[ 5 / 64 ] loss: 0.391
[ 6 / 64 ] loss: 0.578
[ 7 / 64 ] loss: 0.124
[ 8 / 64 ] loss: 0.277
[ 9 / 64 ] loss: 0.190
[ 10 / 64 ] loss: 0.165
[ 11 / 64 ] loss: 0.369
[ 12 / 64 ] loss: 0.428
[ 13 / 64 ] loss: 0.225
[ 14 / 64 ] loss: 0.270
[ 15 / 64 ] loss: 0.250
[ 16 / 64 ] loss: 0.183
[ 17 / 64 ] loss: 0.166
[ 18 / 64 ] loss: 0.444
[ 19 / 64 ] loss: 0.461
[ 20 / 64 ] loss: 0.391
[ 21 / 64 ] loss: 0.122
[ 22 / 64 ] loss: 0.320
[ 23 / 64 ] loss: 0.300
[ 24 / 64 ] loss: 0.182
[ 25 / 64 ] loss: 0.278
[ 26 / 64 ] loss: 0.606
[ 27 / 64 ] loss: 0.403
[ 28 / 64 ] loss: 0.282
[ 29 / 64 ] loss: 0.357
[ 30 / 64 ] loss: 0.517
[ 31 / 64 ] loss: 0.402
[ 32 / 64 ] loss: 0.284
[ 33 / 64 ] loss: 0.526
[ 34 / 64 ] loss: 0.265
[ 35 / 64 ] loss: 0.364
[ 36 / 64 ] loss: 0.406
[ 37 / 64 ] loss: 0.191
[ 38 / 64 ] loss: 0.231
[ 39 / 64 ] loss: 0.347
[ 40 / 64 ] loss: 0.275
[ 41 / 64 ] loss: 0.124
[ 42 / 64 ] loss: 0.100
[ 43 / 64 ] loss: 0.288
[ 44 / 64 ] loss: 0.243
[ 45 / 64 ] loss: 0.170
[ 46 / 64 ] loss: 0.290
[ 47 / 64 ] loss: 0.356
[ 48 / 64 ] loss: 0.100
[ 49 / 64 ] loss: 0.397
[ 50 / 64 ] loss: 0.089
[ 51 / 64 ] loss: 0.388
[ 52 / 64 ] loss: 0.126
[ 53 / 64 ] loss: 0.461
[ 54 / 64 ] loss: 0.161
[ 55 / 64 ] loss: 0.288
[ 56 / 64 ] loss: 0.659
[ 57 / 64 ] loss: 0.212
[ 58 / 64 ] loss: 0.101
[ 59 / 64 ] loss: 0.251
[ 60 / 64 ] loss: 0.571
[ 61 / 64 ] loss: 0.224
[ 62 / 64 ] loss: 0.511
[ 63 / 64 ] loss: 0.213
[ 64 / 64 ] loss: 0.047
0.29738806147361174
Accuracy: 0.875332 -- Precision: 0.882883 -- Recall: 0.903226 -- F1: 0.892938 -- AUC: 0.882056
保存模型参数
========= epoch: 22 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.171
[ 2 / 64 ] loss: 0.168
[ 3 / 64 ] loss: 0.250
[ 4 / 64 ] loss: 0.242
[ 5 / 64 ] loss: 0.165
[ 6 / 64 ] loss: 0.162
[ 7 / 64 ] loss: 0.359
[ 8 / 64 ] loss: 0.277
[ 9 / 64 ] loss: 0.308
[ 10 / 64 ] loss: 0.237
[ 11 / 64 ] loss: 0.205
[ 12 / 64 ] loss: 0.079
[ 13 / 64 ] loss: 0.180
[ 14 / 64 ] loss: 0.276
[ 15 / 64 ] loss: 0.098
[ 16 / 64 ] loss: 0.700
[ 17 / 64 ] loss: 0.245
[ 18 / 64 ] loss: 0.102
[ 19 / 64 ] loss: 0.434
[ 20 / 64 ] loss: 0.170
[ 21 / 64 ] loss: 0.281
[ 22 / 64 ] loss: 0.228
[ 23 / 64 ] loss: 0.130
[ 24 / 64 ] loss: 0.179
[ 25 / 64 ] loss: 0.435
[ 26 / 64 ] loss: 0.306
[ 27 / 64 ] loss: 0.157
[ 28 / 64 ] loss: 0.259
[ 29 / 64 ] loss: 0.394
[ 30 / 64 ] loss: 0.141
[ 31 / 64 ] loss: 0.171
[ 32 / 64 ] loss: 0.266
[ 33 / 64 ] loss: 0.557
[ 34 / 64 ] loss: 0.075
[ 35 / 64 ] loss: 0.165
[ 36 / 64 ] loss: 0.389
[ 37 / 64 ] loss: 0.440
[ 38 / 64 ] loss: 0.235
[ 39 / 64 ] loss: 0.214
[ 40 / 64 ] loss: 0.297
[ 41 / 64 ] loss: 0.166
[ 42 / 64 ] loss: 0.198
[ 43 / 64 ] loss: 0.219
[ 44 / 64 ] loss: 0.101
[ 45 / 64 ] loss: 0.406
[ 46 / 64 ] loss: 0.387
[ 47 / 64 ] loss: 0.196
[ 48 / 64 ] loss: 0.397
[ 49 / 64 ] loss: 0.236
[ 50 / 64 ] loss: 0.068
[ 51 / 64 ] loss: 0.432
[ 52 / 64 ] loss: 0.340
[ 53 / 64 ] loss: 0.183
[ 54 / 64 ] loss: 0.244
[ 55 / 64 ] loss: 0.318
[ 56 / 64 ] loss: 0.209
[ 57 / 64 ] loss: 0.346
[ 58 / 64 ] loss: 0.437
[ 59 / 64 ] loss: 0.383
[ 60 / 64 ] loss: 0.384
[ 61 / 64 ] loss: 0.358
[ 62 / 64 ] loss: 0.076
[ 63 / 64 ] loss: 0.072
[ 64 / 64 ] loss: 0.055
0.2555755293578841
Accuracy: 0.867374 -- Precision: 0.852321 -- Recall: 0.930876 -- F1: 0.889868 -- AUC: 0.907805
========= epoch: 23 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.344
[ 2 / 64 ] loss: 0.073
[ 3 / 64 ] loss: 0.302
[ 4 / 64 ] loss: 0.282
[ 5 / 64 ] loss: 0.237
[ 6 / 64 ] loss: 0.279
[ 7 / 64 ] loss: 0.131
[ 8 / 64 ] loss: 0.481
[ 9 / 64 ] loss: 0.067
[ 10 / 64 ] loss: 0.553
[ 11 / 64 ] loss: 0.361
[ 12 / 64 ] loss: 0.329
[ 13 / 64 ] loss: 0.390
[ 14 / 64 ] loss: 0.068
[ 15 / 64 ] loss: 0.527
[ 16 / 64 ] loss: 0.078
[ 17 / 64 ] loss: 0.103
[ 18 / 64 ] loss: 0.373
[ 19 / 64 ] loss: 0.123
[ 20 / 64 ] loss: 0.160
[ 21 / 64 ] loss: 0.178
[ 22 / 64 ] loss: 0.074
[ 23 / 64 ] loss: 0.404
[ 24 / 64 ] loss: 0.231
[ 25 / 64 ] loss: 0.447
[ 26 / 64 ] loss: 0.286
[ 27 / 64 ] loss: 0.963
[ 28 / 64 ] loss: 0.690
[ 29 / 64 ] loss: 0.681
[ 30 / 64 ] loss: 0.318
[ 31 / 64 ] loss: 0.263
[ 32 / 64 ] loss: 0.290
[ 33 / 64 ] loss: 0.589
[ 34 / 64 ] loss: 0.338
[ 35 / 64 ] loss: 0.274
[ 36 / 64 ] loss: 0.312
[ 37 / 64 ] loss: 0.338
[ 38 / 64 ] loss: 0.416
[ 39 / 64 ] loss: 0.119
[ 40 / 64 ] loss: 0.379
[ 41 / 64 ] loss: 0.133
[ 42 / 64 ] loss: 0.347
[ 43 / 64 ] loss: 0.390
[ 44 / 64 ] loss: 0.157
[ 45 / 64 ] loss: 0.329
[ 46 / 64 ] loss: 0.172
[ 47 / 64 ] loss: 0.094
[ 48 / 64 ] loss: 0.282
[ 49 / 64 ] loss: 0.088
[ 50 / 64 ] loss: 0.325
[ 51 / 64 ] loss: 0.195
[ 52 / 64 ] loss: 0.091
[ 53 / 64 ] loss: 0.176
[ 54 / 64 ] loss: 0.172
[ 55 / 64 ] loss: 0.235
[ 56 / 64 ] loss: 0.312
[ 57 / 64 ] loss: 0.519
[ 58 / 64 ] loss: 0.459
[ 59 / 64 ] loss: 0.320
[ 60 / 64 ] loss: 0.265
[ 61 / 64 ] loss: 0.353
[ 62 / 64 ] loss: 0.385
[ 63 / 64 ] loss: 0.172
[ 64 / 64 ] loss: 0.442
0.30103887140285224
Accuracy: 0.851459 -- Precision: 0.870968 -- Recall: 0.870968 -- F1: 0.870968 -- AUC: 0.882834
========= epoch: 24 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.306
[ 2 / 64 ] loss: 0.166
[ 3 / 64 ] loss: 0.215
[ 4 / 64 ] loss: 0.321
[ 5 / 64 ] loss: 0.444
[ 6 / 64 ] loss: 0.363
[ 7 / 64 ] loss: 0.205
[ 8 / 64 ] loss: 0.370
[ 9 / 64 ] loss: 0.297
[ 10 / 64 ] loss: 0.141
[ 11 / 64 ] loss: 0.233
[ 12 / 64 ] loss: 0.097
[ 13 / 64 ] loss: 0.085
[ 14 / 64 ] loss: 0.269
[ 15 / 64 ] loss: 0.276
[ 16 / 64 ] loss: 0.152
[ 17 / 64 ] loss: 0.464
[ 18 / 64 ] loss: 0.450
[ 19 / 64 ] loss: 0.415
[ 20 / 64 ] loss: 0.429
[ 21 / 64 ] loss: 0.484
[ 22 / 64 ] loss: 0.236
[ 23 / 64 ] loss: 0.221
[ 24 / 64 ] loss: 0.258
[ 25 / 64 ] loss: 0.596
[ 26 / 64 ] loss: 0.251
[ 27 / 64 ] loss: 0.279
[ 28 / 64 ] loss: 0.383
[ 29 / 64 ] loss: 0.204
[ 30 / 64 ] loss: 0.086
[ 31 / 64 ] loss: 0.133
[ 32 / 64 ] loss: 0.076
[ 33 / 64 ] loss: 0.277
[ 34 / 64 ] loss: 0.169
[ 35 / 64 ] loss: 0.195
[ 36 / 64 ] loss: 0.431
[ 37 / 64 ] loss: 0.229
[ 38 / 64 ] loss: 0.071
[ 39 / 64 ] loss: 0.217
[ 40 / 64 ] loss: 0.440
[ 41 / 64 ] loss: 0.249
[ 42 / 64 ] loss: 0.112
[ 43 / 64 ] loss: 0.114
[ 44 / 64 ] loss: 0.363
[ 45 / 64 ] loss: 0.196
[ 46 / 64 ] loss: 0.304
[ 47 / 64 ] loss: 0.188
[ 48 / 64 ] loss: 0.221
[ 49 / 64 ] loss: 0.308
[ 50 / 64 ] loss: 0.254
[ 51 / 64 ] loss: 0.302
[ 52 / 64 ] loss: 0.078
[ 53 / 64 ] loss: 0.068
[ 54 / 64 ] loss: 0.181
[ 55 / 64 ] loss: 0.237
[ 56 / 64 ] loss: 0.175
[ 57 / 64 ] loss: 0.309
[ 58 / 64 ] loss: 0.158
[ 59 / 64 ] loss: 0.165
[ 60 / 64 ] loss: 0.186
[ 61 / 64 ] loss: 0.171
[ 62 / 64 ] loss: 0.521
[ 63 / 64 ] loss: 0.301
[ 64 / 64 ] loss: 0.870
0.26505571452435106
Accuracy: 0.848806 -- Precision: 0.873832 -- Recall: 0.861751 -- F1: 0.867749 -- AUC: 0.883353
========= epoch: 25 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.100
[ 2 / 64 ] loss: 0.122
[ 3 / 64 ] loss: 0.245
[ 4 / 64 ] loss: 0.278
[ 5 / 64 ] loss: 0.172
[ 6 / 64 ] loss: 0.132
[ 7 / 64 ] loss: 0.400
[ 8 / 64 ] loss: 0.201
[ 9 / 64 ] loss: 0.506
[ 10 / 64 ] loss: 0.084
[ 11 / 64 ] loss: 0.491
[ 12 / 64 ] loss: 0.425
[ 13 / 64 ] loss: 0.788
[ 14 / 64 ] loss: 0.538
[ 15 / 64 ] loss: 0.403
[ 16 / 64 ] loss: 0.097
[ 17 / 64 ] loss: 0.088
[ 18 / 64 ] loss: 0.338
[ 19 / 64 ] loss: 0.385
[ 20 / 64 ] loss: 0.159
[ 21 / 64 ] loss: 0.373
[ 22 / 64 ] loss: 0.363
[ 23 / 64 ] loss: 0.277
[ 24 / 64 ] loss: 0.181
[ 25 / 64 ] loss: 0.267
[ 26 / 64 ] loss: 0.326
[ 27 / 64 ] loss: 0.291
[ 28 / 64 ] loss: 0.085
[ 29 / 64 ] loss: 0.224
[ 30 / 64 ] loss: 0.325
[ 31 / 64 ] loss: 0.223
[ 32 / 64 ] loss: 0.272
[ 33 / 64 ] loss: 0.357
[ 34 / 64 ] loss: 0.080
[ 35 / 64 ] loss: 0.103
[ 36 / 64 ] loss: 0.192
[ 37 / 64 ] loss: 0.234
[ 38 / 64 ] loss: 0.528
[ 39 / 64 ] loss: 0.170
[ 40 / 64 ] loss: 0.426
[ 41 / 64 ] loss: 0.374
[ 42 / 64 ] loss: 0.315
[ 43 / 64 ] loss: 0.259
[ 44 / 64 ] loss: 0.209
[ 45 / 64 ] loss: 0.362
[ 46 / 64 ] loss: 0.325
[ 47 / 64 ] loss: 0.189
[ 48 / 64 ] loss: 0.209
[ 49 / 64 ] loss: 0.150
[ 50 / 64 ] loss: 0.140
[ 51 / 64 ] loss: 0.236
[ 52 / 64 ] loss: 0.073
[ 53 / 64 ] loss: 0.069
[ 54 / 64 ] loss: 0.294
[ 55 / 64 ] loss: 0.258
[ 56 / 64 ] loss: 0.172
[ 57 / 64 ] loss: 0.287
[ 58 / 64 ] loss: 0.332
[ 59 / 64 ] loss: 0.068
[ 60 / 64 ] loss: 0.230
[ 61 / 64 ] loss: 0.066
[ 62 / 64 ] loss: 0.068
[ 63 / 64 ] loss: 0.171
[ 64 / 64 ] loss: 0.991
0.2671666677342728
Accuracy: 0.856764 -- Precision: 0.879070 -- Recall: 0.870968 -- F1: 0.875000 -- AUC: 0.874597
========= epoch: 26 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.375
[ 2 / 64 ] loss: 0.201
[ 3 / 64 ] loss: 0.098
[ 4 / 64 ] loss: 0.406
[ 5 / 64 ] loss: 0.340
[ 6 / 64 ] loss: 0.092
[ 7 / 64 ] loss: 0.778
[ 8 / 64 ] loss: 0.074
[ 9 / 64 ] loss: 0.235
[ 10 / 64 ] loss: 0.214
[ 11 / 64 ] loss: 0.094
[ 12 / 64 ] loss: 0.070
[ 13 / 64 ] loss: 0.068
[ 14 / 64 ] loss: 0.183
[ 15 / 64 ] loss: 0.113
[ 16 / 64 ] loss: 0.337
[ 17 / 64 ] loss: 0.049
[ 18 / 64 ] loss: 0.476
[ 19 / 64 ] loss: 0.172
[ 20 / 64 ] loss: 0.084
[ 21 / 64 ] loss: 0.268
[ 22 / 64 ] loss: 0.165
[ 23 / 64 ] loss: 0.319
[ 24 / 64 ] loss: 0.167
[ 25 / 64 ] loss: 0.286
[ 26 / 64 ] loss: 0.073
[ 27 / 64 ] loss: 0.083
[ 28 / 64 ] loss: 0.321
[ 29 / 64 ] loss: 0.545
[ 30 / 64 ] loss: 0.339
[ 31 / 64 ] loss: 0.286
[ 32 / 64 ] loss: 0.169
[ 33 / 64 ] loss: 0.172
[ 34 / 64 ] loss: 0.275
[ 35 / 64 ] loss: 0.057
[ 36 / 64 ] loss: 0.416
[ 37 / 64 ] loss: 0.213
[ 38 / 64 ] loss: 0.239
[ 39 / 64 ] loss: 0.160
[ 40 / 64 ] loss: 0.365
[ 41 / 64 ] loss: 0.056
[ 42 / 64 ] loss: 0.442
[ 43 / 64 ] loss: 0.820
[ 44 / 64 ] loss: 0.227
[ 45 / 64 ] loss: 0.183
[ 46 / 64 ] loss: 0.085
[ 47 / 64 ] loss: 0.275
[ 48 / 64 ] loss: 0.278
[ 49 / 64 ] loss: 0.071
[ 50 / 64 ] loss: 0.208
[ 51 / 64 ] loss: 0.615
[ 52 / 64 ] loss: 0.178
[ 53 / 64 ] loss: 0.213
[ 54 / 64 ] loss: 0.259
[ 55 / 64 ] loss: 0.538
[ 56 / 64 ] loss: 0.321
[ 57 / 64 ] loss: 0.169
[ 58 / 64 ] loss: 0.152
[ 59 / 64 ] loss: 0.359
[ 60 / 64 ] loss: 0.314
[ 61 / 64 ] loss: 0.086
[ 62 / 64 ] loss: 0.359
[ 63 / 64 ] loss: 0.234
[ 64 / 64 ] loss: 0.092
0.24860789813101292
Accuracy: 0.835544 -- Precision: 0.841410 -- Recall: 0.880184 -- F1: 0.860360 -- AUC: 0.842886
========= epoch: 27 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.463
[ 2 / 64 ] loss: 0.342
[ 3 / 64 ] loss: 0.510
[ 4 / 64 ] loss: 0.180
[ 5 / 64 ] loss: 0.757
[ 6 / 64 ] loss: 0.337
[ 7 / 64 ] loss: 0.427
[ 8 / 64 ] loss: 0.129
[ 9 / 64 ] loss: 0.165
[ 10 / 64 ] loss: 0.215
[ 11 / 64 ] loss: 0.377
[ 12 / 64 ] loss: 0.385
[ 13 / 64 ] loss: 0.279
[ 14 / 64 ] loss: 0.409
[ 15 / 64 ] loss: 0.397
[ 16 / 64 ] loss: 0.234
[ 17 / 64 ] loss: 0.267
[ 18 / 64 ] loss: 0.358
[ 19 / 64 ] loss: 0.557
[ 20 / 64 ] loss: 0.116
[ 21 / 64 ] loss: 0.228
[ 22 / 64 ] loss: 0.147
[ 23 / 64 ] loss: 0.197
[ 24 / 64 ] loss: 0.387
[ 25 / 64 ] loss: 0.089
[ 26 / 64 ] loss: 0.192
[ 27 / 64 ] loss: 0.204
[ 28 / 64 ] loss: 0.113
[ 29 / 64 ] loss: 0.124
[ 30 / 64 ] loss: 0.156
[ 31 / 64 ] loss: 0.084
[ 32 / 64 ] loss: 0.236
[ 33 / 64 ] loss: 0.167
[ 34 / 64 ] loss: 0.248
[ 35 / 64 ] loss: 0.188
[ 36 / 64 ] loss: 0.323
[ 37 / 64 ] loss: 0.281
[ 38 / 64 ] loss: 0.207
[ 39 / 64 ] loss: 0.178
[ 40 / 64 ] loss: 0.062
[ 41 / 64 ] loss: 0.060
[ 42 / 64 ] loss: 0.199
[ 43 / 64 ] loss: 0.540
[ 44 / 64 ] loss: 0.391
[ 45 / 64 ] loss: 0.284
[ 46 / 64 ] loss: 0.400
[ 47 / 64 ] loss: 0.285
[ 48 / 64 ] loss: 0.386
[ 49 / 64 ] loss: 0.226
[ 50 / 64 ] loss: 0.192
[ 51 / 64 ] loss: 0.325
[ 52 / 64 ] loss: 0.256
[ 53 / 64 ] loss: 0.103
[ 54 / 64 ] loss: 0.186
[ 55 / 64 ] loss: 0.385
[ 56 / 64 ] loss: 0.164
[ 57 / 64 ] loss: 0.087
[ 58 / 64 ] loss: 0.301
[ 59 / 64 ] loss: 0.360
[ 60 / 64 ] loss: 0.311
[ 61 / 64 ] loss: 0.139
[ 62 / 64 ] loss: 0.245
[ 63 / 64 ] loss: 0.077
[ 64 / 64 ] loss: 0.071
0.26076433941489086
Accuracy: 0.843501 -- Precision: 0.865741 -- Recall: 0.861751 -- F1: 0.863741 -- AUC: 0.858900
========= epoch: 28 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.372
[ 2 / 64 ] loss: 0.190
[ 3 / 64 ] loss: 0.291
[ 4 / 64 ] loss: 0.128
[ 5 / 64 ] loss: 0.061
[ 6 / 64 ] loss: 0.728
[ 7 / 64 ] loss: 0.481
[ 8 / 64 ] loss: 0.056
[ 9 / 64 ] loss: 0.420
[ 10 / 64 ] loss: 0.165
[ 11 / 64 ] loss: 0.688
[ 12 / 64 ] loss: 0.087
[ 13 / 64 ] loss: 0.146
[ 14 / 64 ] loss: 0.078
[ 15 / 64 ] loss: 0.130
[ 16 / 64 ] loss: 0.557
[ 17 / 64 ] loss: 0.181
[ 18 / 64 ] loss: 0.281
[ 19 / 64 ] loss: 0.134
[ 20 / 64 ] loss: 0.224
[ 21 / 64 ] loss: 0.185
[ 22 / 64 ] loss: 0.103
[ 23 / 64 ] loss: 0.279
[ 24 / 64 ] loss: 0.091
[ 25 / 64 ] loss: 0.118
[ 26 / 64 ] loss: 0.291
[ 27 / 64 ] loss: 0.327
[ 28 / 64 ] loss: 0.298
[ 29 / 64 ] loss: 0.171
[ 30 / 64 ] loss: 0.063
[ 31 / 64 ] loss: 0.128
[ 32 / 64 ] loss: 0.064
[ 33 / 64 ] loss: 0.178
[ 34 / 64 ] loss: 0.138
[ 35 / 64 ] loss: 0.137
[ 36 / 64 ] loss: 0.195
[ 37 / 64 ] loss: 0.292
[ 38 / 64 ] loss: 0.270
[ 39 / 64 ] loss: 0.325
[ 40 / 64 ] loss: 0.302
[ 41 / 64 ] loss: 0.171
[ 42 / 64 ] loss: 0.515
[ 43 / 64 ] loss: 0.190
[ 44 / 64 ] loss: 0.052
[ 45 / 64 ] loss: 0.214
[ 46 / 64 ] loss: 0.251
[ 47 / 64 ] loss: 0.119
[ 48 / 64 ] loss: 0.227
[ 49 / 64 ] loss: 0.222
[ 50 / 64 ] loss: 0.245
[ 51 / 64 ] loss: 0.347
[ 52 / 64 ] loss: 0.419
[ 53 / 64 ] loss: 0.202
[ 54 / 64 ] loss: 0.322
[ 55 / 64 ] loss: 0.455
[ 56 / 64 ] loss: 0.275
[ 57 / 64 ] loss: 0.446
[ 58 / 64 ] loss: 0.067
[ 59 / 64 ] loss: 0.239
[ 60 / 64 ] loss: 0.068
[ 61 / 64 ] loss: 0.253
[ 62 / 64 ] loss: 0.238
[ 63 / 64 ] loss: 0.227
[ 64 / 64 ] loss: 0.047
0.23687446565600112
Accuracy: 0.838196 -- Precision: 0.839130 -- Recall: 0.889401 -- F1: 0.863535 -- AUC: 0.889257
========= epoch: 29 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.110
[ 2 / 64 ] loss: 0.381
[ 3 / 64 ] loss: 0.088
[ 4 / 64 ] loss: 0.196
[ 5 / 64 ] loss: 0.106
[ 6 / 64 ] loss: 0.089
[ 7 / 64 ] loss: 0.427
[ 8 / 64 ] loss: 0.236
[ 9 / 64 ] loss: 0.075
[ 10 / 64 ] loss: 0.427
[ 11 / 64 ] loss: 0.073
[ 12 / 64 ] loss: 0.291
[ 13 / 64 ] loss: 0.529
[ 14 / 64 ] loss: 0.075
[ 15 / 64 ] loss: 0.234
[ 16 / 64 ] loss: 0.351
[ 17 / 64 ] loss: 0.068
[ 18 / 64 ] loss: 0.297
[ 19 / 64 ] loss: 0.075
[ 20 / 64 ] loss: 0.225
[ 21 / 64 ] loss: 0.204
[ 22 / 64 ] loss: 0.430
[ 23 / 64 ] loss: 0.342
[ 24 / 64 ] loss: 0.178
[ 25 / 64 ] loss: 0.055
[ 26 / 64 ] loss: 0.296
[ 27 / 64 ] loss: 0.055
[ 28 / 64 ] loss: 0.297
[ 29 / 64 ] loss: 0.353
[ 30 / 64 ] loss: 0.294
[ 31 / 64 ] loss: 0.103
[ 32 / 64 ] loss: 0.302
[ 33 / 64 ] loss: 0.125
[ 34 / 64 ] loss: 0.592
[ 35 / 64 ] loss: 0.385
[ 36 / 64 ] loss: 0.558
[ 37 / 64 ] loss: 0.511
[ 38 / 64 ] loss: 0.418
[ 39 / 64 ] loss: 0.612
[ 40 / 64 ] loss: 0.502
[ 41 / 64 ] loss: 0.465
[ 42 / 64 ] loss: 0.276
[ 43 / 64 ] loss: 0.265
[ 44 / 64 ] loss: 0.214
[ 45 / 64 ] loss: 0.088
[ 46 / 64 ] loss: 0.289
[ 47 / 64 ] loss: 0.174
[ 48 / 64 ] loss: 0.342
[ 49 / 64 ] loss: 0.146
[ 50 / 64 ] loss: 0.192
[ 51 / 64 ] loss: 0.246
[ 52 / 64 ] loss: 0.313
[ 53 / 64 ] loss: 0.147
[ 54 / 64 ] loss: 0.335
[ 55 / 64 ] loss: 0.104
[ 56 / 64 ] loss: 0.393
[ 57 / 64 ] loss: 0.308
[ 58 / 64 ] loss: 0.270
[ 59 / 64 ] loss: 0.176
[ 60 / 64 ] loss: 0.337
[ 61 / 64 ] loss: 0.587
[ 62 / 64 ] loss: 0.452
[ 63 / 64 ] loss: 0.373
[ 64 / 64 ] loss: 0.091
0.27420761104440317
Accuracy: 0.795756 -- Precision: 0.884615 -- Recall: 0.741935 -- F1: 0.807018 -- AUC: 0.813911
========= epoch: 30 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.213
[ 2 / 64 ] loss: 0.250
[ 3 / 64 ] loss: 0.238
[ 4 / 64 ] loss: 0.463
[ 5 / 64 ] loss: 0.073
[ 6 / 64 ] loss: 0.342
[ 7 / 64 ] loss: 0.245
[ 8 / 64 ] loss: 0.294
[ 9 / 64 ] loss: 0.446
[ 10 / 64 ] loss: 0.349
[ 11 / 64 ] loss: 0.301
[ 12 / 64 ] loss: 0.066
[ 13 / 64 ] loss: 0.402
[ 14 / 64 ] loss: 0.140
[ 15 / 64 ] loss: 0.266
[ 16 / 64 ] loss: 0.289
[ 17 / 64 ] loss: 0.202
[ 18 / 64 ] loss: 0.131
[ 19 / 64 ] loss: 0.384
[ 20 / 64 ] loss: 0.099
[ 21 / 64 ] loss: 0.197
[ 22 / 64 ] loss: 0.254
[ 23 / 64 ] loss: 0.440
[ 24 / 64 ] loss: 0.269
[ 25 / 64 ] loss: 0.205
[ 26 / 64 ] loss: 0.075
[ 27 / 64 ] loss: 0.273
[ 28 / 64 ] loss: 0.086
[ 29 / 64 ] loss: 0.308
[ 30 / 64 ] loss: 0.314
[ 31 / 64 ] loss: 0.208
[ 32 / 64 ] loss: 0.121
[ 33 / 64 ] loss: 0.176
[ 34 / 64 ] loss: 0.325
[ 35 / 64 ] loss: 0.326
[ 36 / 64 ] loss: 0.327
[ 37 / 64 ] loss: 0.244
[ 38 / 64 ] loss: 0.161
[ 39 / 64 ] loss: 0.067
[ 40 / 64 ] loss: 0.059
[ 41 / 64 ] loss: 0.560
[ 42 / 64 ] loss: 0.209
[ 43 / 64 ] loss: 0.336
[ 44 / 64 ] loss: 0.203
[ 45 / 64 ] loss: 0.060
[ 46 / 64 ] loss: 0.060
[ 47 / 64 ] loss: 0.291
[ 48 / 64 ] loss: 0.119
[ 49 / 64 ] loss: 0.465
[ 50 / 64 ] loss: 0.406
[ 51 / 64 ] loss: 0.382
[ 52 / 64 ] loss: 0.203
[ 53 / 64 ] loss: 0.099
[ 54 / 64 ] loss: 0.191
[ 55 / 64 ] loss: 0.162
[ 56 / 64 ] loss: 0.227
[ 57 / 64 ] loss: 0.222
[ 58 / 64 ] loss: 0.175
[ 59 / 64 ] loss: 0.061
[ 60 / 64 ] loss: 0.066
[ 61 / 64 ] loss: 0.056
[ 62 / 64 ] loss: 0.066
[ 63 / 64 ] loss: 0.320
[ 64 / 64 ] loss: 0.044
0.22825291490880772
Accuracy: 0.862069 -- Precision: 0.883721 -- Recall: 0.875576 -- F1: 0.879630 -- AUC: 0.847307
========= epoch: 31 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.347
[ 2 / 64 ] loss: 0.253
[ 3 / 64 ] loss: 0.056
[ 4 / 64 ] loss: 0.076
[ 5 / 64 ] loss: 0.171
[ 6 / 64 ] loss: 0.306
[ 7 / 64 ] loss: 0.164
[ 8 / 64 ] loss: 0.601
[ 9 / 64 ] loss: 0.195
[ 10 / 64 ] loss: 0.451
[ 11 / 64 ] loss: 0.398
[ 12 / 64 ] loss: 0.410
[ 13 / 64 ] loss: 0.107
[ 14 / 64 ] loss: 0.119
[ 15 / 64 ] loss: 0.391
[ 16 / 64 ] loss: 0.244
[ 17 / 64 ] loss: 0.181
[ 18 / 64 ] loss: 0.300
[ 19 / 64 ] loss: 0.058
[ 20 / 64 ] loss: 0.177
[ 21 / 64 ] loss: 0.046
[ 22 / 64 ] loss: 0.174
[ 23 / 64 ] loss: 0.262
[ 24 / 64 ] loss: 0.312
[ 25 / 64 ] loss: 0.341
[ 26 / 64 ] loss: 0.257
[ 27 / 64 ] loss: 0.102
[ 28 / 64 ] loss: 0.505
[ 29 / 64 ] loss: 0.394
[ 30 / 64 ] loss: 0.064
[ 31 / 64 ] loss: 0.055
[ 32 / 64 ] loss: 0.277
[ 33 / 64 ] loss: 0.327
[ 34 / 64 ] loss: 0.110
[ 35 / 64 ] loss: 0.225
[ 36 / 64 ] loss: 0.485
[ 37 / 64 ] loss: 0.243
[ 38 / 64 ] loss: 0.197
[ 39 / 64 ] loss: 0.304
[ 40 / 64 ] loss: 0.284
[ 41 / 64 ] loss: 0.333
[ 42 / 64 ] loss: 0.274
[ 43 / 64 ] loss: 0.567
[ 44 / 64 ] loss: 0.408
[ 45 / 64 ] loss: 0.237
[ 46 / 64 ] loss: 0.440
[ 47 / 64 ] loss: 0.195
[ 48 / 64 ] loss: 0.067
[ 49 / 64 ] loss: 0.307
[ 50 / 64 ] loss: 0.214
[ 51 / 64 ] loss: 0.176
[ 52 / 64 ] loss: 0.078
[ 53 / 64 ] loss: 0.088
[ 54 / 64 ] loss: 0.289
[ 55 / 64 ] loss: 0.076
[ 56 / 64 ] loss: 0.059
[ 57 / 64 ] loss: 0.357
[ 58 / 64 ] loss: 0.214
[ 59 / 64 ] loss: 0.185
[ 60 / 64 ] loss: 0.194
[ 61 / 64 ] loss: 0.254
[ 62 / 64 ] loss: 0.177
[ 63 / 64 ] loss: 0.072
[ 64 / 64 ] loss: 0.110
0.23967630037805066
Accuracy: 0.840849 -- Precision: 0.858447 -- Recall: 0.866359 -- F1: 0.862385 -- AUC: 0.835916
========= epoch: 32 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.296
[ 2 / 64 ] loss: 0.068
[ 3 / 64 ] loss: 0.314
[ 4 / 64 ] loss: 0.096
[ 5 / 64 ] loss: 0.350
[ 6 / 64 ] loss: 0.131
[ 7 / 64 ] loss: 0.098
[ 8 / 64 ] loss: 0.312
[ 9 / 64 ] loss: 0.052
[ 10 / 64 ] loss: 0.322
[ 11 / 64 ] loss: 0.196
[ 12 / 64 ] loss: 0.469
[ 13 / 64 ] loss: 0.345
[ 14 / 64 ] loss: 0.183
[ 15 / 64 ] loss: 0.069
[ 16 / 64 ] loss: 0.189
[ 17 / 64 ] loss: 0.061
[ 18 / 64 ] loss: 0.252
[ 19 / 64 ] loss: 0.217
[ 20 / 64 ] loss: 0.184
[ 21 / 64 ] loss: 0.160
[ 22 / 64 ] loss: 0.183
[ 23 / 64 ] loss: 0.059
[ 24 / 64 ] loss: 0.049
[ 25 / 64 ] loss: 0.045
[ 26 / 64 ] loss: 0.327
[ 27 / 64 ] loss: 0.553
[ 28 / 64 ] loss: 0.184
[ 29 / 64 ] loss: 0.165
[ 30 / 64 ] loss: 0.514
[ 31 / 64 ] loss: 0.315
[ 32 / 64 ] loss: 0.174
[ 33 / 64 ] loss: 0.214
[ 34 / 64 ] loss: 0.082
[ 35 / 64 ] loss: 0.424
[ 36 / 64 ] loss: 0.139
[ 37 / 64 ] loss: 0.294
[ 38 / 64 ] loss: 0.416
[ 39 / 64 ] loss: 0.352
[ 40 / 64 ] loss: 0.184
[ 41 / 64 ] loss: 0.051
[ 42 / 64 ] loss: 0.446
[ 43 / 64 ] loss: 0.195
[ 44 / 64 ] loss: 0.200
[ 45 / 64 ] loss: 0.396
[ 46 / 64 ] loss: 0.313
[ 47 / 64 ] loss: 0.202
[ 48 / 64 ] loss: 0.175
[ 49 / 64 ] loss: 0.167
[ 50 / 64 ] loss: 0.187
[ 51 / 64 ] loss: 0.286
[ 52 / 64 ] loss: 0.185
[ 53 / 64 ] loss: 0.247
[ 54 / 64 ] loss: 0.282
[ 55 / 64 ] loss: 0.324
[ 56 / 64 ] loss: 0.198
[ 57 / 64 ] loss: 0.437
[ 58 / 64 ] loss: 0.591
[ 59 / 64 ] loss: 0.070
[ 60 / 64 ] loss: 0.271
[ 61 / 64 ] loss: 0.334
[ 62 / 64 ] loss: 0.279
[ 63 / 64 ] loss: 0.098
[ 64 / 64 ] loss: 0.054
0.23474530701059848
Accuracy: 0.832891 -- Precision: 0.831897 -- Recall: 0.889401 -- F1: 0.859688 -- AUC: 0.842900
========= epoch: 33 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.098
[ 2 / 64 ] loss: 0.197
[ 3 / 64 ] loss: 0.166
[ 4 / 64 ] loss: 0.308
[ 5 / 64 ] loss: 0.175
[ 6 / 64 ] loss: 0.528
[ 7 / 64 ] loss: 0.330
[ 8 / 64 ] loss: 0.144
[ 9 / 64 ] loss: 0.443
[ 10 / 64 ] loss: 0.256
[ 11 / 64 ] loss: 0.283
[ 12 / 64 ] loss: 0.168
[ 13 / 64 ] loss: 0.291
[ 14 / 64 ] loss: 0.450
[ 15 / 64 ] loss: 0.353
[ 16 / 64 ] loss: 0.255
[ 17 / 64 ] loss: 0.269
[ 18 / 64 ] loss: 0.209
[ 19 / 64 ] loss: 0.173
[ 20 / 64 ] loss: 0.175
[ 21 / 64 ] loss: 0.103
[ 22 / 64 ] loss: 0.181
[ 23 / 64 ] loss: 0.208
[ 24 / 64 ] loss: 0.138
[ 25 / 64 ] loss: 0.178
[ 26 / 64 ] loss: 0.304
[ 27 / 64 ] loss: 0.288
[ 28 / 64 ] loss: 0.192
[ 29 / 64 ] loss: 0.207
[ 30 / 64 ] loss: 0.235
[ 31 / 64 ] loss: 0.446
[ 32 / 64 ] loss: 0.057
[ 33 / 64 ] loss: 0.359
[ 34 / 64 ] loss: 0.175
[ 35 / 64 ] loss: 0.171
[ 36 / 64 ] loss: 0.058
[ 37 / 64 ] loss: 0.201
[ 38 / 64 ] loss: 0.194
[ 39 / 64 ] loss: 0.119
[ 40 / 64 ] loss: 0.097
[ 41 / 64 ] loss: 0.307
[ 42 / 64 ] loss: 0.143
[ 43 / 64 ] loss: 0.091
[ 44 / 64 ] loss: 0.532
[ 45 / 64 ] loss: 0.312
[ 46 / 64 ] loss: 0.203
[ 47 / 64 ] loss: 0.125
[ 48 / 64 ] loss: 0.703
[ 49 / 64 ] loss: 0.417
[ 50 / 64 ] loss: 0.544
[ 51 / 64 ] loss: 0.077
[ 52 / 64 ] loss: 0.313
[ 53 / 64 ] loss: 0.358
[ 54 / 64 ] loss: 0.362
[ 55 / 64 ] loss: 0.269
[ 56 / 64 ] loss: 0.342
[ 57 / 64 ] loss: 0.302
[ 58 / 64 ] loss: 0.061
[ 59 / 64 ] loss: 0.176
[ 60 / 64 ] loss: 0.068
[ 61 / 64 ] loss: 0.305
[ 62 / 64 ] loss: 0.232
[ 63 / 64 ] loss: 0.210
[ 64 / 64 ] loss: 0.063
0.24526648683240637
Accuracy: 0.851459 -- Precision: 0.828571 -- Recall: 0.935484 -- F1: 0.878788 -- AUC: 0.880228
========= epoch: 34 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.104
[ 2 / 64 ] loss: 0.781
[ 3 / 64 ] loss: 0.075
[ 4 / 64 ] loss: 0.404
[ 5 / 64 ] loss: 0.213
[ 6 / 64 ] loss: 0.228
[ 7 / 64 ] loss: 0.086
[ 8 / 64 ] loss: 0.075
[ 9 / 64 ] loss: 0.116
[ 10 / 64 ] loss: 0.139
[ 11 / 64 ] loss: 0.070
[ 12 / 64 ] loss: 0.213
[ 13 / 64 ] loss: 0.060
[ 14 / 64 ] loss: 0.246
[ 15 / 64 ] loss: 0.404
[ 16 / 64 ] loss: 0.180
[ 17 / 64 ] loss: 0.127
[ 18 / 64 ] loss: 0.500
[ 19 / 64 ] loss: 0.319
[ 20 / 64 ] loss: 0.203
[ 21 / 64 ] loss: 0.082
[ 22 / 64 ] loss: 0.058
[ 23 / 64 ] loss: 0.112
[ 24 / 64 ] loss: 0.302
[ 25 / 64 ] loss: 0.123
[ 26 / 64 ] loss: 0.054
[ 27 / 64 ] loss: 0.415
[ 28 / 64 ] loss: 0.175
[ 29 / 64 ] loss: 0.192
[ 30 / 64 ] loss: 0.069
[ 31 / 64 ] loss: 0.054
[ 32 / 64 ] loss: 0.154
[ 33 / 64 ] loss: 0.067
[ 34 / 64 ] loss: 0.086
[ 35 / 64 ] loss: 0.052
[ 36 / 64 ] loss: 0.064
[ 37 / 64 ] loss: 0.041
[ 38 / 64 ] loss: 0.074
[ 39 / 64 ] loss: 0.378
[ 40 / 64 ] loss: 0.175
[ 41 / 64 ] loss: 0.180
[ 42 / 64 ] loss: 0.426
[ 43 / 64 ] loss: 0.302
[ 44 / 64 ] loss: 0.179
[ 45 / 64 ] loss: 0.329
[ 46 / 64 ] loss: 0.052
[ 47 / 64 ] loss: 0.409
[ 48 / 64 ] loss: 0.297
[ 49 / 64 ] loss: 0.235
[ 50 / 64 ] loss: 0.078
[ 51 / 64 ] loss: 0.187
[ 52 / 64 ] loss: 0.111
[ 53 / 64 ] loss: 0.435
[ 54 / 64 ] loss: 0.365
[ 55 / 64 ] loss: 0.321
[ 56 / 64 ] loss: 0.185
[ 57 / 64 ] loss: 0.376
[ 58 / 64 ] loss: 0.254
[ 59 / 64 ] loss: 0.328
[ 60 / 64 ] loss: 0.357
[ 61 / 64 ] loss: 0.050
[ 62 / 64 ] loss: 0.338
[ 63 / 64 ] loss: 0.216
[ 64 / 64 ] loss: 0.061
0.20846402895404026
Accuracy: 0.824934 -- Precision: 0.854460 -- Recall: 0.838710 -- F1: 0.846512 -- AUC: 0.836362
========= epoch: 35 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.239
[ 2 / 64 ] loss: 0.194
[ 3 / 64 ] loss: 0.184
[ 4 / 64 ] loss: 0.265
[ 5 / 64 ] loss: 0.494
[ 6 / 64 ] loss: 0.191
[ 7 / 64 ] loss: 0.371
[ 8 / 64 ] loss: 0.615
[ 9 / 64 ] loss: 0.228
[ 10 / 64 ] loss: 0.304
[ 11 / 64 ] loss: 0.509
[ 12 / 64 ] loss: 0.291
[ 13 / 64 ] loss: 0.374
[ 14 / 64 ] loss: 0.409
[ 15 / 64 ] loss: 0.333
[ 16 / 64 ] loss: 0.264
[ 17 / 64 ] loss: 0.167
[ 18 / 64 ] loss: 0.353
[ 19 / 64 ] loss: 0.188
[ 20 / 64 ] loss: 0.377
[ 21 / 64 ] loss: 0.493
[ 22 / 64 ] loss: 0.360
[ 23 / 64 ] loss: 0.670
[ 24 / 64 ] loss: 0.684
[ 25 / 64 ] loss: 0.548
[ 26 / 64 ] loss: 0.293
[ 27 / 64 ] loss: 0.436
[ 28 / 64 ] loss: 0.256
[ 29 / 64 ] loss: 0.267
[ 30 / 64 ] loss: 0.268
[ 31 / 64 ] loss: 0.230
[ 32 / 64 ] loss: 0.547
[ 33 / 64 ] loss: 0.532
[ 34 / 64 ] loss: 0.341
[ 35 / 64 ] loss: 0.221
[ 36 / 64 ] loss: 0.142
[ 37 / 64 ] loss: 0.392
[ 38 / 64 ] loss: 0.113
[ 39 / 64 ] loss: 0.449
[ 40 / 64 ] loss: 0.387
[ 41 / 64 ] loss: 0.288
[ 42 / 64 ] loss: 0.217
[ 43 / 64 ] loss: 0.231
[ 44 / 64 ] loss: 0.484
[ 45 / 64 ] loss: 0.349
[ 46 / 64 ] loss: 0.421
[ 47 / 64 ] loss: 0.470
[ 48 / 64 ] loss: 0.363
[ 49 / 64 ] loss: 0.210
[ 50 / 64 ] loss: 0.295
[ 51 / 64 ] loss: 0.153
[ 52 / 64 ] loss: 0.264
[ 53 / 64 ] loss: 0.214
[ 54 / 64 ] loss: 0.386
[ 55 / 64 ] loss: 0.174
[ 56 / 64 ] loss: 0.364
[ 57 / 64 ] loss: 0.281
[ 58 / 64 ] loss: 0.077
[ 59 / 64 ] loss: 0.177
[ 60 / 64 ] loss: 0.190
[ 61 / 64 ] loss: 0.182
[ 62 / 64 ] loss: 0.177
[ 63 / 64 ] loss: 0.183
[ 64 / 64 ] loss: 0.056
0.31542548892321065
Accuracy: 0.832891 -- Precision: 0.843750 -- Recall: 0.870968 -- F1: 0.857143 -- AUC: 0.830069
========= epoch: 36 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.491
[ 2 / 64 ] loss: 0.172
[ 3 / 64 ] loss: 0.268
[ 4 / 64 ] loss: 0.265
[ 5 / 64 ] loss: 0.145
[ 6 / 64 ] loss: 0.631
[ 7 / 64 ] loss: 0.363
[ 8 / 64 ] loss: 0.177
[ 9 / 64 ] loss: 0.211
[ 10 / 64 ] loss: 0.249
[ 11 / 64 ] loss: 0.198
[ 12 / 64 ] loss: 0.318
[ 13 / 64 ] loss: 0.281
[ 14 / 64 ] loss: 0.385
[ 15 / 64 ] loss: 0.370
[ 16 / 64 ] loss: 0.423
[ 17 / 64 ] loss: 0.429
[ 18 / 64 ] loss: 0.342
[ 19 / 64 ] loss: 0.189
[ 20 / 64 ] loss: 0.209
[ 21 / 64 ] loss: 0.332
[ 22 / 64 ] loss: 0.129
[ 23 / 64 ] loss: 0.240
[ 24 / 64 ] loss: 0.309
[ 25 / 64 ] loss: 0.382
[ 26 / 64 ] loss: 0.235
[ 27 / 64 ] loss: 0.398
[ 28 / 64 ] loss: 0.207
[ 29 / 64 ] loss: 0.224
[ 30 / 64 ] loss: 0.197
[ 31 / 64 ] loss: 0.490
[ 32 / 64 ] loss: 0.359
[ 33 / 64 ] loss: 0.148
[ 34 / 64 ] loss: 0.176
[ 35 / 64 ] loss: 0.100
[ 36 / 64 ] loss: 0.383
[ 37 / 64 ] loss: 0.422
[ 38 / 64 ] loss: 0.294
[ 39 / 64 ] loss: 0.337
[ 40 / 64 ] loss: 0.109
[ 41 / 64 ] loss: 0.188
[ 42 / 64 ] loss: 0.311
[ 43 / 64 ] loss: 0.221
[ 44 / 64 ] loss: 0.199
[ 45 / 64 ] loss: 0.171
[ 46 / 64 ] loss: 0.272
[ 47 / 64 ] loss: 0.264
[ 48 / 64 ] loss: 0.290
[ 49 / 64 ] loss: 0.120
[ 50 / 64 ] loss: 0.076
[ 51 / 64 ] loss: 0.182
[ 52 / 64 ] loss: 0.255
[ 53 / 64 ] loss: 0.204
[ 54 / 64 ] loss: 0.101
[ 55 / 64 ] loss: 0.149
[ 56 / 64 ] loss: 0.172
[ 57 / 64 ] loss: 0.178
[ 58 / 64 ] loss: 0.205
[ 59 / 64 ] loss: 0.103
[ 60 / 64 ] loss: 0.225
[ 61 / 64 ] loss: 0.141
[ 62 / 64 ] loss: 0.066
[ 63 / 64 ] loss: 0.170
[ 64 / 64 ] loss: 0.076
0.24881696875672787
Accuracy: 0.856764 -- Precision: 0.814672 -- Recall: 0.972350 -- F1: 0.886555 -- AUC: 0.919686
========= epoch: 37 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.438
[ 2 / 64 ] loss: 0.049
[ 3 / 64 ] loss: 0.164
[ 4 / 64 ] loss: 0.194
[ 5 / 64 ] loss: 0.272
[ 6 / 64 ] loss: 0.161
[ 7 / 64 ] loss: 0.096
[ 8 / 64 ] loss: 0.258
[ 9 / 64 ] loss: 0.202
[ 10 / 64 ] loss: 0.267
[ 11 / 64 ] loss: 0.048
[ 12 / 64 ] loss: 0.512
[ 13 / 64 ] loss: 0.226
[ 14 / 64 ] loss: 0.164
[ 15 / 64 ] loss: 0.526
[ 16 / 64 ] loss: 0.160
[ 17 / 64 ] loss: 0.240
[ 18 / 64 ] loss: 0.238
[ 19 / 64 ] loss: 0.231
[ 20 / 64 ] loss: 0.219
[ 21 / 64 ] loss: 0.303
[ 22 / 64 ] loss: 0.289
[ 23 / 64 ] loss: 0.180
[ 24 / 64 ] loss: 0.117
[ 25 / 64 ] loss: 0.482
[ 26 / 64 ] loss: 0.328
[ 27 / 64 ] loss: 0.105
[ 28 / 64 ] loss: 0.178
[ 29 / 64 ] loss: 0.192
[ 30 / 64 ] loss: 0.052
[ 31 / 64 ] loss: 0.219
[ 32 / 64 ] loss: 0.542
[ 33 / 64 ] loss: 0.154
[ 34 / 64 ] loss: 0.445
[ 35 / 64 ] loss: 0.178
[ 36 / 64 ] loss: 0.059
[ 37 / 64 ] loss: 0.325
[ 38 / 64 ] loss: 0.108
[ 39 / 64 ] loss: 0.177
[ 40 / 64 ] loss: 0.411
[ 41 / 64 ] loss: 0.170
[ 42 / 64 ] loss: 0.103
[ 43 / 64 ] loss: 0.385
[ 44 / 64 ] loss: 0.066
[ 45 / 64 ] loss: 0.258
[ 46 / 64 ] loss: 0.151
[ 47 / 64 ] loss: 0.169
[ 48 / 64 ] loss: 0.270
[ 49 / 64 ] loss: 0.068
[ 50 / 64 ] loss: 0.070
[ 51 / 64 ] loss: 0.069
[ 52 / 64 ] loss: 0.239
[ 53 / 64 ] loss: 0.258
[ 54 / 64 ] loss: 0.236
[ 55 / 64 ] loss: 0.056
[ 56 / 64 ] loss: 0.313
[ 57 / 64 ] loss: 0.446
[ 58 / 64 ] loss: 0.166
[ 59 / 64 ] loss: 0.074
[ 60 / 64 ] loss: 0.046
[ 61 / 64 ] loss: 0.092
[ 62 / 64 ] loss: 0.130
[ 63 / 64 ] loss: 0.337
[ 64 / 64 ] loss: 0.044
0.21446582372300327
Accuracy: 0.870027 -- Precision: 0.852941 -- Recall: 0.935484 -- F1: 0.892308 -- AUC: 0.851498
========= epoch: 38 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.266
[ 2 / 64 ] loss: 0.068
[ 3 / 64 ] loss: 0.297
[ 4 / 64 ] loss: 0.040
[ 5 / 64 ] loss: 0.245
[ 6 / 64 ] loss: 0.193
[ 7 / 64 ] loss: 0.237
[ 8 / 64 ] loss: 0.043
[ 9 / 64 ] loss: 0.040
[ 10 / 64 ] loss: 0.328
[ 11 / 64 ] loss: 0.170
[ 12 / 64 ] loss: 0.045
[ 13 / 64 ] loss: 0.435
[ 14 / 64 ] loss: 0.041
[ 15 / 64 ] loss: 0.250
[ 16 / 64 ] loss: 0.234
[ 17 / 64 ] loss: 0.380
[ 18 / 64 ] loss: 0.488
[ 19 / 64 ] loss: 0.218
[ 20 / 64 ] loss: 0.282
[ 21 / 64 ] loss: 0.324
[ 22 / 64 ] loss: 0.119
[ 23 / 64 ] loss: 0.238
[ 24 / 64 ] loss: 0.199
[ 25 / 64 ] loss: 0.160
[ 26 / 64 ] loss: 0.319
[ 27 / 64 ] loss: 0.172
[ 28 / 64 ] loss: 0.366
[ 29 / 64 ] loss: 0.060
[ 30 / 64 ] loss: 0.212
[ 31 / 64 ] loss: 0.106
[ 32 / 64 ] loss: 0.257
[ 33 / 64 ] loss: 0.450
[ 34 / 64 ] loss: 0.337
[ 35 / 64 ] loss: 0.124
[ 36 / 64 ] loss: 0.106
[ 37 / 64 ] loss: 0.125
[ 38 / 64 ] loss: 0.185
[ 39 / 64 ] loss: 0.190
[ 40 / 64 ] loss: 0.085
[ 41 / 64 ] loss: 0.175
[ 42 / 64 ] loss: 0.256
[ 43 / 64 ] loss: 0.231
[ 44 / 64 ] loss: 0.373
[ 45 / 64 ] loss: 0.061
[ 46 / 64 ] loss: 0.187
[ 47 / 64 ] loss: 0.659
[ 48 / 64 ] loss: 0.187
[ 49 / 64 ] loss: 0.268
[ 50 / 64 ] loss: 0.092
[ 51 / 64 ] loss: 0.375
[ 52 / 64 ] loss: 0.188
[ 53 / 64 ] loss: 0.053
[ 54 / 64 ] loss: 0.228
[ 55 / 64 ] loss: 0.042
[ 56 / 64 ] loss: 0.170
[ 57 / 64 ] loss: 0.327
[ 58 / 64 ] loss: 0.047
[ 59 / 64 ] loss: 0.047
[ 60 / 64 ] loss: 0.042
[ 61 / 64 ] loss: 0.172
[ 62 / 64 ] loss: 0.143
[ 63 / 64 ] loss: 0.227
[ 64 / 64 ] loss: 0.041
0.20353186083957553
Accuracy: 0.856764 -- Precision: 0.893720 -- Recall: 0.852535 -- F1: 0.872642 -- AUC: 0.903773
========= epoch: 39 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.179
[ 2 / 64 ] loss: 0.059
[ 3 / 64 ] loss: 0.355
[ 4 / 64 ] loss: 0.049
[ 5 / 64 ] loss: 0.158
[ 6 / 64 ] loss: 0.451
[ 7 / 64 ] loss: 0.198
[ 8 / 64 ] loss: 0.293
[ 9 / 64 ] loss: 0.183
[ 10 / 64 ] loss: 0.111
[ 11 / 64 ] loss: 0.039
[ 12 / 64 ] loss: 0.045
[ 13 / 64 ] loss: 0.342
[ 14 / 64 ] loss: 0.038
[ 15 / 64 ] loss: 0.159
[ 16 / 64 ] loss: 0.044
[ 17 / 64 ] loss: 0.193
[ 18 / 64 ] loss: 0.174
[ 19 / 64 ] loss: 0.138
[ 20 / 64 ] loss: 0.037
[ 21 / 64 ] loss: 0.358
[ 22 / 64 ] loss: 0.198
[ 23 / 64 ] loss: 0.064
[ 24 / 64 ] loss: 0.059
[ 25 / 64 ] loss: 0.135
[ 26 / 64 ] loss: 0.182
[ 27 / 64 ] loss: 0.325
[ 28 / 64 ] loss: 0.036
[ 29 / 64 ] loss: 0.044
[ 30 / 64 ] loss: 0.258
[ 31 / 64 ] loss: 0.319
[ 32 / 64 ] loss: 0.255
[ 33 / 64 ] loss: 0.348
[ 34 / 64 ] loss: 0.216
[ 35 / 64 ] loss: 0.042
[ 36 / 64 ] loss: 0.189
[ 37 / 64 ] loss: 0.048
[ 38 / 64 ] loss: 0.177
[ 39 / 64 ] loss: 0.176
[ 40 / 64 ] loss: 0.173
[ 41 / 64 ] loss: 0.254
[ 42 / 64 ] loss: 0.460
[ 43 / 64 ] loss: 0.116
[ 44 / 64 ] loss: 0.172
[ 45 / 64 ] loss: 0.152
[ 46 / 64 ] loss: 0.081
[ 47 / 64 ] loss: 0.164
[ 48 / 64 ] loss: 0.499
[ 49 / 64 ] loss: 0.210
[ 50 / 64 ] loss: 0.251
[ 51 / 64 ] loss: 0.199
[ 52 / 64 ] loss: 0.196
[ 53 / 64 ] loss: 0.116
[ 54 / 64 ] loss: 0.066
[ 55 / 64 ] loss: 0.063
[ 56 / 64 ] loss: 0.468
[ 57 / 64 ] loss: 0.097
[ 58 / 64 ] loss: 0.173
[ 59 / 64 ] loss: 0.185
[ 60 / 64 ] loss: 0.189
[ 61 / 64 ] loss: 0.401
[ 62 / 64 ] loss: 0.050
[ 63 / 64 ] loss: 0.346
[ 64 / 64 ] loss: 0.056
0.1845465551596135
Accuracy: 0.856764 -- Precision: 0.822134 -- Recall: 0.958525 -- F1: 0.885106 -- AUC: 0.920737
========= epoch: 40 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.048
[ 2 / 64 ] loss: 0.057
[ 3 / 64 ] loss: 0.653
[ 4 / 64 ] loss: 0.053
[ 5 / 64 ] loss: 0.043
[ 6 / 64 ] loss: 0.174
[ 7 / 64 ] loss: 0.295
[ 8 / 64 ] loss: 0.309
[ 9 / 64 ] loss: 0.048
[ 10 / 64 ] loss: 0.450
[ 11 / 64 ] loss: 0.042
[ 12 / 64 ] loss: 0.305
[ 13 / 64 ] loss: 0.174
[ 14 / 64 ] loss: 0.242
[ 15 / 64 ] loss: 0.044
[ 16 / 64 ] loss: 0.219
[ 17 / 64 ] loss: 0.239
[ 18 / 64 ] loss: 0.358
[ 19 / 64 ] loss: 0.078
[ 20 / 64 ] loss: 0.142
[ 21 / 64 ] loss: 0.194
[ 22 / 64 ] loss: 0.441
[ 23 / 64 ] loss: 0.101
[ 24 / 64 ] loss: 0.094
[ 25 / 64 ] loss: 0.273
[ 26 / 64 ] loss: 0.384
[ 27 / 64 ] loss: 0.229
[ 28 / 64 ] loss: 0.263
[ 29 / 64 ] loss: 0.049
[ 30 / 64 ] loss: 0.428
[ 31 / 64 ] loss: 0.280
[ 32 / 64 ] loss: 0.279
[ 33 / 64 ] loss: 0.165
[ 34 / 64 ] loss: 0.083
[ 35 / 64 ] loss: 0.402
[ 36 / 64 ] loss: 0.406
[ 37 / 64 ] loss: 0.059
[ 38 / 64 ] loss: 0.206
[ 39 / 64 ] loss: 0.062
[ 40 / 64 ] loss: 0.081
[ 41 / 64 ] loss: 0.064
[ 42 / 64 ] loss: 0.177
[ 43 / 64 ] loss: 0.229
[ 44 / 64 ] loss: 0.343
[ 45 / 64 ] loss: 0.132
[ 46 / 64 ] loss: 0.169
[ 47 / 64 ] loss: 0.051
[ 48 / 64 ] loss: 0.118
[ 49 / 64 ] loss: 0.128
[ 50 / 64 ] loss: 0.144
[ 51 / 64 ] loss: 0.114
[ 52 / 64 ] loss: 0.053
[ 53 / 64 ] loss: 0.210
[ 54 / 64 ] loss: 0.157
[ 55 / 64 ] loss: 0.105
[ 56 / 64 ] loss: 0.190
[ 57 / 64 ] loss: 0.258
[ 58 / 64 ] loss: 0.071
[ 59 / 64 ] loss: 0.300
[ 60 / 64 ] loss: 0.348
[ 61 / 64 ] loss: 0.251
[ 62 / 64 ] loss: 0.118
[ 63 / 64 ] loss: 0.173
[ 64 / 64 ] loss: 0.034
0.19357936910819262
Accuracy: 0.875332 -- Precision: 0.882883 -- Recall: 0.903226 -- F1: 0.892938 -- AUC: 0.893390
========= epoch: 41 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.358
[ 2 / 64 ] loss: 0.050
[ 3 / 64 ] loss: 0.265
[ 4 / 64 ] loss: 0.175
[ 5 / 64 ] loss: 0.177
[ 6 / 64 ] loss: 0.253
[ 7 / 64 ] loss: 0.218
[ 8 / 64 ] loss: 0.301
[ 9 / 64 ] loss: 0.123
[ 10 / 64 ] loss: 0.393
[ 11 / 64 ] loss: 0.380
[ 12 / 64 ] loss: 0.235
[ 13 / 64 ] loss: 0.232
[ 14 / 64 ] loss: 0.091
[ 15 / 64 ] loss: 0.174
[ 16 / 64 ] loss: 0.184
[ 17 / 64 ] loss: 0.175
[ 18 / 64 ] loss: 0.055
[ 19 / 64 ] loss: 0.169
[ 20 / 64 ] loss: 0.052
[ 21 / 64 ] loss: 0.110
[ 22 / 64 ] loss: 0.074
[ 23 / 64 ] loss: 0.080
[ 24 / 64 ] loss: 0.203
[ 25 / 64 ] loss: 0.219
[ 26 / 64 ] loss: 0.258
[ 27 / 64 ] loss: 0.057
[ 28 / 64 ] loss: 0.176
[ 29 / 64 ] loss: 0.123
[ 30 / 64 ] loss: 0.114
[ 31 / 64 ] loss: 0.103
[ 32 / 64 ] loss: 0.301
[ 33 / 64 ] loss: 0.120
[ 34 / 64 ] loss: 0.182
[ 35 / 64 ] loss: 0.155
[ 36 / 64 ] loss: 0.040
[ 37 / 64 ] loss: 0.480
[ 38 / 64 ] loss: 0.134
[ 39 / 64 ] loss: 0.059
[ 40 / 64 ] loss: 0.309
[ 41 / 64 ] loss: 0.292
[ 42 / 64 ] loss: 0.166
[ 43 / 64 ] loss: 0.174
[ 44 / 64 ] loss: 0.068
[ 45 / 64 ] loss: 0.188
[ 46 / 64 ] loss: 0.426
[ 47 / 64 ] loss: 0.242
[ 48 / 64 ] loss: 0.424
[ 49 / 64 ] loss: 0.067
[ 50 / 64 ] loss: 0.075
[ 51 / 64 ] loss: 0.411
[ 52 / 64 ] loss: 0.065
[ 53 / 64 ] loss: 0.088
[ 54 / 64 ] loss: 0.065
[ 55 / 64 ] loss: 0.215
[ 56 / 64 ] loss: 0.061
[ 57 / 64 ] loss: 0.040
[ 58 / 64 ] loss: 0.167
[ 59 / 64 ] loss: 0.339
[ 60 / 64 ] loss: 0.050
[ 61 / 64 ] loss: 0.232
[ 62 / 64 ] loss: 0.192
[ 63 / 64 ] loss: 0.056
[ 64 / 64 ] loss: 0.211
0.18233346688793972
Accuracy: 0.856764 -- Precision: 0.838174 -- Recall: 0.930876 -- F1: 0.882096 -- AUC: 0.887097
========= epoch: 42 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.036
[ 2 / 64 ] loss: 0.328
[ 3 / 64 ] loss: 0.237
[ 4 / 64 ] loss: 0.066
[ 5 / 64 ] loss: 0.297
[ 6 / 64 ] loss: 0.193
[ 7 / 64 ] loss: 0.185
[ 8 / 64 ] loss: 0.576
[ 9 / 64 ] loss: 0.322
[ 10 / 64 ] loss: 0.390
[ 11 / 64 ] loss: 0.323
[ 12 / 64 ] loss: 0.343
[ 13 / 64 ] loss: 0.069
[ 14 / 64 ] loss: 0.309
[ 15 / 64 ] loss: 0.148
[ 16 / 64 ] loss: 0.288
[ 17 / 64 ] loss: 0.153
[ 18 / 64 ] loss: 0.261
[ 19 / 64 ] loss: 0.290
[ 20 / 64 ] loss: 0.418
[ 21 / 64 ] loss: 0.315
[ 22 / 64 ] loss: 0.402
[ 23 / 64 ] loss: 0.291
[ 24 / 64 ] loss: 0.447
[ 25 / 64 ] loss: 0.225
[ 26 / 64 ] loss: 0.094
[ 27 / 64 ] loss: 0.242
[ 28 / 64 ] loss: 0.199
[ 29 / 64 ] loss: 0.151
[ 30 / 64 ] loss: 0.217
[ 31 / 64 ] loss: 0.072
[ 32 / 64 ] loss: 0.385
[ 33 / 64 ] loss: 0.337
[ 34 / 64 ] loss: 0.201
[ 35 / 64 ] loss: 0.157
[ 36 / 64 ] loss: 0.166
[ 37 / 64 ] loss: 0.066
[ 38 / 64 ] loss: 0.270
[ 39 / 64 ] loss: 0.500
[ 40 / 64 ] loss: 0.168
[ 41 / 64 ] loss: 0.272
[ 42 / 64 ] loss: 0.167
[ 43 / 64 ] loss: 0.369
[ 44 / 64 ] loss: 0.070
[ 45 / 64 ] loss: 0.311
[ 46 / 64 ] loss: 0.273
[ 47 / 64 ] loss: 0.169
[ 48 / 64 ] loss: 0.202
[ 49 / 64 ] loss: 0.223
[ 50 / 64 ] loss: 0.058
[ 51 / 64 ] loss: 0.352
[ 52 / 64 ] loss: 0.167
[ 53 / 64 ] loss: 0.304
[ 54 / 64 ] loss: 0.408
[ 55 / 64 ] loss: 0.299
[ 56 / 64 ] loss: 0.282
[ 57 / 64 ] loss: 0.519
[ 58 / 64 ] loss: 0.361
[ 59 / 64 ] loss: 0.201
[ 60 / 64 ] loss: 0.062
[ 61 / 64 ] loss: 0.395
[ 62 / 64 ] loss: 0.271
[ 63 / 64 ] loss: 0.324
[ 64 / 64 ] loss: 0.068
0.2540925500798039
Accuracy: 0.870027 -- Precision: 0.862069 -- Recall: 0.921659 -- F1: 0.890869 -- AUC: 0.895060
========= epoch: 43 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.092
[ 2 / 64 ] loss: 0.164
[ 3 / 64 ] loss: 0.188
[ 4 / 64 ] loss: 0.196
[ 5 / 64 ] loss: 0.280
[ 6 / 64 ] loss: 0.186
[ 7 / 64 ] loss: 0.091
[ 8 / 64 ] loss: 0.063
[ 9 / 64 ] loss: 0.295
[ 10 / 64 ] loss: 0.290
[ 11 / 64 ] loss: 0.051
[ 12 / 64 ] loss: 0.575
[ 13 / 64 ] loss: 0.521
[ 14 / 64 ] loss: 0.314
[ 15 / 64 ] loss: 0.181
[ 16 / 64 ] loss: 0.398
[ 17 / 64 ] loss: 0.296
[ 18 / 64 ] loss: 0.166
[ 19 / 64 ] loss: 0.191
[ 20 / 64 ] loss: 0.183
[ 21 / 64 ] loss: 0.282
[ 22 / 64 ] loss: 0.434
[ 23 / 64 ] loss: 0.367
[ 24 / 64 ] loss: 0.172
[ 25 / 64 ] loss: 0.346
[ 26 / 64 ] loss: 0.309
[ 27 / 64 ] loss: 0.571
[ 28 / 64 ] loss: 0.433
[ 29 / 64 ] loss: 0.402
[ 30 / 64 ] loss: 0.098
[ 31 / 64 ] loss: 0.216
[ 32 / 64 ] loss: 0.509
[ 33 / 64 ] loss: 0.185
[ 34 / 64 ] loss: 0.330
[ 35 / 64 ] loss: 0.177
[ 36 / 64 ] loss: 0.133
[ 37 / 64 ] loss: 0.344
[ 38 / 64 ] loss: 0.230
[ 39 / 64 ] loss: 0.209
[ 40 / 64 ] loss: 0.320
[ 41 / 64 ] loss: 0.177
[ 42 / 64 ] loss: 0.184
[ 43 / 64 ] loss: 0.093
[ 44 / 64 ] loss: 0.216
[ 45 / 64 ] loss: 0.275
[ 46 / 64 ] loss: 0.460
[ 47 / 64 ] loss: 0.264
[ 48 / 64 ] loss: 0.189
[ 49 / 64 ] loss: 0.306
[ 50 / 64 ] loss: 0.055
[ 51 / 64 ] loss: 0.080
[ 52 / 64 ] loss: 0.384
[ 53 / 64 ] loss: 0.450
[ 54 / 64 ] loss: 0.312
[ 55 / 64 ] loss: 0.184
[ 56 / 64 ] loss: 0.097
[ 57 / 64 ] loss: 0.323
[ 58 / 64 ] loss: 0.384
[ 59 / 64 ] loss: 0.379
[ 60 / 64 ] loss: 0.420
[ 61 / 64 ] loss: 0.182
[ 62 / 64 ] loss: 0.108
[ 63 / 64 ] loss: 0.452
[ 64 / 64 ] loss: 0.178
0.26468078239122406
Accuracy: 0.862069 -- Precision: 0.823529 -- Recall: 0.967742 -- F1: 0.889831 -- AUC: 0.897897
========= epoch: 44 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.304
[ 2 / 64 ] loss: 0.228
[ 3 / 64 ] loss: 0.214
[ 4 / 64 ] loss: 0.128
[ 5 / 64 ] loss: 0.448
[ 6 / 64 ] loss: 0.078
[ 7 / 64 ] loss: 0.181
[ 8 / 64 ] loss: 0.258
[ 9 / 64 ] loss: 0.221
[ 10 / 64 ] loss: 0.200
[ 11 / 64 ] loss: 0.160
[ 12 / 64 ] loss: 0.199
[ 13 / 64 ] loss: 0.174
[ 14 / 64 ] loss: 0.322
[ 15 / 64 ] loss: 0.300
[ 16 / 64 ] loss: 0.278
[ 17 / 64 ] loss: 0.110
[ 18 / 64 ] loss: 0.104
[ 19 / 64 ] loss: 0.079
[ 20 / 64 ] loss: 0.205
[ 21 / 64 ] loss: 0.249
[ 22 / 64 ] loss: 0.358
[ 23 / 64 ] loss: 0.200
[ 24 / 64 ] loss: 0.343
[ 25 / 64 ] loss: 0.060
[ 26 / 64 ] loss: 0.695
[ 27 / 64 ] loss: 0.299
[ 28 / 64 ] loss: 0.043
[ 29 / 64 ] loss: 0.351
[ 30 / 64 ] loss: 0.316
[ 31 / 64 ] loss: 0.175
[ 32 / 64 ] loss: 0.063
[ 33 / 64 ] loss: 0.048
[ 34 / 64 ] loss: 0.173
[ 35 / 64 ] loss: 0.414
[ 36 / 64 ] loss: 0.232
[ 37 / 64 ] loss: 0.256
[ 38 / 64 ] loss: 0.140
[ 39 / 64 ] loss: 0.326
[ 40 / 64 ] loss: 0.064
[ 41 / 64 ] loss: 0.149
[ 42 / 64 ] loss: 0.217
[ 43 / 64 ] loss: 0.281
[ 44 / 64 ] loss: 0.530
[ 45 / 64 ] loss: 0.065
[ 46 / 64 ] loss: 0.172
[ 47 / 64 ] loss: 0.384
[ 48 / 64 ] loss: 0.320
[ 49 / 64 ] loss: 0.429
[ 50 / 64 ] loss: 0.105
[ 51 / 64 ] loss: 0.165
[ 52 / 64 ] loss: 0.205
[ 53 / 64 ] loss: 0.266
[ 54 / 64 ] loss: 0.301
[ 55 / 64 ] loss: 0.125
[ 56 / 64 ] loss: 0.246
[ 57 / 64 ] loss: 0.165
[ 58 / 64 ] loss: 0.185
[ 59 / 64 ] loss: 0.251
[ 60 / 64 ] loss: 0.293
[ 61 / 64 ] loss: 0.293
[ 62 / 64 ] loss: 0.125
[ 63 / 64 ] loss: 0.307
[ 64 / 64 ] loss: 0.070
0.22880374285159633
Accuracy: 0.862069 -- Precision: 0.845188 -- Recall: 0.930876 -- F1: 0.885965 -- AUC: 0.888407
========= epoch: 45 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.171
[ 2 / 64 ] loss: 0.055
[ 3 / 64 ] loss: 0.416
[ 4 / 64 ] loss: 0.178
[ 5 / 64 ] loss: 0.055
[ 6 / 64 ] loss: 0.413
[ 7 / 64 ] loss: 0.313
[ 8 / 64 ] loss: 0.164
[ 9 / 64 ] loss: 0.053
[ 10 / 64 ] loss: 0.208
[ 11 / 64 ] loss: 0.192
[ 12 / 64 ] loss: 0.163
[ 13 / 64 ] loss: 0.252
[ 14 / 64 ] loss: 0.288
[ 15 / 64 ] loss: 0.071
[ 16 / 64 ] loss: 0.769
[ 17 / 64 ] loss: 0.262
[ 18 / 64 ] loss: 0.073
[ 19 / 64 ] loss: 0.176
[ 20 / 64 ] loss: 0.060
[ 21 / 64 ] loss: 0.181
[ 22 / 64 ] loss: 0.182
[ 23 / 64 ] loss: 0.172
[ 24 / 64 ] loss: 0.357
[ 25 / 64 ] loss: 0.058
[ 26 / 64 ] loss: 0.181
[ 27 / 64 ] loss: 0.072
[ 28 / 64 ] loss: 0.051
[ 29 / 64 ] loss: 0.167
[ 30 / 64 ] loss: 0.108
[ 31 / 64 ] loss: 0.477
[ 32 / 64 ] loss: 0.379
[ 33 / 64 ] loss: 0.054
[ 34 / 64 ] loss: 0.325
[ 35 / 64 ] loss: 0.248
[ 36 / 64 ] loss: 0.426
[ 37 / 64 ] loss: 0.276
[ 38 / 64 ] loss: 0.073
[ 39 / 64 ] loss: 0.166
[ 40 / 64 ] loss: 0.334
[ 41 / 64 ] loss: 0.535
[ 42 / 64 ] loss: 0.043
[ 43 / 64 ] loss: 0.078
[ 44 / 64 ] loss: 0.266
[ 45 / 64 ] loss: 0.253
[ 46 / 64 ] loss: 0.346
[ 47 / 64 ] loss: 0.114
[ 48 / 64 ] loss: 0.264
[ 49 / 64 ] loss: 0.162
[ 50 / 64 ] loss: 0.057
[ 51 / 64 ] loss: 0.139
[ 52 / 64 ] loss: 0.056
[ 53 / 64 ] loss: 0.166
[ 54 / 64 ] loss: 0.267
[ 55 / 64 ] loss: 0.167
[ 56 / 64 ] loss: 0.181
[ 57 / 64 ] loss: 0.056
[ 58 / 64 ] loss: 0.211
[ 59 / 64 ] loss: 0.097
[ 60 / 64 ] loss: 0.062
[ 61 / 64 ] loss: 0.253
[ 62 / 64 ] loss: 0.139
[ 63 / 64 ] loss: 0.040
[ 64 / 64 ] loss: 0.059
0.1973339316318743
Accuracy: 0.867374 -- Precision: 0.864629 -- Recall: 0.912442 -- F1: 0.887892 -- AUC: 0.885124
========= epoch: 46 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.321
[ 2 / 64 ] loss: 0.297
[ 3 / 64 ] loss: 0.197
[ 4 / 64 ] loss: 0.159
[ 5 / 64 ] loss: 0.046
[ 6 / 64 ] loss: 0.309
[ 7 / 64 ] loss: 0.351
[ 8 / 64 ] loss: 0.165
[ 9 / 64 ] loss: 0.166
[ 10 / 64 ] loss: 0.181
[ 11 / 64 ] loss: 0.053
[ 12 / 64 ] loss: 0.259
[ 13 / 64 ] loss: 0.047
[ 14 / 64 ] loss: 0.310
[ 15 / 64 ] loss: 0.043
[ 16 / 64 ] loss: 0.056
[ 17 / 64 ] loss: 0.047
[ 18 / 64 ] loss: 0.455
[ 19 / 64 ] loss: 0.145
[ 20 / 64 ] loss: 0.097
[ 21 / 64 ] loss: 0.190
[ 22 / 64 ] loss: 0.039
[ 23 / 64 ] loss: 0.423
[ 24 / 64 ] loss: 0.394
[ 25 / 64 ] loss: 0.335
[ 26 / 64 ] loss: 0.191
[ 27 / 64 ] loss: 0.616
[ 28 / 64 ] loss: 0.044
[ 29 / 64 ] loss: 0.174
[ 30 / 64 ] loss: 0.171
[ 31 / 64 ] loss: 0.127
[ 32 / 64 ] loss: 0.057
[ 33 / 64 ] loss: 0.533
[ 34 / 64 ] loss: 0.334
[ 35 / 64 ] loss: 0.043
[ 36 / 64 ] loss: 0.182
[ 37 / 64 ] loss: 0.165
[ 38 / 64 ] loss: 0.412
[ 39 / 64 ] loss: 0.168
[ 40 / 64 ] loss: 0.182
[ 41 / 64 ] loss: 0.165
[ 42 / 64 ] loss: 0.114
[ 43 / 64 ] loss: 0.236
[ 44 / 64 ] loss: 0.160
[ 45 / 64 ] loss: 0.047
[ 46 / 64 ] loss: 0.416
[ 47 / 64 ] loss: 0.242
[ 48 / 64 ] loss: 0.296
[ 49 / 64 ] loss: 0.176
[ 50 / 64 ] loss: 0.276
[ 51 / 64 ] loss: 0.200
[ 52 / 64 ] loss: 0.162
[ 53 / 64 ] loss: 0.055
[ 54 / 64 ] loss: 0.082
[ 55 / 64 ] loss: 0.057
[ 56 / 64 ] loss: 0.191
[ 57 / 64 ] loss: 0.296
[ 58 / 64 ] loss: 0.171
[ 59 / 64 ] loss: 0.166
[ 60 / 64 ] loss: 0.207
[ 61 / 64 ] loss: 0.270
[ 62 / 64 ] loss: 0.193
[ 63 / 64 ] loss: 0.335
[ 64 / 64 ] loss: 0.063
0.204057986731641
Accuracy: 0.859416 -- Precision: 0.847458 -- Recall: 0.921659 -- F1: 0.883002 -- AUC: 0.896025
========= epoch: 47 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.175
[ 2 / 64 ] loss: 0.146
[ 3 / 64 ] loss: 0.060
[ 4 / 64 ] loss: 0.256
[ 5 / 64 ] loss: 0.516
[ 6 / 64 ] loss: 0.210
[ 7 / 64 ] loss: 0.208
[ 8 / 64 ] loss: 0.311
[ 9 / 64 ] loss: 0.287
[ 10 / 64 ] loss: 0.395
[ 11 / 64 ] loss: 0.159
[ 12 / 64 ] loss: 0.195
[ 13 / 64 ] loss: 0.164
[ 14 / 64 ] loss: 0.051
[ 15 / 64 ] loss: 0.060
[ 16 / 64 ] loss: 0.144
[ 17 / 64 ] loss: 0.542
[ 18 / 64 ] loss: 0.318
[ 19 / 64 ] loss: 0.278
[ 20 / 64 ] loss: 0.185
[ 21 / 64 ] loss: 0.315
[ 22 / 64 ] loss: 0.061
[ 23 / 64 ] loss: 0.290
[ 24 / 64 ] loss: 0.283
[ 25 / 64 ] loss: 0.319
[ 26 / 64 ] loss: 0.140
[ 27 / 64 ] loss: 0.195
[ 28 / 64 ] loss: 0.286
[ 29 / 64 ] loss: 0.175
[ 30 / 64 ] loss: 0.207
[ 31 / 64 ] loss: 0.058
[ 32 / 64 ] loss: 0.342
[ 33 / 64 ] loss: 0.165
[ 34 / 64 ] loss: 0.080
[ 35 / 64 ] loss: 0.299
[ 36 / 64 ] loss: 0.168
[ 37 / 64 ] loss: 0.058
[ 38 / 64 ] loss: 0.185
[ 39 / 64 ] loss: 0.057
[ 40 / 64 ] loss: 0.082
[ 41 / 64 ] loss: 0.066
[ 42 / 64 ] loss: 0.111
[ 43 / 64 ] loss: 0.313
[ 44 / 64 ] loss: 0.122
[ 45 / 64 ] loss: 0.188
[ 46 / 64 ] loss: 0.292
[ 47 / 64 ] loss: 0.357
[ 48 / 64 ] loss: 0.307
[ 49 / 64 ] loss: 0.067
[ 50 / 64 ] loss: 0.052
[ 51 / 64 ] loss: 0.238
[ 52 / 64 ] loss: 0.044
[ 53 / 64 ] loss: 0.368
[ 54 / 64 ] loss: 0.158
[ 55 / 64 ] loss: 0.204
[ 56 / 64 ] loss: 0.217
[ 57 / 64 ] loss: 0.061
[ 58 / 64 ] loss: 0.142
[ 59 / 64 ] loss: 0.072
[ 60 / 64 ] loss: 0.169
[ 61 / 64 ] loss: 0.044
[ 62 / 64 ] loss: 0.118
[ 63 / 64 ] loss: 0.190
[ 64 / 64 ] loss: 0.043
0.1932783339289017
Accuracy: 0.872679 -- Precision: 0.859574 -- Recall: 0.930876 -- F1: 0.893805 -- AUC: 0.908194
========= epoch: 48 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.083
[ 2 / 64 ] loss: 0.507
[ 3 / 64 ] loss: 0.288
[ 4 / 64 ] loss: 0.404
[ 5 / 64 ] loss: 0.313
[ 6 / 64 ] loss: 0.271
[ 7 / 64 ] loss: 0.037
[ 8 / 64 ] loss: 0.183
[ 9 / 64 ] loss: 0.196
[ 10 / 64 ] loss: 0.175
[ 11 / 64 ] loss: 0.120
[ 12 / 64 ] loss: 0.377
[ 13 / 64 ] loss: 0.048
[ 14 / 64 ] loss: 0.159
[ 15 / 64 ] loss: 0.063
[ 16 / 64 ] loss: 0.228
[ 17 / 64 ] loss: 0.109
[ 18 / 64 ] loss: 0.123
[ 19 / 64 ] loss: 0.224
[ 20 / 64 ] loss: 0.186
[ 21 / 64 ] loss: 0.076
[ 22 / 64 ] loss: 0.096
[ 23 / 64 ] loss: 0.107
[ 24 / 64 ] loss: 0.221
[ 25 / 64 ] loss: 0.040
[ 26 / 64 ] loss: 0.137
[ 27 / 64 ] loss: 0.305
[ 28 / 64 ] loss: 0.046
[ 29 / 64 ] loss: 0.047
[ 30 / 64 ] loss: 0.039
[ 31 / 64 ] loss: 0.347
[ 32 / 64 ] loss: 0.195
[ 33 / 64 ] loss: 0.291
[ 34 / 64 ] loss: 0.123
[ 35 / 64 ] loss: 0.171
[ 36 / 64 ] loss: 0.190
[ 37 / 64 ] loss: 0.176
[ 38 / 64 ] loss: 0.440
[ 39 / 64 ] loss: 0.037
[ 40 / 64 ] loss: 0.068
[ 41 / 64 ] loss: 0.050
[ 42 / 64 ] loss: 0.334
[ 43 / 64 ] loss: 0.167
[ 44 / 64 ] loss: 0.171
[ 45 / 64 ] loss: 0.185
[ 46 / 64 ] loss: 0.175
[ 47 / 64 ] loss: 0.172
[ 48 / 64 ] loss: 0.044
[ 49 / 64 ] loss: 0.189
[ 50 / 64 ] loss: 0.215
[ 51 / 64 ] loss: 0.133
[ 52 / 64 ] loss: 0.174
[ 53 / 64 ] loss: 0.190
[ 54 / 64 ] loss: 0.044
[ 55 / 64 ] loss: 0.326
[ 56 / 64 ] loss: 0.063
[ 57 / 64 ] loss: 0.062
[ 58 / 64 ] loss: 0.037
[ 59 / 64 ] loss: 0.175
[ 60 / 64 ] loss: 0.046
[ 61 / 64 ] loss: 0.230
[ 62 / 64 ] loss: 0.369
[ 63 / 64 ] loss: 0.032
[ 64 / 64 ] loss: 0.032
0.16972368577262387
Accuracy: 0.862069 -- Precision: 0.866667 -- Recall: 0.898618 -- F1: 0.882353 -- AUC: 0.896731
========= epoch: 49 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.212
[ 2 / 64 ] loss: 0.050
[ 3 / 64 ] loss: 0.503
[ 4 / 64 ] loss: 0.036
[ 5 / 64 ] loss: 0.036
[ 6 / 64 ] loss: 0.184
[ 7 / 64 ] loss: 0.199
[ 8 / 64 ] loss: 0.353
[ 9 / 64 ] loss: 0.050
[ 10 / 64 ] loss: 0.193
[ 11 / 64 ] loss: 0.336
[ 12 / 64 ] loss: 0.168
[ 13 / 64 ] loss: 0.298
[ 14 / 64 ] loss: 0.298
[ 15 / 64 ] loss: 0.033
[ 16 / 64 ] loss: 0.165
[ 17 / 64 ] loss: 0.298
[ 18 / 64 ] loss: 0.064
[ 19 / 64 ] loss: 0.040
[ 20 / 64 ] loss: 0.042
[ 21 / 64 ] loss: 0.197
[ 22 / 64 ] loss: 0.195
[ 23 / 64 ] loss: 0.161
[ 24 / 64 ] loss: 0.051
[ 25 / 64 ] loss: 0.171
[ 26 / 64 ] loss: 0.166
[ 27 / 64 ] loss: 0.230
[ 28 / 64 ] loss: 0.218
[ 29 / 64 ] loss: 0.035
[ 30 / 64 ] loss: 0.037
[ 31 / 64 ] loss: 0.035
[ 32 / 64 ] loss: 0.215
[ 33 / 64 ] loss: 0.198
[ 34 / 64 ] loss: 0.425
[ 35 / 64 ] loss: 0.166
[ 36 / 64 ] loss: 0.162
[ 37 / 64 ] loss: 0.056
[ 38 / 64 ] loss: 0.159
[ 39 / 64 ] loss: 0.035
[ 40 / 64 ] loss: 0.056
[ 41 / 64 ] loss: 0.305
[ 42 / 64 ] loss: 0.197
[ 43 / 64 ] loss: 0.282
[ 44 / 64 ] loss: 0.292
[ 45 / 64 ] loss: 0.162
[ 46 / 64 ] loss: 0.139
[ 47 / 64 ] loss: 0.168
[ 48 / 64 ] loss: 0.279
[ 49 / 64 ] loss: 0.396
[ 50 / 64 ] loss: 0.320
[ 51 / 64 ] loss: 0.160
[ 52 / 64 ] loss: 0.112
[ 53 / 64 ] loss: 0.472
[ 54 / 64 ] loss: 0.316
[ 55 / 64 ] loss: 0.236
[ 56 / 64 ] loss: 0.070
[ 57 / 64 ] loss: 0.047
[ 58 / 64 ] loss: 0.440
[ 59 / 64 ] loss: 0.180
[ 60 / 64 ] loss: 0.045
[ 61 / 64 ] loss: 0.057
[ 62 / 64 ] loss: 0.052
[ 63 / 64 ] loss: 0.339
[ 64 / 64 ] loss: 0.053
0.1819412417244166
Accuracy: 0.862069 -- Precision: 0.863436 -- Recall: 0.903226 -- F1: 0.882883 -- AUC: 0.900662
========= epoch: 50 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 64 ] loss: 0.320
[ 2 / 64 ] loss: 0.052
[ 3 / 64 ] loss: 0.052
[ 4 / 64 ] loss: 0.285
[ 5 / 64 ] loss: 0.244
[ 6 / 64 ] loss: 0.052
[ 7 / 64 ] loss: 0.048
[ 8 / 64 ] loss: 0.173
[ 9 / 64 ] loss: 0.204
[ 10 / 64 ] loss: 0.045
[ 11 / 64 ] loss: 0.061
[ 12 / 64 ] loss: 0.144
[ 13 / 64 ] loss: 0.041
[ 14 / 64 ] loss: 0.058
[ 15 / 64 ] loss: 0.176
[ 16 / 64 ] loss: 0.090
[ 17 / 64 ] loss: 0.045
[ 18 / 64 ] loss: 0.176
[ 19 / 64 ] loss: 0.041
[ 20 / 64 ] loss: 0.322
[ 21 / 64 ] loss: 0.039
[ 22 / 64 ] loss: 0.315
[ 23 / 64 ] loss: 0.122
[ 24 / 64 ] loss: 0.042
[ 25 / 64 ] loss: 0.315
[ 26 / 64 ] loss: 0.265
[ 27 / 64 ] loss: 0.118
[ 28 / 64 ] loss: 0.455
[ 29 / 64 ] loss: 0.297
[ 30 / 64 ] loss: 0.187
[ 31 / 64 ] loss: 0.157
[ 32 / 64 ] loss: 0.459
[ 33 / 64 ] loss: 0.269
[ 34 / 64 ] loss: 0.341
[ 35 / 64 ] loss: 0.302
[ 36 / 64 ] loss: 0.146
[ 37 / 64 ] loss: 0.176
[ 38 / 64 ] loss: 0.235
[ 39 / 64 ] loss: 0.130
[ 40 / 64 ] loss: 0.062
[ 41 / 64 ] loss: 0.184
[ 42 / 64 ] loss: 0.042
[ 43 / 64 ] loss: 0.155
[ 44 / 64 ] loss: 0.187
[ 45 / 64 ] loss: 0.262
[ 46 / 64 ] loss: 0.050
[ 47 / 64 ] loss: 0.048
[ 48 / 64 ] loss: 0.087
[ 49 / 64 ] loss: 0.105
[ 50 / 64 ] loss: 0.207
[ 51 / 64 ] loss: 0.494
[ 52 / 64 ] loss: 0.057
[ 53 / 64 ] loss: 0.204
[ 54 / 64 ] loss: 0.188
[ 55 / 64 ] loss: 0.039
[ 56 / 64 ] loss: 0.296
[ 57 / 64 ] loss: 0.250
[ 58 / 64 ] loss: 0.039
[ 59 / 64 ] loss: 0.198
[ 60 / 64 ] loss: 0.085
[ 61 / 64 ] loss: 0.056
[ 62 / 64 ] loss: 0.171
[ 63 / 64 ] loss: 0.081
[ 64 / 64 ] loss: 0.041
0.1653562985593453
Accuracy: 0.867374 -- Precision: 0.852321 -- Recall: 0.930876 -- F1: 0.889868 -- AUC: 0.906927
