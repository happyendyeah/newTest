nohup: ignoring input
训练集: 1515
测试集: 377
加载further pretrained模型成功
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 1 ==============
Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.695
[ 2 / 95 ] loss: 0.699
[ 3 / 95 ] loss: 0.708
[ 4 / 95 ] loss: 0.690
[ 5 / 95 ] loss: 0.695
[ 6 / 95 ] loss: 0.687
[ 7 / 95 ] loss: 0.682
[ 8 / 95 ] loss: 0.670
[ 9 / 95 ] loss: 0.661
[ 10 / 95 ] loss: 0.675
[ 11 / 95 ] loss: 0.642
[ 12 / 95 ] loss: 0.675
[ 13 / 95 ] loss: 0.652
[ 14 / 95 ] loss: 0.641
[ 15 / 95 ] loss: 0.671
[ 16 / 95 ] loss: 0.605
[ 17 / 95 ] loss: 0.731
[ 18 / 95 ] loss: 0.670
[ 19 / 95 ] loss: 0.628
[ 20 / 95 ] loss: 0.666
[ 21 / 95 ] loss: 0.531
[ 22 / 95 ] loss: 0.660
[ 23 / 95 ] loss: 0.607
[ 24 / 95 ] loss: 0.554
[ 25 / 95 ] loss: 0.569
[ 26 / 95 ] loss: 0.590
[ 27 / 95 ] loss: 0.638
[ 28 / 95 ] loss: 0.712
[ 29 / 95 ] loss: 0.707
[ 30 / 95 ] loss: 0.585
[ 31 / 95 ] loss: 0.604
[ 32 / 95 ] loss: 0.571
[ 33 / 95 ] loss: 0.593
[ 34 / 95 ] loss: 0.509
[ 35 / 95 ] loss: 0.593
[ 36 / 95 ] loss: 0.645
[ 37 / 95 ] loss: 0.624
[ 38 / 95 ] loss: 0.775
[ 39 / 95 ] loss: 0.643
[ 40 / 95 ] loss: 0.635
[ 41 / 95 ] loss: 0.740
[ 42 / 95 ] loss: 0.576
[ 43 / 95 ] loss: 0.603
[ 44 / 95 ] loss: 0.562
[ 45 / 95 ] loss: 0.868
[ 46 / 95 ] loss: 0.546
[ 47 / 95 ] loss: 0.521
[ 48 / 95 ] loss: 0.557
[ 49 / 95 ] loss: 0.436
[ 50 / 95 ] loss: 0.593
[ 51 / 95 ] loss: 0.629
[ 52 / 95 ] loss: 0.555
[ 53 / 95 ] loss: 0.630
[ 54 / 95 ] loss: 0.719
[ 55 / 95 ] loss: 0.666
[ 56 / 95 ] loss: 0.635
[ 57 / 95 ] loss: 0.595
[ 58 / 95 ] loss: 0.600
[ 59 / 95 ] loss: 0.538
[ 60 / 95 ] loss: 0.501
[ 61 / 95 ] loss: 0.489
[ 62 / 95 ] loss: 0.503
[ 63 / 95 ] loss: 0.560
[ 64 / 95 ] loss: 0.578
[ 65 / 95 ] loss: 0.624
[ 66 / 95 ] loss: 0.449
[ 67 / 95 ] loss: 0.525
[ 68 / 95 ] loss: 0.615
[ 69 / 95 ] loss: 0.442
[ 70 / 95 ] loss: 0.569
[ 71 / 95 ] loss: 0.602
[ 72 / 95 ] loss: 0.439
[ 73 / 95 ] loss: 0.617
[ 74 / 95 ] loss: 0.463
[ 75 / 95 ] loss: 0.465
[ 76 / 95 ] loss: 0.522
[ 77 / 95 ] loss: 0.555
[ 78 / 95 ] loss: 0.665
[ 79 / 95 ] loss: 0.589
[ 80 / 95 ] loss: 0.485
[ 81 / 95 ] loss: 0.677
[ 82 / 95 ] loss: 0.557
[ 83 / 95 ] loss: 0.691
[ 84 / 95 ] loss: 0.736
[ 85 / 95 ] loss: 0.536
[ 86 / 95 ] loss: 0.569
[ 87 / 95 ] loss: 0.568
[ 88 / 95 ] loss: 0.532
[ 89 / 95 ] loss: 0.579
[ 90 / 95 ] loss: 0.472
[ 91 / 95 ] loss: 0.560
[ 92 / 95 ] loss: 0.455
[ 93 / 95 ] loss: 0.593
[ 94 / 95 ] loss: 0.573
[ 95 / 95 ] loss: 0.599
0.6032982923482594
Accuracy: 0.734748 -- Precision: 0.690554 -- Recall: 0.976959 -- F1: 0.809160 -- AUC: 0.779637
保存模型参数
========= epoch: 2 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.660
[ 2 / 95 ] loss: 0.491
[ 3 / 95 ] loss: 0.730
[ 4 / 95 ] loss: 0.516
[ 5 / 95 ] loss: 0.419
[ 6 / 95 ] loss: 0.581
[ 7 / 95 ] loss: 0.561
[ 8 / 95 ] loss: 0.590
[ 9 / 95 ] loss: 0.547
[ 10 / 95 ] loss: 0.412
[ 11 / 95 ] loss: 0.641
[ 12 / 95 ] loss: 0.760
[ 13 / 95 ] loss: 0.594
[ 14 / 95 ] loss: 0.514
[ 15 / 95 ] loss: 0.531
[ 16 / 95 ] loss: 0.631
[ 17 / 95 ] loss: 0.700
[ 18 / 95 ] loss: 0.857
[ 19 / 95 ] loss: 0.558
[ 20 / 95 ] loss: 0.697
[ 21 / 95 ] loss: 0.472
[ 22 / 95 ] loss: 0.566
[ 23 / 95 ] loss: 0.616
[ 24 / 95 ] loss: 0.662
[ 25 / 95 ] loss: 0.559
[ 26 / 95 ] loss: 0.386
[ 27 / 95 ] loss: 0.532
[ 28 / 95 ] loss: 0.427
[ 29 / 95 ] loss: 0.604
[ 30 / 95 ] loss: 0.645
[ 31 / 95 ] loss: 0.702
[ 32 / 95 ] loss: 0.572
[ 33 / 95 ] loss: 0.564
[ 34 / 95 ] loss: 0.486
[ 35 / 95 ] loss: 0.473
[ 36 / 95 ] loss: 0.482
[ 37 / 95 ] loss: 0.430
[ 38 / 95 ] loss: 0.775
[ 39 / 95 ] loss: 0.399
[ 40 / 95 ] loss: 0.621
[ 41 / 95 ] loss: 0.562
[ 42 / 95 ] loss: 0.716
[ 43 / 95 ] loss: 0.544
[ 44 / 95 ] loss: 0.848
[ 45 / 95 ] loss: 0.748
[ 46 / 95 ] loss: 0.478
[ 47 / 95 ] loss: 0.598
[ 48 / 95 ] loss: 0.616
[ 49 / 95 ] loss: 0.488
[ 50 / 95 ] loss: 0.434
[ 51 / 95 ] loss: 0.560
[ 52 / 95 ] loss: 0.600
[ 53 / 95 ] loss: 0.379
[ 54 / 95 ] loss: 0.570
[ 55 / 95 ] loss: 0.605
[ 56 / 95 ] loss: 0.534
[ 57 / 95 ] loss: 0.547
[ 58 / 95 ] loss: 0.514
[ 59 / 95 ] loss: 0.482
[ 60 / 95 ] loss: 0.640
[ 61 / 95 ] loss: 0.602
[ 62 / 95 ] loss: 0.693
[ 63 / 95 ] loss: 0.514
[ 64 / 95 ] loss: 0.529
[ 65 / 95 ] loss: 0.509
[ 66 / 95 ] loss: 0.327
[ 67 / 95 ] loss: 0.666
[ 68 / 95 ] loss: 0.489
[ 69 / 95 ] loss: 0.565
[ 70 / 95 ] loss: 0.349
[ 71 / 95 ] loss: 0.424
[ 72 / 95 ] loss: 0.529
[ 73 / 95 ] loss: 0.572
[ 74 / 95 ] loss: 0.674
[ 75 / 95 ] loss: 0.689
[ 76 / 95 ] loss: 0.451
[ 77 / 95 ] loss: 0.486
[ 78 / 95 ] loss: 0.589
[ 79 / 95 ] loss: 0.433
[ 80 / 95 ] loss: 0.603
[ 81 / 95 ] loss: 0.547
[ 82 / 95 ] loss: 0.575
[ 83 / 95 ] loss: 0.677
[ 84 / 95 ] loss: 0.479
[ 85 / 95 ] loss: 0.604
[ 86 / 95 ] loss: 0.589
[ 87 / 95 ] loss: 0.570
[ 88 / 95 ] loss: 0.620
[ 89 / 95 ] loss: 0.407
[ 90 / 95 ] loss: 0.307
[ 91 / 95 ] loss: 0.432
[ 92 / 95 ] loss: 0.566
[ 93 / 95 ] loss: 0.652
[ 94 / 95 ] loss: 0.348
[ 95 / 95 ] loss: 0.597
0.558844563521837
Accuracy: 0.761273 -- Precision: 0.772532 -- Recall: 0.829493 -- F1: 0.800000 -- AUC: 0.803283
保存模型参数
========= epoch: 3 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.616
[ 2 / 95 ] loss: 0.676
[ 3 / 95 ] loss: 0.469
[ 4 / 95 ] loss: 0.515
[ 5 / 95 ] loss: 0.572
[ 6 / 95 ] loss: 0.665
[ 7 / 95 ] loss: 0.643
[ 8 / 95 ] loss: 0.621
[ 9 / 95 ] loss: 0.592
[ 10 / 95 ] loss: 0.454
[ 11 / 95 ] loss: 0.613
[ 12 / 95 ] loss: 0.561
[ 13 / 95 ] loss: 0.430
[ 14 / 95 ] loss: 0.356
[ 15 / 95 ] loss: 0.364
[ 16 / 95 ] loss: 0.617
[ 17 / 95 ] loss: 0.644
[ 18 / 95 ] loss: 0.331
[ 19 / 95 ] loss: 0.288
[ 20 / 95 ] loss: 0.423
[ 21 / 95 ] loss: 0.521
[ 22 / 95 ] loss: 0.298
[ 23 / 95 ] loss: 0.375
[ 24 / 95 ] loss: 0.789
[ 25 / 95 ] loss: 0.493
[ 26 / 95 ] loss: 0.496
[ 27 / 95 ] loss: 0.441
[ 28 / 95 ] loss: 0.746
[ 29 / 95 ] loss: 0.428
[ 30 / 95 ] loss: 0.429
[ 31 / 95 ] loss: 0.320
[ 32 / 95 ] loss: 0.437
[ 33 / 95 ] loss: 0.637
[ 34 / 95 ] loss: 0.557
[ 35 / 95 ] loss: 0.431
[ 36 / 95 ] loss: 0.215
[ 37 / 95 ] loss: 0.537
[ 38 / 95 ] loss: 0.433
[ 39 / 95 ] loss: 0.510
[ 40 / 95 ] loss: 0.320
[ 41 / 95 ] loss: 0.544
[ 42 / 95 ] loss: 0.735
[ 43 / 95 ] loss: 0.364
[ 44 / 95 ] loss: 0.550
[ 45 / 95 ] loss: 0.391
[ 46 / 95 ] loss: 0.635
[ 47 / 95 ] loss: 0.560
[ 48 / 95 ] loss: 0.546
[ 49 / 95 ] loss: 0.381
[ 50 / 95 ] loss: 0.445
[ 51 / 95 ] loss: 0.592
[ 52 / 95 ] loss: 0.519
[ 53 / 95 ] loss: 0.824
[ 54 / 95 ] loss: 0.419
[ 55 / 95 ] loss: 0.304
[ 56 / 95 ] loss: 0.618
[ 57 / 95 ] loss: 0.306
[ 58 / 95 ] loss: 0.405
[ 59 / 95 ] loss: 0.408
[ 60 / 95 ] loss: 0.491
[ 61 / 95 ] loss: 0.613
[ 62 / 95 ] loss: 0.506
[ 63 / 95 ] loss: 0.760
[ 64 / 95 ] loss: 0.866
[ 65 / 95 ] loss: 0.618
[ 66 / 95 ] loss: 0.637
[ 67 / 95 ] loss: 0.748
[ 68 / 95 ] loss: 0.854
[ 69 / 95 ] loss: 0.439
[ 70 / 95 ] loss: 0.358
[ 71 / 95 ] loss: 0.342
[ 72 / 95 ] loss: 0.556
[ 73 / 95 ] loss: 0.417
[ 74 / 95 ] loss: 0.414
[ 75 / 95 ] loss: 0.622
[ 76 / 95 ] loss: 0.637
[ 77 / 95 ] loss: 0.447
[ 78 / 95 ] loss: 0.590
[ 79 / 95 ] loss: 0.599
[ 80 / 95 ] loss: 0.723
[ 81 / 95 ] loss: 0.470
[ 82 / 95 ] loss: 0.341
[ 83 / 95 ] loss: 0.465
[ 84 / 95 ] loss: 0.535
[ 85 / 95 ] loss: 0.483
[ 86 / 95 ] loss: 0.527
[ 87 / 95 ] loss: 0.543
[ 88 / 95 ] loss: 0.481
[ 89 / 95 ] loss: 0.487
[ 90 / 95 ] loss: 0.521
[ 91 / 95 ] loss: 0.431
[ 92 / 95 ] loss: 0.676
[ 93 / 95 ] loss: 0.449
[ 94 / 95 ] loss: 0.507
[ 95 / 95 ] loss: 0.735
0.5189268341189937
Accuracy: 0.790451 -- Precision: 0.802632 -- Recall: 0.843318 -- F1: 0.822472 -- AUC: 0.814113
保存模型参数
========= epoch: 4 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.497
[ 2 / 95 ] loss: 0.617
[ 3 / 95 ] loss: 0.538
[ 4 / 95 ] loss: 0.630
[ 5 / 95 ] loss: 0.468
[ 6 / 95 ] loss: 0.595
[ 7 / 95 ] loss: 0.352
[ 8 / 95 ] loss: 0.511
[ 9 / 95 ] loss: 0.489
[ 10 / 95 ] loss: 0.547
[ 11 / 95 ] loss: 0.386
[ 12 / 95 ] loss: 0.429
[ 13 / 95 ] loss: 0.500
[ 14 / 95 ] loss: 0.342
[ 15 / 95 ] loss: 0.486
[ 16 / 95 ] loss: 0.438
[ 17 / 95 ] loss: 0.576
[ 18 / 95 ] loss: 0.509
[ 19 / 95 ] loss: 0.326
[ 20 / 95 ] loss: 0.593
[ 21 / 95 ] loss: 0.377
[ 22 / 95 ] loss: 0.375
[ 23 / 95 ] loss: 0.407
[ 24 / 95 ] loss: 0.297
[ 25 / 95 ] loss: 0.657
[ 26 / 95 ] loss: 0.416
[ 27 / 95 ] loss: 0.624
[ 28 / 95 ] loss: 0.678
[ 29 / 95 ] loss: 0.210
[ 30 / 95 ] loss: 0.566
[ 31 / 95 ] loss: 0.627
[ 32 / 95 ] loss: 0.692
[ 33 / 95 ] loss: 0.417
[ 34 / 95 ] loss: 0.753
[ 35 / 95 ] loss: 0.479
[ 36 / 95 ] loss: 0.587
[ 37 / 95 ] loss: 0.598
[ 38 / 95 ] loss: 0.646
[ 39 / 95 ] loss: 0.642
[ 40 / 95 ] loss: 0.424
[ 41 / 95 ] loss: 0.529
[ 42 / 95 ] loss: 0.613
[ 43 / 95 ] loss: 0.557
[ 44 / 95 ] loss: 0.656
[ 45 / 95 ] loss: 0.496
[ 46 / 95 ] loss: 0.608
[ 47 / 95 ] loss: 0.469
[ 48 / 95 ] loss: 0.496
[ 49 / 95 ] loss: 0.445
[ 50 / 95 ] loss: 0.283
[ 51 / 95 ] loss: 0.410
[ 52 / 95 ] loss: 0.613
[ 53 / 95 ] loss: 0.717
[ 54 / 95 ] loss: 0.221
[ 55 / 95 ] loss: 0.326
[ 56 / 95 ] loss: 0.350
[ 57 / 95 ] loss: 0.375
[ 58 / 95 ] loss: 0.386
[ 59 / 95 ] loss: 0.259
[ 60 / 95 ] loss: 0.333
[ 61 / 95 ] loss: 0.626
[ 62 / 95 ] loss: 0.196
[ 63 / 95 ] loss: 0.403
[ 64 / 95 ] loss: 0.262
[ 65 / 95 ] loss: 0.639
[ 66 / 95 ] loss: 0.217
[ 67 / 95 ] loss: 0.515
[ 68 / 95 ] loss: 0.601
[ 69 / 95 ] loss: 0.721
[ 70 / 95 ] loss: 0.532
[ 71 / 95 ] loss: 0.702
[ 72 / 95 ] loss: 0.471
[ 73 / 95 ] loss: 0.430
[ 74 / 95 ] loss: 0.461
[ 75 / 95 ] loss: 0.494
[ 76 / 95 ] loss: 0.556
[ 77 / 95 ] loss: 0.624
[ 78 / 95 ] loss: 0.478
[ 79 / 95 ] loss: 0.454
[ 80 / 95 ] loss: 0.484
[ 81 / 95 ] loss: 0.280
[ 82 / 95 ] loss: 0.851
[ 83 / 95 ] loss: 0.961
[ 84 / 95 ] loss: 0.439
[ 85 / 95 ] loss: 0.531
[ 86 / 95 ] loss: 0.481
[ 87 / 95 ] loss: 0.492
[ 88 / 95 ] loss: 0.501
[ 89 / 95 ] loss: 0.474
[ 90 / 95 ] loss: 0.662
[ 91 / 95 ] loss: 0.774
[ 92 / 95 ] loss: 0.612
[ 93 / 95 ] loss: 0.573
[ 94 / 95 ] loss: 0.408
[ 95 / 95 ] loss: 0.384
0.5024551201807825
Accuracy: 0.758621 -- Precision: 0.769231 -- Recall: 0.829493 -- F1: 0.798226 -- AUC: 0.812846
========= epoch: 5 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.504
[ 2 / 95 ] loss: 0.527
[ 3 / 95 ] loss: 0.754
[ 4 / 95 ] loss: 0.468
[ 5 / 95 ] loss: 0.575
[ 6 / 95 ] loss: 0.363
[ 7 / 95 ] loss: 0.688
[ 8 / 95 ] loss: 0.553
[ 9 / 95 ] loss: 0.591
[ 10 / 95 ] loss: 0.487
[ 11 / 95 ] loss: 0.479
[ 12 / 95 ] loss: 0.324
[ 13 / 95 ] loss: 0.429
[ 14 / 95 ] loss: 0.549
[ 15 / 95 ] loss: 0.394
[ 16 / 95 ] loss: 0.536
[ 17 / 95 ] loss: 0.344
[ 18 / 95 ] loss: 0.249
[ 19 / 95 ] loss: 0.459
[ 20 / 95 ] loss: 0.518
[ 21 / 95 ] loss: 0.640
[ 22 / 95 ] loss: 0.267
[ 23 / 95 ] loss: 0.661
[ 24 / 95 ] loss: 0.784
[ 25 / 95 ] loss: 0.704
[ 26 / 95 ] loss: 0.714
[ 27 / 95 ] loss: 0.686
[ 28 / 95 ] loss: 0.983
[ 29 / 95 ] loss: 0.513
[ 30 / 95 ] loss: 0.491
[ 31 / 95 ] loss: 0.439
[ 32 / 95 ] loss: 0.445
[ 33 / 95 ] loss: 0.709
[ 34 / 95 ] loss: 0.844
[ 35 / 95 ] loss: 0.455
[ 36 / 95 ] loss: 0.335
[ 37 / 95 ] loss: 0.644
[ 38 / 95 ] loss: 0.509
[ 39 / 95 ] loss: 0.460
[ 40 / 95 ] loss: 0.329
[ 41 / 95 ] loss: 0.582
[ 42 / 95 ] loss: 0.390
[ 43 / 95 ] loss: 0.440
[ 44 / 95 ] loss: 0.398
[ 45 / 95 ] loss: 0.662
[ 46 / 95 ] loss: 0.407
[ 47 / 95 ] loss: 0.543
[ 48 / 95 ] loss: 0.196
[ 49 / 95 ] loss: 0.645
[ 50 / 95 ] loss: 0.520
[ 51 / 95 ] loss: 0.203
[ 52 / 95 ] loss: 0.662
[ 53 / 95 ] loss: 0.557
[ 54 / 95 ] loss: 0.643
[ 55 / 95 ] loss: 0.550
[ 56 / 95 ] loss: 0.318
[ 57 / 95 ] loss: 0.497
[ 58 / 95 ] loss: 0.391
[ 59 / 95 ] loss: 0.369
[ 60 / 95 ] loss: 0.417
[ 61 / 95 ] loss: 0.435
[ 62 / 95 ] loss: 0.296
[ 63 / 95 ] loss: 0.686
[ 64 / 95 ] loss: 0.367
[ 65 / 95 ] loss: 0.352
[ 66 / 95 ] loss: 0.389
[ 67 / 95 ] loss: 0.524
[ 68 / 95 ] loss: 0.401
[ 69 / 95 ] loss: 0.440
[ 70 / 95 ] loss: 0.400
[ 71 / 95 ] loss: 0.606
[ 72 / 95 ] loss: 0.541
[ 73 / 95 ] loss: 0.547
[ 74 / 95 ] loss: 0.522
[ 75 / 95 ] loss: 0.633
[ 76 / 95 ] loss: 0.488
[ 77 / 95 ] loss: 0.536
[ 78 / 95 ] loss: 0.620
[ 79 / 95 ] loss: 0.399
[ 80 / 95 ] loss: 0.491
[ 81 / 95 ] loss: 0.389
[ 82 / 95 ] loss: 0.423
[ 83 / 95 ] loss: 0.383
[ 84 / 95 ] loss: 0.586
[ 85 / 95 ] loss: 0.490
[ 86 / 95 ] loss: 0.430
[ 87 / 95 ] loss: 0.433
[ 88 / 95 ] loss: 0.749
[ 89 / 95 ] loss: 0.271
[ 90 / 95 ] loss: 0.617
[ 91 / 95 ] loss: 0.531
[ 92 / 95 ] loss: 0.419
[ 93 / 95 ] loss: 0.481
[ 94 / 95 ] loss: 0.300
[ 95 / 95 ] loss: 0.498
0.49966467085637545
Accuracy: 0.726790 -- Precision: 0.682692 -- Recall: 0.981567 -- F1: 0.805293 -- AUC: 0.858439
========= epoch: 6 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.372
[ 2 / 95 ] loss: 0.246
[ 3 / 95 ] loss: 0.551
[ 4 / 95 ] loss: 0.592
[ 5 / 95 ] loss: 0.348
[ 6 / 95 ] loss: 0.627
[ 7 / 95 ] loss: 0.626
[ 8 / 95 ] loss: 0.544
[ 9 / 95 ] loss: 0.800
[ 10 / 95 ] loss: 0.476
[ 11 / 95 ] loss: 1.047
[ 12 / 95 ] loss: 0.594
[ 13 / 95 ] loss: 0.504
[ 14 / 95 ] loss: 0.539
[ 15 / 95 ] loss: 0.276
[ 16 / 95 ] loss: 0.495
[ 17 / 95 ] loss: 0.488
[ 18 / 95 ] loss: 0.552
[ 19 / 95 ] loss: 0.442
[ 20 / 95 ] loss: 0.397
[ 21 / 95 ] loss: 0.440
[ 22 / 95 ] loss: 0.399
[ 23 / 95 ] loss: 0.719
[ 24 / 95 ] loss: 0.922
[ 25 / 95 ] loss: 0.819
[ 26 / 95 ] loss: 0.732
[ 27 / 95 ] loss: 0.600
[ 28 / 95 ] loss: 0.485
[ 29 / 95 ] loss: 0.522
[ 30 / 95 ] loss: 0.492
[ 31 / 95 ] loss: 0.461
[ 32 / 95 ] loss: 0.492
[ 33 / 95 ] loss: 0.591
[ 34 / 95 ] loss: 0.592
[ 35 / 95 ] loss: 0.287
[ 36 / 95 ] loss: 0.416
[ 37 / 95 ] loss: 0.536
[ 38 / 95 ] loss: 0.372
[ 39 / 95 ] loss: 0.482
[ 40 / 95 ] loss: 0.390
[ 41 / 95 ] loss: 0.270
[ 42 / 95 ] loss: 0.335
[ 43 / 95 ] loss: 0.407
[ 44 / 95 ] loss: 0.260
[ 45 / 95 ] loss: 0.387
[ 46 / 95 ] loss: 0.332
[ 47 / 95 ] loss: 0.607
[ 48 / 95 ] loss: 0.463
[ 49 / 95 ] loss: 0.384
[ 50 / 95 ] loss: 0.397
[ 51 / 95 ] loss: 0.321
[ 52 / 95 ] loss: 0.413
[ 53 / 95 ] loss: 0.466
[ 54 / 95 ] loss: 1.008
[ 55 / 95 ] loss: 0.567
[ 56 / 95 ] loss: 0.488
[ 57 / 95 ] loss: 0.479
[ 58 / 95 ] loss: 0.617
[ 59 / 95 ] loss: 0.397
[ 60 / 95 ] loss: 0.321
[ 61 / 95 ] loss: 0.587
[ 62 / 95 ] loss: 0.549
[ 63 / 95 ] loss: 0.744
[ 64 / 95 ] loss: 0.534
[ 65 / 95 ] loss: 0.627
[ 66 / 95 ] loss: 0.240
[ 67 / 95 ] loss: 0.717
[ 68 / 95 ] loss: 0.416
[ 69 / 95 ] loss: 0.748
[ 70 / 95 ] loss: 0.454
[ 71 / 95 ] loss: 0.624
[ 72 / 95 ] loss: 0.573
[ 73 / 95 ] loss: 0.713
[ 74 / 95 ] loss: 0.611
[ 75 / 95 ] loss: 0.676
[ 76 / 95 ] loss: 0.532
[ 77 / 95 ] loss: 0.536
[ 78 / 95 ] loss: 0.539
[ 79 / 95 ] loss: 0.488
[ 80 / 95 ] loss: 0.605
[ 81 / 95 ] loss: 0.676
[ 82 / 95 ] loss: 0.500
[ 83 / 95 ] loss: 0.582
[ 84 / 95 ] loss: 0.383
[ 85 / 95 ] loss: 0.518
[ 86 / 95 ] loss: 0.496
[ 87 / 95 ] loss: 0.445
[ 88 / 95 ] loss: 0.405
[ 89 / 95 ] loss: 0.653
[ 90 / 95 ] loss: 0.404
[ 91 / 95 ] loss: 0.601
[ 92 / 95 ] loss: 0.389
[ 93 / 95 ] loss: 0.611
[ 94 / 95 ] loss: 0.503
[ 95 / 95 ] loss: 0.284
0.5172983258962631
Accuracy: 0.830239 -- Precision: 0.797665 -- Recall: 0.944700 -- F1: 0.864979 -- AUC: 0.840899
保存模型参数
========= epoch: 7 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.273
[ 2 / 95 ] loss: 0.356
[ 3 / 95 ] loss: 0.453
[ 4 / 95 ] loss: 0.657
[ 5 / 95 ] loss: 0.564
[ 6 / 95 ] loss: 0.579
[ 7 / 95 ] loss: 0.316
[ 8 / 95 ] loss: 0.405
[ 9 / 95 ] loss: 0.752
[ 10 / 95 ] loss: 0.480
[ 11 / 95 ] loss: 0.648
[ 12 / 95 ] loss: 0.377
[ 13 / 95 ] loss: 0.536
[ 14 / 95 ] loss: 0.690
[ 15 / 95 ] loss: 0.303
[ 16 / 95 ] loss: 0.480
[ 17 / 95 ] loss: 0.419
[ 18 / 95 ] loss: 0.441
[ 19 / 95 ] loss: 0.338
[ 20 / 95 ] loss: 0.200
[ 21 / 95 ] loss: 0.553
[ 22 / 95 ] loss: 0.303
[ 23 / 95 ] loss: 0.416
[ 24 / 95 ] loss: 0.727
[ 25 / 95 ] loss: 0.469
[ 26 / 95 ] loss: 0.698
[ 27 / 95 ] loss: 0.237
[ 28 / 95 ] loss: 0.460
[ 29 / 95 ] loss: 0.514
[ 30 / 95 ] loss: 0.692
[ 31 / 95 ] loss: 0.563
[ 32 / 95 ] loss: 0.317
[ 33 / 95 ] loss: 0.562
[ 34 / 95 ] loss: 0.566
[ 35 / 95 ] loss: 0.430
[ 36 / 95 ] loss: 0.659
[ 37 / 95 ] loss: 0.581
[ 38 / 95 ] loss: 0.543
[ 39 / 95 ] loss: 0.354
[ 40 / 95 ] loss: 0.612
[ 41 / 95 ] loss: 0.397
[ 42 / 95 ] loss: 0.315
[ 43 / 95 ] loss: 0.300
[ 44 / 95 ] loss: 0.625
[ 45 / 95 ] loss: 0.409
[ 46 / 95 ] loss: 0.615
[ 47 / 95 ] loss: 0.614
[ 48 / 95 ] loss: 0.511
[ 49 / 95 ] loss: 0.482
[ 50 / 95 ] loss: 0.354
[ 51 / 95 ] loss: 0.258
[ 52 / 95 ] loss: 0.517
[ 53 / 95 ] loss: 0.505
[ 54 / 95 ] loss: 0.514
[ 55 / 95 ] loss: 0.364
[ 56 / 95 ] loss: 0.454
[ 57 / 95 ] loss: 0.977
[ 58 / 95 ] loss: 0.384
[ 59 / 95 ] loss: 0.568
[ 60 / 95 ] loss: 0.709
[ 61 / 95 ] loss: 0.661
[ 62 / 95 ] loss: 0.728
[ 63 / 95 ] loss: 0.757
[ 64 / 95 ] loss: 0.698
[ 65 / 95 ] loss: 0.585
[ 66 / 95 ] loss: 0.666
[ 67 / 95 ] loss: 0.809
[ 68 / 95 ] loss: 0.670
[ 69 / 95 ] loss: 0.595
[ 70 / 95 ] loss: 0.421
[ 71 / 95 ] loss: 0.720
[ 72 / 95 ] loss: 0.523
[ 73 / 95 ] loss: 0.656
[ 74 / 95 ] loss: 0.445
[ 75 / 95 ] loss: 0.566
[ 76 / 95 ] loss: 0.572
[ 77 / 95 ] loss: 0.631
[ 78 / 95 ] loss: 0.618
[ 79 / 95 ] loss: 0.523
[ 80 / 95 ] loss: 0.627
[ 81 / 95 ] loss: 0.612
[ 82 / 95 ] loss: 0.680
[ 83 / 95 ] loss: 0.629
[ 84 / 95 ] loss: 0.724
[ 85 / 95 ] loss: 0.612
[ 86 / 95 ] loss: 0.762
[ 87 / 95 ] loss: 0.554
[ 88 / 95 ] loss: 0.500
[ 89 / 95 ] loss: 0.637
[ 90 / 95 ] loss: 0.544
[ 91 / 95 ] loss: 0.740
[ 92 / 95 ] loss: 0.365
[ 93 / 95 ] loss: 0.426
[ 94 / 95 ] loss: 0.525
[ 95 / 95 ] loss: 0.450
0.5328922061543716
Accuracy: 0.790451 -- Precision: 0.751825 -- Recall: 0.949309 -- F1: 0.839104 -- AUC: 0.859101
========= epoch: 8 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.394
[ 2 / 95 ] loss: 0.477
[ 3 / 95 ] loss: 0.602
[ 4 / 95 ] loss: 0.464
[ 5 / 95 ] loss: 0.394
[ 6 / 95 ] loss: 0.459
[ 7 / 95 ] loss: 0.575
[ 8 / 95 ] loss: 0.388
[ 9 / 95 ] loss: 0.351
[ 10 / 95 ] loss: 0.247
[ 11 / 95 ] loss: 0.508
[ 12 / 95 ] loss: 0.232
[ 13 / 95 ] loss: 0.551
[ 14 / 95 ] loss: 0.509
[ 15 / 95 ] loss: 0.501
[ 16 / 95 ] loss: 0.464
[ 17 / 95 ] loss: 0.435
[ 18 / 95 ] loss: 0.579
[ 19 / 95 ] loss: 0.320
[ 20 / 95 ] loss: 0.500
[ 21 / 95 ] loss: 0.370
[ 22 / 95 ] loss: 0.325
[ 23 / 95 ] loss: 0.496
[ 24 / 95 ] loss: 0.307
[ 25 / 95 ] loss: 0.415
[ 26 / 95 ] loss: 0.286
[ 27 / 95 ] loss: 0.783
[ 28 / 95 ] loss: 0.649
[ 29 / 95 ] loss: 0.393
[ 30 / 95 ] loss: 0.678
[ 31 / 95 ] loss: 0.479
[ 32 / 95 ] loss: 0.322
[ 33 / 95 ] loss: 0.703
[ 34 / 95 ] loss: 0.444
[ 35 / 95 ] loss: 0.523
[ 36 / 95 ] loss: 0.622
[ 37 / 95 ] loss: 0.823
[ 38 / 95 ] loss: 0.470
[ 39 / 95 ] loss: 0.497
[ 40 / 95 ] loss: 0.349
[ 41 / 95 ] loss: 0.668
[ 42 / 95 ] loss: 0.481
[ 43 / 95 ] loss: 0.638
[ 44 / 95 ] loss: 0.689
[ 45 / 95 ] loss: 0.796
[ 46 / 95 ] loss: 0.966
[ 47 / 95 ] loss: 0.551
[ 48 / 95 ] loss: 0.654
[ 49 / 95 ] loss: 0.661
[ 50 / 95 ] loss: 0.613
[ 51 / 95 ] loss: 0.645
[ 52 / 95 ] loss: 0.560
[ 53 / 95 ] loss: 0.536
[ 54 / 95 ] loss: 0.713
[ 55 / 95 ] loss: 0.715
[ 56 / 95 ] loss: 0.844
[ 57 / 95 ] loss: 0.752
[ 58 / 95 ] loss: 0.572
[ 59 / 95 ] loss: 0.689
[ 60 / 95 ] loss: 0.819
[ 61 / 95 ] loss: 0.849
[ 62 / 95 ] loss: 0.609
[ 63 / 95 ] loss: 0.854
[ 64 / 95 ] loss: 0.655
[ 65 / 95 ] loss: 0.670
[ 66 / 95 ] loss: 0.629
[ 67 / 95 ] loss: 0.701
[ 68 / 95 ] loss: 0.784
[ 69 / 95 ] loss: 0.702
[ 70 / 95 ] loss: 0.678
[ 71 / 95 ] loss: 0.695
[ 72 / 95 ] loss: 0.673
[ 73 / 95 ] loss: 0.647
[ 74 / 95 ] loss: 0.684
[ 75 / 95 ] loss: 0.701
[ 76 / 95 ] loss: 0.681
[ 77 / 95 ] loss: 0.679
[ 78 / 95 ] loss: 0.697
[ 79 / 95 ] loss: 0.647
[ 80 / 95 ] loss: 0.714
[ 81 / 95 ] loss: 0.670
[ 82 / 95 ] loss: 0.687
[ 83 / 95 ] loss: 0.685
[ 84 / 95 ] loss: 0.642
[ 85 / 95 ] loss: 0.621
[ 86 / 95 ] loss: 0.695
[ 87 / 95 ] loss: 0.716
[ 88 / 95 ] loss: 0.746
[ 89 / 95 ] loss: 0.690
[ 90 / 95 ] loss: 0.709
[ 91 / 95 ] loss: 0.737
[ 92 / 95 ] loss: 0.757
[ 93 / 95 ] loss: 0.718
[ 94 / 95 ] loss: 0.656
[ 95 / 95 ] loss: 0.737
0.5985105683929042
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.638522
========= epoch: 9 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.679
[ 2 / 95 ] loss: 0.686
[ 3 / 95 ] loss: 0.685
[ 4 / 95 ] loss: 0.702
[ 5 / 95 ] loss: 0.689
[ 6 / 95 ] loss: 0.709
[ 7 / 95 ] loss: 0.712
[ 8 / 95 ] loss: 0.696
[ 9 / 95 ] loss: 0.707
[ 10 / 95 ] loss: 0.698
[ 11 / 95 ] loss: 0.684
[ 12 / 95 ] loss: 0.688
[ 13 / 95 ] loss: 0.703
[ 14 / 95 ] loss: 0.671
[ 15 / 95 ] loss: 0.679
[ 16 / 95 ] loss: 0.675
[ 17 / 95 ] loss: 0.703
[ 18 / 95 ] loss: 0.749
[ 19 / 95 ] loss: 0.664
[ 20 / 95 ] loss: 0.701
[ 21 / 95 ] loss: 0.648
[ 22 / 95 ] loss: 0.696
[ 23 / 95 ] loss: 0.728
[ 24 / 95 ] loss: 0.709
[ 25 / 95 ] loss: 0.718
[ 26 / 95 ] loss: 0.692
[ 27 / 95 ] loss: 0.703
[ 28 / 95 ] loss: 0.653
[ 29 / 95 ] loss: 0.717
[ 30 / 95 ] loss: 0.698
[ 31 / 95 ] loss: 0.668
[ 32 / 95 ] loss: 0.696
[ 33 / 95 ] loss: 0.672
[ 34 / 95 ] loss: 0.706
[ 35 / 95 ] loss: 0.692
[ 36 / 95 ] loss: 0.667
[ 37 / 95 ] loss: 0.690
[ 38 / 95 ] loss: 0.674
[ 39 / 95 ] loss: 0.666
[ 40 / 95 ] loss: 0.673
[ 41 / 95 ] loss: 0.681
[ 42 / 95 ] loss: 0.675
[ 43 / 95 ] loss: 0.750
[ 44 / 95 ] loss: 0.642
[ 45 / 95 ] loss: 0.711
[ 46 / 95 ] loss: 0.722
[ 47 / 95 ] loss: 0.657
[ 48 / 95 ] loss: 0.697
[ 49 / 95 ] loss: 0.648
[ 50 / 95 ] loss: 0.678
[ 51 / 95 ] loss: 0.712
[ 52 / 95 ] loss: 0.715
[ 53 / 95 ] loss: 0.719
[ 54 / 95 ] loss: 0.650
[ 55 / 95 ] loss: 0.754
[ 56 / 95 ] loss: 0.659
[ 57 / 95 ] loss: 0.638
[ 58 / 95 ] loss: 0.641
[ 59 / 95 ] loss: 0.688
[ 60 / 95 ] loss: 0.661
[ 61 / 95 ] loss: 0.641
[ 62 / 95 ] loss: 0.649
[ 63 / 95 ] loss: 0.612
[ 64 / 95 ] loss: 0.664
[ 65 / 95 ] loss: 0.728
[ 66 / 95 ] loss: 0.632
[ 67 / 95 ] loss: 0.667
[ 68 / 95 ] loss: 0.610
[ 69 / 95 ] loss: 0.630
[ 70 / 95 ] loss: 0.717
[ 71 / 95 ] loss: 0.810
[ 72 / 95 ] loss: 0.685
[ 73 / 95 ] loss: 0.697
[ 74 / 95 ] loss: 0.672
[ 75 / 95 ] loss: 0.700
[ 76 / 95 ] loss: 0.597
[ 77 / 95 ] loss: 0.753
[ 78 / 95 ] loss: 0.673
[ 79 / 95 ] loss: 0.652
[ 80 / 95 ] loss: 0.693
[ 81 / 95 ] loss: 0.634
[ 82 / 95 ] loss: 0.636
[ 83 / 95 ] loss: 0.629
[ 84 / 95 ] loss: 0.751
[ 85 / 95 ] loss: 0.717
[ 86 / 95 ] loss: 0.728
[ 87 / 95 ] loss: 0.674
[ 88 / 95 ] loss: 0.666
[ 89 / 95 ] loss: 0.730
[ 90 / 95 ] loss: 0.624
[ 91 / 95 ] loss: 0.682
[ 92 / 95 ] loss: 0.717
[ 93 / 95 ] loss: 0.688
[ 94 / 95 ] loss: 0.660
[ 95 / 95 ] loss: 0.642
0.6842631848234879
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.517339
========= epoch: 10 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.716
[ 2 / 95 ] loss: 0.674
[ 3 / 95 ] loss: 0.619
[ 4 / 95 ] loss: 0.675
[ 5 / 95 ] loss: 0.623
[ 6 / 95 ] loss: 0.742
[ 7 / 95 ] loss: 0.669
[ 8 / 95 ] loss: 0.691
[ 9 / 95 ] loss: 0.808
[ 10 / 95 ] loss: 0.709
[ 11 / 95 ] loss: 0.679
[ 12 / 95 ] loss: 0.650
[ 13 / 95 ] loss: 0.688
[ 14 / 95 ] loss: 0.687
[ 15 / 95 ] loss: 0.640
[ 16 / 95 ] loss: 0.703
[ 17 / 95 ] loss: 0.785
[ 18 / 95 ] loss: 0.696
[ 19 / 95 ] loss: 0.700
[ 20 / 95 ] loss: 0.754
[ 21 / 95 ] loss: 0.656
[ 22 / 95 ] loss: 0.706
[ 23 / 95 ] loss: 0.641
[ 24 / 95 ] loss: 0.689
[ 25 / 95 ] loss: 0.709
[ 26 / 95 ] loss: 0.677
[ 27 / 95 ] loss: 0.662
[ 28 / 95 ] loss: 0.715
[ 29 / 95 ] loss: 0.682
[ 30 / 95 ] loss: 0.694
[ 31 / 95 ] loss: 0.674
[ 32 / 95 ] loss: 0.711
[ 33 / 95 ] loss: 0.678
[ 34 / 95 ] loss: 0.705
[ 35 / 95 ] loss: 0.682
[ 36 / 95 ] loss: 0.679
[ 37 / 95 ] loss: 0.652
[ 38 / 95 ] loss: 0.645
[ 39 / 95 ] loss: 0.683
[ 40 / 95 ] loss: 0.684
[ 41 / 95 ] loss: 0.698
[ 42 / 95 ] loss: 0.722
[ 43 / 95 ] loss: 0.683
[ 44 / 95 ] loss: 0.618
[ 45 / 95 ] loss: 0.630
[ 46 / 95 ] loss: 0.720
[ 47 / 95 ] loss: 0.708
[ 48 / 95 ] loss: 0.714
[ 49 / 95 ] loss: 0.632
[ 50 / 95 ] loss: 0.632
[ 51 / 95 ] loss: 0.662
[ 52 / 95 ] loss: 0.674
[ 53 / 95 ] loss: 0.627
[ 54 / 95 ] loss: 0.640
[ 55 / 95 ] loss: 0.702
[ 56 / 95 ] loss: 0.567
[ 57 / 95 ] loss: 0.698
[ 58 / 95 ] loss: 0.651
[ 59 / 95 ] loss: 0.620
[ 60 / 95 ] loss: 0.686
[ 61 / 95 ] loss: 0.697
[ 62 / 95 ] loss: 0.729
[ 63 / 95 ] loss: 0.777
[ 64 / 95 ] loss: 0.769
[ 65 / 95 ] loss: 0.688
[ 66 / 95 ] loss: 0.734
[ 67 / 95 ] loss: 0.760
[ 68 / 95 ] loss: 0.716
[ 69 / 95 ] loss: 0.704
[ 70 / 95 ] loss: 0.647
[ 71 / 95 ] loss: 0.697
[ 72 / 95 ] loss: 0.632
[ 73 / 95 ] loss: 0.699
[ 74 / 95 ] loss: 0.739
[ 75 / 95 ] loss: 0.632
[ 76 / 95 ] loss: 0.690
[ 77 / 95 ] loss: 0.726
[ 78 / 95 ] loss: 0.667
[ 79 / 95 ] loss: 0.706
[ 80 / 95 ] loss: 0.664
[ 81 / 95 ] loss: 0.684
[ 82 / 95 ] loss: 0.703
[ 83 / 95 ] loss: 0.708
[ 84 / 95 ] loss: 0.681
[ 85 / 95 ] loss: 0.664
[ 86 / 95 ] loss: 0.659
[ 87 / 95 ] loss: 0.714
[ 88 / 95 ] loss: 0.676
[ 89 / 95 ] loss: 0.679
[ 90 / 95 ] loss: 0.666
[ 91 / 95 ] loss: 0.652
[ 92 / 95 ] loss: 0.693
[ 93 / 95 ] loss: 0.686
[ 94 / 95 ] loss: 0.684
[ 95 / 95 ] loss: 0.722
0.6858500976311533
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.618980
========= epoch: 11 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.709
[ 2 / 95 ] loss: 0.708
[ 3 / 95 ] loss: 0.707
[ 4 / 95 ] loss: 0.717
[ 5 / 95 ] loss: 0.710
[ 6 / 95 ] loss: 0.742
[ 7 / 95 ] loss: 0.728
[ 8 / 95 ] loss: 0.690
[ 9 / 95 ] loss: 0.665
[ 10 / 95 ] loss: 0.635
[ 11 / 95 ] loss: 0.714
[ 12 / 95 ] loss: 0.664
[ 13 / 95 ] loss: 0.675
[ 14 / 95 ] loss: 0.664
[ 15 / 95 ] loss: 0.646
[ 16 / 95 ] loss: 0.673
[ 17 / 95 ] loss: 0.681
[ 18 / 95 ] loss: 0.728
[ 19 / 95 ] loss: 0.702
[ 20 / 95 ] loss: 0.720
[ 21 / 95 ] loss: 0.628
[ 22 / 95 ] loss: 0.672
[ 23 / 95 ] loss: 0.701
[ 24 / 95 ] loss: 0.691
[ 25 / 95 ] loss: 0.682
[ 26 / 95 ] loss: 0.638
[ 27 / 95 ] loss: 0.646
[ 28 / 95 ] loss: 0.677
[ 29 / 95 ] loss: 0.652
[ 30 / 95 ] loss: 0.689
[ 31 / 95 ] loss: 0.643
[ 32 / 95 ] loss: 0.636
[ 33 / 95 ] loss: 0.685
[ 34 / 95 ] loss: 0.709
[ 35 / 95 ] loss: 0.628
[ 36 / 95 ] loss: 0.580
[ 37 / 95 ] loss: 0.631
[ 38 / 95 ] loss: 0.688
[ 39 / 95 ] loss: 0.627
[ 40 / 95 ] loss: 0.695
[ 41 / 95 ] loss: 0.717
[ 42 / 95 ] loss: 0.710
[ 43 / 95 ] loss: 0.666
[ 44 / 95 ] loss: 0.779
[ 45 / 95 ] loss: 0.676
[ 46 / 95 ] loss: 0.688
[ 47 / 95 ] loss: 0.712
[ 48 / 95 ] loss: 0.720
[ 49 / 95 ] loss: 0.702
[ 50 / 95 ] loss: 0.655
[ 51 / 95 ] loss: 0.574
[ 52 / 95 ] loss: 0.673
[ 53 / 95 ] loss: 0.661
[ 54 / 95 ] loss: 0.674
[ 55 / 95 ] loss: 0.664
[ 56 / 95 ] loss: 0.646
[ 57 / 95 ] loss: 0.746
[ 58 / 95 ] loss: 0.681
[ 59 / 95 ] loss: 0.663
[ 60 / 95 ] loss: 0.590
[ 61 / 95 ] loss: 0.665
[ 62 / 95 ] loss: 0.685
[ 63 / 95 ] loss: 0.720
[ 64 / 95 ] loss: 0.658
[ 65 / 95 ] loss: 0.747
[ 66 / 95 ] loss: 0.666
[ 67 / 95 ] loss: 0.610
[ 68 / 95 ] loss: 0.640
[ 69 / 95 ] loss: 0.709
[ 70 / 95 ] loss: 0.685
[ 71 / 95 ] loss: 0.693
[ 72 / 95 ] loss: 0.715
[ 73 / 95 ] loss: 0.712
[ 74 / 95 ] loss: 0.599
[ 75 / 95 ] loss: 0.726
[ 76 / 95 ] loss: 0.715
[ 77 / 95 ] loss: 0.676
[ 78 / 95 ] loss: 0.683
[ 79 / 95 ] loss: 0.690
[ 80 / 95 ] loss: 0.657
[ 81 / 95 ] loss: 0.643
[ 82 / 95 ] loss: 0.720
[ 83 / 95 ] loss: 0.634
[ 84 / 95 ] loss: 0.675
[ 85 / 95 ] loss: 0.694
[ 86 / 95 ] loss: 0.747
[ 87 / 95 ] loss: 0.700
[ 88 / 95 ] loss: 0.640
[ 89 / 95 ] loss: 0.700
[ 90 / 95 ] loss: 0.688
[ 91 / 95 ] loss: 0.689
[ 92 / 95 ] loss: 0.697
[ 93 / 95 ] loss: 0.704
[ 94 / 95 ] loss: 0.650
[ 95 / 95 ] loss: 0.707
0.6801759807687057
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.635412
========= epoch: 12 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.662
[ 2 / 95 ] loss: 0.686
[ 3 / 95 ] loss: 0.636
[ 4 / 95 ] loss: 0.719
[ 5 / 95 ] loss: 0.672
[ 6 / 95 ] loss: 0.657
[ 7 / 95 ] loss: 0.675
[ 8 / 95 ] loss: 0.666
[ 9 / 95 ] loss: 0.650
[ 10 / 95 ] loss: 0.709
[ 11 / 95 ] loss: 0.675
[ 12 / 95 ] loss: 0.696
[ 13 / 95 ] loss: 0.686
[ 14 / 95 ] loss: 0.661
[ 15 / 95 ] loss: 0.685
[ 16 / 95 ] loss: 0.629
[ 17 / 95 ] loss: 0.688
[ 18 / 95 ] loss: 0.780
[ 19 / 95 ] loss: 0.649
[ 20 / 95 ] loss: 0.669
[ 21 / 95 ] loss: 0.646
[ 22 / 95 ] loss: 0.630
[ 23 / 95 ] loss: 0.662
[ 24 / 95 ] loss: 0.625
[ 25 / 95 ] loss: 0.655
[ 26 / 95 ] loss: 0.735
[ 27 / 95 ] loss: 0.645
[ 28 / 95 ] loss: 0.677
[ 29 / 95 ] loss: 0.563
[ 30 / 95 ] loss: 0.682
[ 31 / 95 ] loss: 0.622
[ 32 / 95 ] loss: 0.635
[ 33 / 95 ] loss: 0.630
[ 34 / 95 ] loss: 0.654
[ 35 / 95 ] loss: 0.572
[ 36 / 95 ] loss: 0.746
[ 37 / 95 ] loss: 0.512
[ 38 / 95 ] loss: 0.660
[ 39 / 95 ] loss: 0.692
[ 40 / 95 ] loss: 0.641
[ 41 / 95 ] loss: 0.673
[ 42 / 95 ] loss: 0.663
[ 43 / 95 ] loss: 0.649
[ 44 / 95 ] loss: 0.672
[ 45 / 95 ] loss: 0.663
[ 46 / 95 ] loss: 0.636
[ 47 / 95 ] loss: 0.799
[ 48 / 95 ] loss: 0.634
[ 49 / 95 ] loss: 0.832
[ 50 / 95 ] loss: 0.577
[ 51 / 95 ] loss: 0.688
[ 52 / 95 ] loss: 0.820
[ 53 / 95 ] loss: 0.586
[ 54 / 95 ] loss: 0.636
[ 55 / 95 ] loss: 0.697
[ 56 / 95 ] loss: 0.726
[ 57 / 95 ] loss: 0.662
[ 58 / 95 ] loss: 0.658
[ 59 / 95 ] loss: 0.648
[ 60 / 95 ] loss: 0.659
[ 61 / 95 ] loss: 0.671
[ 62 / 95 ] loss: 0.733
[ 63 / 95 ] loss: 0.707
[ 64 / 95 ] loss: 0.757
[ 65 / 95 ] loss: 0.743
[ 66 / 95 ] loss: 0.671
[ 67 / 95 ] loss: 0.699
[ 68 / 95 ] loss: 0.682
[ 69 / 95 ] loss: 0.699
[ 70 / 95 ] loss: 0.677
[ 71 / 95 ] loss: 0.692
[ 72 / 95 ] loss: 0.700
[ 73 / 95 ] loss: 0.702
[ 74 / 95 ] loss: 0.682
[ 75 / 95 ] loss: 0.682
[ 76 / 95 ] loss: 0.686
[ 77 / 95 ] loss: 0.692
[ 78 / 95 ] loss: 0.673
[ 79 / 95 ] loss: 0.698
[ 80 / 95 ] loss: 0.700
[ 81 / 95 ] loss: 0.673
[ 82 / 95 ] loss: 0.639
[ 83 / 95 ] loss: 0.660
[ 84 / 95 ] loss: 0.705
[ 85 / 95 ] loss: 0.681
[ 86 / 95 ] loss: 0.638
[ 87 / 95 ] loss: 0.638
[ 88 / 95 ] loss: 0.665
[ 89 / 95 ] loss: 0.646
[ 90 / 95 ] loss: 0.634
[ 91 / 95 ] loss: 0.685
[ 92 / 95 ] loss: 0.686
[ 93 / 95 ] loss: 0.668
[ 94 / 95 ] loss: 0.623
[ 95 / 95 ] loss: 0.800
0.6736134566758808
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.647797
========= epoch: 13 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.669
[ 2 / 95 ] loss: 0.577
[ 3 / 95 ] loss: 0.617
[ 4 / 95 ] loss: 0.631
[ 5 / 95 ] loss: 0.671
[ 6 / 95 ] loss: 0.717
[ 7 / 95 ] loss: 0.714
[ 8 / 95 ] loss: 0.636
[ 9 / 95 ] loss: 0.659
[ 10 / 95 ] loss: 0.688
[ 11 / 95 ] loss: 0.651
[ 12 / 95 ] loss: 0.678
[ 13 / 95 ] loss: 0.616
[ 14 / 95 ] loss: 0.641
[ 15 / 95 ] loss: 0.636
[ 16 / 95 ] loss: 0.615
[ 17 / 95 ] loss: 0.764
[ 18 / 95 ] loss: 0.698
[ 19 / 95 ] loss: 0.523
[ 20 / 95 ] loss: 0.562
[ 21 / 95 ] loss: 0.685
[ 22 / 95 ] loss: 0.630
[ 23 / 95 ] loss: 0.720
[ 24 / 95 ] loss: 0.721
[ 25 / 95 ] loss: 0.709
[ 26 / 95 ] loss: 0.764
[ 27 / 95 ] loss: 0.641
[ 28 / 95 ] loss: 0.726
[ 29 / 95 ] loss: 0.666
[ 30 / 95 ] loss: 0.679
[ 31 / 95 ] loss: 0.693
[ 32 / 95 ] loss: 0.669
[ 33 / 95 ] loss: 0.643
[ 34 / 95 ] loss: 0.667
[ 35 / 95 ] loss: 0.688
[ 36 / 95 ] loss: 0.656
[ 37 / 95 ] loss: 0.662
[ 38 / 95 ] loss: 0.669
[ 39 / 95 ] loss: 0.740
[ 40 / 95 ] loss: 0.665
[ 41 / 95 ] loss: 0.711
[ 42 / 95 ] loss: 0.763
[ 43 / 95 ] loss: 0.688
[ 44 / 95 ] loss: 0.602
[ 45 / 95 ] loss: 0.584
[ 46 / 95 ] loss: 0.671
[ 47 / 95 ] loss: 0.560
[ 48 / 95 ] loss: 0.600
[ 49 / 95 ] loss: 0.534
[ 50 / 95 ] loss: 0.647
[ 51 / 95 ] loss: 0.720
[ 52 / 95 ] loss: 0.608
[ 53 / 95 ] loss: 0.656
[ 54 / 95 ] loss: 0.624
[ 55 / 95 ] loss: 0.653
[ 56 / 95 ] loss: 0.682
[ 57 / 95 ] loss: 0.498
[ 58 / 95 ] loss: 0.728
[ 59 / 95 ] loss: 0.662
[ 60 / 95 ] loss: 0.650
[ 61 / 95 ] loss: 0.563
[ 62 / 95 ] loss: 0.762
[ 63 / 95 ] loss: 0.807
[ 64 / 95 ] loss: 0.600
[ 65 / 95 ] loss: 0.661
[ 66 / 95 ] loss: 0.563
[ 67 / 95 ] loss: 0.647
[ 68 / 95 ] loss: 0.549
[ 69 / 95 ] loss: 0.676
[ 70 / 95 ] loss: 0.490
[ 71 / 95 ] loss: 0.535
[ 72 / 95 ] loss: 0.576
[ 73 / 95 ] loss: 0.689
[ 74 / 95 ] loss: 0.609
[ 75 / 95 ] loss: 0.547
[ 76 / 95 ] loss: 0.596
[ 77 / 95 ] loss: 0.566
[ 78 / 95 ] loss: 0.611
[ 79 / 95 ] loss: 0.651
[ 80 / 95 ] loss: 0.682
[ 81 / 95 ] loss: 0.568
[ 82 / 95 ] loss: 0.662
[ 83 / 95 ] loss: 0.495
[ 84 / 95 ] loss: 0.500
[ 85 / 95 ] loss: 0.715
[ 86 / 95 ] loss: 0.497
[ 87 / 95 ] loss: 0.655
[ 88 / 95 ] loss: 0.557
[ 89 / 95 ] loss: 0.573
[ 90 / 95 ] loss: 0.727
[ 91 / 95 ] loss: 0.629
[ 92 / 95 ] loss: 0.607
[ 93 / 95 ] loss: 0.487
[ 94 / 95 ] loss: 0.508
[ 95 / 95 ] loss: 0.552
0.6379497258286727
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.629752
========= epoch: 14 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.602
[ 2 / 95 ] loss: 0.814
[ 3 / 95 ] loss: 0.650
[ 4 / 95 ] loss: 0.678
[ 5 / 95 ] loss: 0.628
[ 6 / 95 ] loss: 0.640
[ 7 / 95 ] loss: 0.557
[ 8 / 95 ] loss: 0.674
[ 9 / 95 ] loss: 0.763
[ 10 / 95 ] loss: 0.531
[ 11 / 95 ] loss: 0.721
[ 12 / 95 ] loss: 0.810
[ 13 / 95 ] loss: 0.812
[ 14 / 95 ] loss: 0.715
[ 15 / 95 ] loss: 0.724
[ 16 / 95 ] loss: 0.387
[ 17 / 95 ] loss: 0.617
[ 18 / 95 ] loss: 0.672
[ 19 / 95 ] loss: 0.613
[ 20 / 95 ] loss: 0.630
[ 21 / 95 ] loss: 0.671
[ 22 / 95 ] loss: 0.808
[ 23 / 95 ] loss: 0.642
[ 24 / 95 ] loss: 0.608
[ 25 / 95 ] loss: 0.515
[ 26 / 95 ] loss: 0.696
[ 27 / 95 ] loss: 0.643
[ 28 / 95 ] loss: 0.684
[ 29 / 95 ] loss: 0.494
[ 30 / 95 ] loss: 0.640
[ 31 / 95 ] loss: 0.691
[ 32 / 95 ] loss: 0.665
[ 33 / 95 ] loss: 0.705
[ 34 / 95 ] loss: 0.719
[ 35 / 95 ] loss: 0.672
[ 36 / 95 ] loss: 0.477
[ 37 / 95 ] loss: 0.616
[ 38 / 95 ] loss: 0.516
[ 39 / 95 ] loss: 0.695
[ 40 / 95 ] loss: 0.752
[ 41 / 95 ] loss: 0.485
[ 42 / 95 ] loss: 0.615
[ 43 / 95 ] loss: 0.665
[ 44 / 95 ] loss: 0.629
[ 45 / 95 ] loss: 0.575
[ 46 / 95 ] loss: 0.583
[ 47 / 95 ] loss: 0.598
[ 48 / 95 ] loss: 0.747
[ 49 / 95 ] loss: 0.701
[ 50 / 95 ] loss: 0.710
[ 51 / 95 ] loss: 0.605
[ 52 / 95 ] loss: 0.739
[ 53 / 95 ] loss: 0.795
[ 54 / 95 ] loss: 0.656
[ 55 / 95 ] loss: 0.612
[ 56 / 95 ] loss: 0.611
[ 57 / 95 ] loss: 0.627
[ 58 / 95 ] loss: 0.683
[ 59 / 95 ] loss: 0.690
[ 60 / 95 ] loss: 0.639
[ 61 / 95 ] loss: 0.571
[ 62 / 95 ] loss: 0.678
[ 63 / 95 ] loss: 0.683
[ 64 / 95 ] loss: 0.657
[ 65 / 95 ] loss: 0.767
[ 66 / 95 ] loss: 0.660
[ 67 / 95 ] loss: 0.767
[ 68 / 95 ] loss: 0.649
[ 69 / 95 ] loss: 0.727
[ 70 / 95 ] loss: 0.551
[ 71 / 95 ] loss: 0.546
[ 72 / 95 ] loss: 0.542
[ 73 / 95 ] loss: 0.609
[ 74 / 95 ] loss: 0.561
[ 75 / 95 ] loss: 0.575
[ 76 / 95 ] loss: 0.604
[ 77 / 95 ] loss: 0.624
[ 78 / 95 ] loss: 0.694
[ 79 / 95 ] loss: 0.677
[ 80 / 95 ] loss: 0.649
[ 81 / 95 ] loss: 0.434
[ 82 / 95 ] loss: 0.680
[ 83 / 95 ] loss: 0.440
[ 84 / 95 ] loss: 0.581
[ 85 / 95 ] loss: 0.552
[ 86 / 95 ] loss: 0.659
[ 87 / 95 ] loss: 0.860
[ 88 / 95 ] loss: 0.557
[ 89 / 95 ] loss: 0.532
[ 90 / 95 ] loss: 0.493
[ 91 / 95 ] loss: 0.725
[ 92 / 95 ] loss: 0.875
[ 93 / 95 ] loss: 0.429
[ 94 / 95 ] loss: 0.634
[ 95 / 95 ] loss: 0.578
0.6413399250883806
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.555847
========= epoch: 15 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.717
[ 2 / 95 ] loss: 0.613
[ 3 / 95 ] loss: 0.605
[ 4 / 95 ] loss: 0.591
[ 5 / 95 ] loss: 0.558
[ 6 / 95 ] loss: 0.578
[ 7 / 95 ] loss: 0.633
[ 8 / 95 ] loss: 0.533
[ 9 / 95 ] loss: 0.660
[ 10 / 95 ] loss: 0.673
[ 11 / 95 ] loss: 0.650
[ 12 / 95 ] loss: 0.561
[ 13 / 95 ] loss: 0.746
[ 14 / 95 ] loss: 0.493
[ 15 / 95 ] loss: 0.643
[ 16 / 95 ] loss: 0.603
[ 17 / 95 ] loss: 0.630
[ 18 / 95 ] loss: 0.585
[ 19 / 95 ] loss: 0.525
[ 20 / 95 ] loss: 0.605
[ 21 / 95 ] loss: 0.526
[ 22 / 95 ] loss: 0.701
[ 23 / 95 ] loss: 0.525
[ 24 / 95 ] loss: 0.612
[ 25 / 95 ] loss: 0.414
[ 26 / 95 ] loss: 0.571
[ 27 / 95 ] loss: 0.642
[ 28 / 95 ] loss: 0.621
[ 29 / 95 ] loss: 0.752
[ 30 / 95 ] loss: 0.686
[ 31 / 95 ] loss: 0.513
[ 32 / 95 ] loss: 0.629
[ 33 / 95 ] loss: 0.770
[ 34 / 95 ] loss: 0.714
[ 35 / 95 ] loss: 0.601
[ 36 / 95 ] loss: 0.703
[ 37 / 95 ] loss: 0.430
[ 38 / 95 ] loss: 0.724
[ 39 / 95 ] loss: 0.710
[ 40 / 95 ] loss: 0.669
[ 41 / 95 ] loss: 0.496
[ 42 / 95 ] loss: 0.883
[ 43 / 95 ] loss: 0.876
[ 44 / 95 ] loss: 0.721
[ 45 / 95 ] loss: 0.656
[ 46 / 95 ] loss: 0.717
[ 47 / 95 ] loss: 0.702
[ 48 / 95 ] loss: 0.779
[ 49 / 95 ] loss: 0.642
[ 50 / 95 ] loss: 0.603
[ 51 / 95 ] loss: 0.610
[ 52 / 95 ] loss: 0.641
[ 53 / 95 ] loss: 0.699
[ 54 / 95 ] loss: 0.595
[ 55 / 95 ] loss: 0.679
[ 56 / 95 ] loss: 0.530
[ 57 / 95 ] loss: 0.670
[ 58 / 95 ] loss: 0.637
[ 59 / 95 ] loss: 0.570
[ 60 / 95 ] loss: 0.574
[ 61 / 95 ] loss: 0.638
[ 62 / 95 ] loss: 0.591
[ 63 / 95 ] loss: 0.740
[ 64 / 95 ] loss: 0.606
[ 65 / 95 ] loss: 0.624
[ 66 / 95 ] loss: 0.656
[ 67 / 95 ] loss: 0.690
[ 68 / 95 ] loss: 0.673
[ 69 / 95 ] loss: 0.587
[ 70 / 95 ] loss: 0.609
[ 71 / 95 ] loss: 0.525
[ 72 / 95 ] loss: 0.628
[ 73 / 95 ] loss: 0.610
[ 74 / 95 ] loss: 0.646
[ 75 / 95 ] loss: 0.597
[ 76 / 95 ] loss: 0.515
[ 77 / 95 ] loss: 0.594
[ 78 / 95 ] loss: 0.706
[ 79 / 95 ] loss: 0.604
[ 80 / 95 ] loss: 0.615
[ 81 / 95 ] loss: 0.649
[ 82 / 95 ] loss: 0.601
[ 83 / 95 ] loss: 0.480
[ 84 / 95 ] loss: 0.667
[ 85 / 95 ] loss: 0.510
[ 86 / 95 ] loss: 0.722
[ 87 / 95 ] loss: 0.735
[ 88 / 95 ] loss: 0.659
[ 89 / 95 ] loss: 0.661
[ 90 / 95 ] loss: 0.576
[ 91 / 95 ] loss: 0.612
[ 92 / 95 ] loss: 0.647
[ 93 / 95 ] loss: 0.593
[ 94 / 95 ] loss: 0.757
[ 95 / 95 ] loss: 0.797
0.6327963505920611
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.565452
========= epoch: 16 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.634
[ 2 / 95 ] loss: 0.605
[ 3 / 95 ] loss: 0.576
[ 4 / 95 ] loss: 0.672
[ 5 / 95 ] loss: 0.484
[ 6 / 95 ] loss: 0.587
[ 7 / 95 ] loss: 0.678
[ 8 / 95 ] loss: 0.587
[ 9 / 95 ] loss: 0.574
[ 10 / 95 ] loss: 0.687
[ 11 / 95 ] loss: 0.513
[ 12 / 95 ] loss: 0.846
[ 13 / 95 ] loss: 0.714
[ 14 / 95 ] loss: 0.553
[ 15 / 95 ] loss: 0.741
[ 16 / 95 ] loss: 0.554
[ 17 / 95 ] loss: 0.535
[ 18 / 95 ] loss: 0.535
[ 19 / 95 ] loss: 0.687
[ 20 / 95 ] loss: 0.812
[ 21 / 95 ] loss: 0.543
[ 22 / 95 ] loss: 0.648
[ 23 / 95 ] loss: 0.555
[ 24 / 95 ] loss: 0.611
[ 25 / 95 ] loss: 0.585
[ 26 / 95 ] loss: 0.578
[ 27 / 95 ] loss: 0.575
[ 28 / 95 ] loss: 0.518
[ 29 / 95 ] loss: 0.697
[ 30 / 95 ] loss: 0.703
[ 31 / 95 ] loss: 0.650
[ 32 / 95 ] loss: 0.558
[ 33 / 95 ] loss: 0.709
[ 34 / 95 ] loss: 0.653
[ 35 / 95 ] loss: 0.728
[ 36 / 95 ] loss: 0.553
[ 37 / 95 ] loss: 0.757
[ 38 / 95 ] loss: 0.629
[ 39 / 95 ] loss: 0.622
[ 40 / 95 ] loss: 0.644
[ 41 / 95 ] loss: 0.631
[ 42 / 95 ] loss: 0.770
[ 43 / 95 ] loss: 0.496
[ 44 / 95 ] loss: 0.489
[ 45 / 95 ] loss: 0.639
[ 46 / 95 ] loss: 0.605
[ 47 / 95 ] loss: 0.563
[ 48 / 95 ] loss: 0.596
[ 49 / 95 ] loss: 0.445
[ 50 / 95 ] loss: 0.529
[ 51 / 95 ] loss: 0.588
[ 52 / 95 ] loss: 0.492
[ 53 / 95 ] loss: 0.691
[ 54 / 95 ] loss: 0.582
[ 55 / 95 ] loss: 0.487
[ 56 / 95 ] loss: 0.535
[ 57 / 95 ] loss: 0.706
[ 58 / 95 ] loss: 0.628
[ 59 / 95 ] loss: 0.694
[ 60 / 95 ] loss: 0.760
[ 61 / 95 ] loss: 0.741
[ 62 / 95 ] loss: 0.559
[ 63 / 95 ] loss: 0.636
[ 64 / 95 ] loss: 0.645
[ 65 / 95 ] loss: 0.722
[ 66 / 95 ] loss: 0.588
[ 67 / 95 ] loss: 0.648
[ 68 / 95 ] loss: 0.469
[ 69 / 95 ] loss: 0.438
[ 70 / 95 ] loss: 0.696
[ 71 / 95 ] loss: 0.743
[ 72 / 95 ] loss: 0.652
[ 73 / 95 ] loss: 0.584
[ 74 / 95 ] loss: 0.627
[ 75 / 95 ] loss: 0.689
[ 76 / 95 ] loss: 0.666
[ 77 / 95 ] loss: 0.634
[ 78 / 95 ] loss: 0.616
[ 79 / 95 ] loss: 0.481
[ 80 / 95 ] loss: 0.695
[ 81 / 95 ] loss: 0.584
[ 82 / 95 ] loss: 0.548
[ 83 / 95 ] loss: 0.616
[ 84 / 95 ] loss: 0.555
[ 85 / 95 ] loss: 0.589
[ 86 / 95 ] loss: 0.534
[ 87 / 95 ] loss: 0.677
[ 88 / 95 ] loss: 0.730
[ 89 / 95 ] loss: 0.642
[ 90 / 95 ] loss: 0.554
[ 91 / 95 ] loss: 0.700
[ 92 / 95 ] loss: 0.473
[ 93 / 95 ] loss: 0.560
[ 94 / 95 ] loss: 0.653
[ 95 / 95 ] loss: 0.570
0.6161532988673762
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.617079
========= epoch: 17 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.580
[ 2 / 95 ] loss: 0.543
[ 3 / 95 ] loss: 0.502
[ 4 / 95 ] loss: 0.668
[ 5 / 95 ] loss: 0.627
[ 6 / 95 ] loss: 0.697
[ 7 / 95 ] loss: 0.585
[ 8 / 95 ] loss: 0.481
[ 9 / 95 ] loss: 0.615
[ 10 / 95 ] loss: 0.621
[ 11 / 95 ] loss: 0.856
[ 12 / 95 ] loss: 0.453
[ 13 / 95 ] loss: 0.599
[ 14 / 95 ] loss: 0.792
[ 15 / 95 ] loss: 0.633
[ 16 / 95 ] loss: 0.784
[ 17 / 95 ] loss: 0.677
[ 18 / 95 ] loss: 0.681
[ 19 / 95 ] loss: 0.522
[ 20 / 95 ] loss: 0.609
[ 21 / 95 ] loss: 0.622
[ 22 / 95 ] loss: 0.565
[ 23 / 95 ] loss: 0.386
[ 24 / 95 ] loss: 0.570
[ 25 / 95 ] loss: 0.577
[ 26 / 95 ] loss: 0.668
[ 27 / 95 ] loss: 0.550
[ 28 / 95 ] loss: 0.721
[ 29 / 95 ] loss: 0.538
[ 30 / 95 ] loss: 0.773
[ 31 / 95 ] loss: 0.586
[ 32 / 95 ] loss: 0.654
[ 33 / 95 ] loss: 0.590
[ 34 / 95 ] loss: 0.663
[ 35 / 95 ] loss: 0.581
[ 36 / 95 ] loss: 0.701
[ 37 / 95 ] loss: 0.594
[ 38 / 95 ] loss: 0.809
[ 39 / 95 ] loss: 0.518
[ 40 / 95 ] loss: 0.477
[ 41 / 95 ] loss: 0.621
[ 42 / 95 ] loss: 0.733
[ 43 / 95 ] loss: 0.608
[ 44 / 95 ] loss: 0.541
[ 45 / 95 ] loss: 0.513
[ 46 / 95 ] loss: 0.652
[ 47 / 95 ] loss: 0.556
[ 48 / 95 ] loss: 0.589
[ 49 / 95 ] loss: 0.512
[ 50 / 95 ] loss: 0.591
[ 51 / 95 ] loss: 0.686
[ 52 / 95 ] loss: 1.051
[ 53 / 95 ] loss: 0.548
[ 54 / 95 ] loss: 0.762
[ 55 / 95 ] loss: 0.627
[ 56 / 95 ] loss: 0.741
[ 57 / 95 ] loss: 0.561
[ 58 / 95 ] loss: 0.573
[ 59 / 95 ] loss: 0.678
[ 60 / 95 ] loss: 0.661
[ 61 / 95 ] loss: 0.704
[ 62 / 95 ] loss: 0.648
[ 63 / 95 ] loss: 0.604
[ 64 / 95 ] loss: 0.766
[ 65 / 95 ] loss: 0.492
[ 66 / 95 ] loss: 0.570
[ 67 / 95 ] loss: 0.660
[ 68 / 95 ] loss: 0.682
[ 69 / 95 ] loss: 0.558
[ 70 / 95 ] loss: 0.646
[ 71 / 95 ] loss: 0.485
[ 72 / 95 ] loss: 0.772
[ 73 / 95 ] loss: 0.597
[ 74 / 95 ] loss: 0.646
[ 75 / 95 ] loss: 0.672
[ 76 / 95 ] loss: 0.624
[ 77 / 95 ] loss: 0.495
[ 78 / 95 ] loss: 0.484
[ 79 / 95 ] loss: 0.566
[ 80 / 95 ] loss: 0.589
[ 81 / 95 ] loss: 0.656
[ 82 / 95 ] loss: 0.518
[ 83 / 95 ] loss: 0.764
[ 84 / 95 ] loss: 0.503
[ 85 / 95 ] loss: 0.659
[ 86 / 95 ] loss: 0.677
[ 87 / 95 ] loss: 0.533
[ 88 / 95 ] loss: 0.778
[ 89 / 95 ] loss: 0.477
[ 90 / 95 ] loss: 0.613
[ 91 / 95 ] loss: 0.678
[ 92 / 95 ] loss: 0.534
[ 93 / 95 ] loss: 0.704
[ 94 / 95 ] loss: 0.811
[ 95 / 95 ] loss: 0.642
0.6239322050621635
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.580026
========= epoch: 18 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.497
[ 2 / 95 ] loss: 0.537
[ 3 / 95 ] loss: 0.640
[ 4 / 95 ] loss: 0.520
[ 5 / 95 ] loss: 0.725
[ 6 / 95 ] loss: 0.574
[ 7 / 95 ] loss: 0.580
[ 8 / 95 ] loss: 0.543
[ 9 / 95 ] loss: 0.627
[ 10 / 95 ] loss: 0.516
[ 11 / 95 ] loss: 0.870
[ 12 / 95 ] loss: 0.503
[ 13 / 95 ] loss: 0.706
[ 14 / 95 ] loss: 0.694
[ 15 / 95 ] loss: 0.736
[ 16 / 95 ] loss: 0.463
[ 17 / 95 ] loss: 0.712
[ 18 / 95 ] loss: 0.597
[ 19 / 95 ] loss: 0.628
[ 20 / 95 ] loss: 0.565
[ 21 / 95 ] loss: 0.541
[ 22 / 95 ] loss: 0.640
[ 23 / 95 ] loss: 0.819
[ 24 / 95 ] loss: 0.466
[ 25 / 95 ] loss: 0.739
[ 26 / 95 ] loss: 0.572
[ 27 / 95 ] loss: 0.596
[ 28 / 95 ] loss: 0.739
[ 29 / 95 ] loss: 0.765
[ 30 / 95 ] loss: 0.606
[ 31 / 95 ] loss: 0.646
[ 32 / 95 ] loss: 0.639
[ 33 / 95 ] loss: 0.547
[ 34 / 95 ] loss: 0.634
[ 35 / 95 ] loss: 0.656
[ 36 / 95 ] loss: 0.481
[ 37 / 95 ] loss: 0.576
[ 38 / 95 ] loss: 0.641
[ 39 / 95 ] loss: 0.619
[ 40 / 95 ] loss: 0.692
[ 41 / 95 ] loss: 0.555
[ 42 / 95 ] loss: 0.642
[ 43 / 95 ] loss: 0.522
[ 44 / 95 ] loss: 0.725
[ 45 / 95 ] loss: 0.602
[ 46 / 95 ] loss: 0.632
[ 47 / 95 ] loss: 0.512
[ 48 / 95 ] loss: 0.512
[ 49 / 95 ] loss: 0.658
[ 50 / 95 ] loss: 0.522
[ 51 / 95 ] loss: 0.824
[ 52 / 95 ] loss: 0.722
[ 53 / 95 ] loss: 0.707
[ 54 / 95 ] loss: 0.543
[ 55 / 95 ] loss: 0.661
[ 56 / 95 ] loss: 0.514
[ 57 / 95 ] loss: 0.557
[ 58 / 95 ] loss: 0.641
[ 59 / 95 ] loss: 0.561
[ 60 / 95 ] loss: 0.405
[ 61 / 95 ] loss: 0.609
[ 62 / 95 ] loss: 0.696
[ 63 / 95 ] loss: 0.555
[ 64 / 95 ] loss: 0.579
[ 65 / 95 ] loss: 0.545
[ 66 / 95 ] loss: 0.825
[ 67 / 95 ] loss: 0.626
[ 68 / 95 ] loss: 0.510
[ 69 / 95 ] loss: 0.632
[ 70 / 95 ] loss: 0.649
[ 71 / 95 ] loss: 0.666
[ 72 / 95 ] loss: 0.622
[ 73 / 95 ] loss: 0.562
[ 74 / 95 ] loss: 0.582
[ 75 / 95 ] loss: 0.463
[ 76 / 95 ] loss: 0.721
[ 77 / 95 ] loss: 0.623
[ 78 / 95 ] loss: 0.812
[ 79 / 95 ] loss: 0.506
[ 80 / 95 ] loss: 0.683
[ 81 / 95 ] loss: 0.756
[ 82 / 95 ] loss: 0.452
[ 83 / 95 ] loss: 0.569
[ 84 / 95 ] loss: 0.556
[ 85 / 95 ] loss: 0.575
[ 86 / 95 ] loss: 0.692
[ 87 / 95 ] loss: 0.705
[ 88 / 95 ] loss: 0.708
[ 89 / 95 ] loss: 0.643
[ 90 / 95 ] loss: 0.650
[ 91 / 95 ] loss: 0.521
[ 92 / 95 ] loss: 0.644
[ 93 / 95 ] loss: 0.701
[ 94 / 95 ] loss: 0.604
[ 95 / 95 ] loss: 0.706
0.6190987455217462
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.536060
========= epoch: 19 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.626
[ 2 / 95 ] loss: 0.491
[ 3 / 95 ] loss: 0.405
[ 4 / 95 ] loss: 0.707
[ 5 / 95 ] loss: 0.585
[ 6 / 95 ] loss: 0.870
[ 7 / 95 ] loss: 0.583
[ 8 / 95 ] loss: 0.680
[ 9 / 95 ] loss: 0.462
[ 10 / 95 ] loss: 0.758
[ 11 / 95 ] loss: 0.606
[ 12 / 95 ] loss: 0.556
[ 13 / 95 ] loss: 0.691
[ 14 / 95 ] loss: 0.624
[ 15 / 95 ] loss: 0.635
[ 16 / 95 ] loss: 0.616
[ 17 / 95 ] loss: 0.451
[ 18 / 95 ] loss: 0.557
[ 19 / 95 ] loss: 0.731
[ 20 / 95 ] loss: 0.705
[ 21 / 95 ] loss: 0.676
[ 22 / 95 ] loss: 0.571
[ 23 / 95 ] loss: 0.660
[ 24 / 95 ] loss: 0.723
[ 25 / 95 ] loss: 0.616
[ 26 / 95 ] loss: 0.837
[ 27 / 95 ] loss: 0.498
[ 28 / 95 ] loss: 0.526
[ 29 / 95 ] loss: 0.774
[ 30 / 95 ] loss: 0.503
[ 31 / 95 ] loss: 0.685
[ 32 / 95 ] loss: 0.711
[ 33 / 95 ] loss: 0.476
[ 34 / 95 ] loss: 0.655
[ 35 / 95 ] loss: 0.509
[ 36 / 95 ] loss: 0.834
[ 37 / 95 ] loss: 0.833
[ 38 / 95 ] loss: 0.519
[ 39 / 95 ] loss: 0.674
[ 40 / 95 ] loss: 0.573
[ 41 / 95 ] loss: 0.683
[ 42 / 95 ] loss: 0.609
[ 43 / 95 ] loss: 0.664
[ 44 / 95 ] loss: 0.570
[ 45 / 95 ] loss: 0.640
[ 46 / 95 ] loss: 0.681
[ 47 / 95 ] loss: 0.607
[ 48 / 95 ] loss: 0.667
[ 49 / 95 ] loss: 0.557
[ 50 / 95 ] loss: 0.596
[ 51 / 95 ] loss: 0.588
[ 52 / 95 ] loss: 0.647
[ 53 / 95 ] loss: 0.669
[ 54 / 95 ] loss: 0.641
[ 55 / 95 ] loss: 0.578
[ 56 / 95 ] loss: 0.715
[ 57 / 95 ] loss: 0.649
[ 58 / 95 ] loss: 0.635
[ 59 / 95 ] loss: 0.597
[ 60 / 95 ] loss: 0.656
[ 61 / 95 ] loss: 0.582
[ 62 / 95 ] loss: 0.496
[ 63 / 95 ] loss: 0.641
[ 64 / 95 ] loss: 0.602
[ 65 / 95 ] loss: 0.593
[ 66 / 95 ] loss: 0.618
[ 67 / 95 ] loss: 0.486
[ 68 / 95 ] loss: 0.631
[ 69 / 95 ] loss: 0.624
[ 70 / 95 ] loss: 0.604
[ 71 / 95 ] loss: 0.651
[ 72 / 95 ] loss: 0.605
[ 73 / 95 ] loss: 0.586
[ 74 / 95 ] loss: 0.571
[ 75 / 95 ] loss: 0.702
[ 76 / 95 ] loss: 0.686
[ 77 / 95 ] loss: 0.522
[ 78 / 95 ] loss: 0.790
[ 79 / 95 ] loss: 0.573
[ 80 / 95 ] loss: 0.653
[ 81 / 95 ] loss: 0.532
[ 82 / 95 ] loss: 0.849
[ 83 / 95 ] loss: 0.561
[ 84 / 95 ] loss: 0.700
[ 85 / 95 ] loss: 0.630
[ 86 / 95 ] loss: 0.647
[ 87 / 95 ] loss: 0.561
[ 88 / 95 ] loss: 0.577
[ 89 / 95 ] loss: 0.634
[ 90 / 95 ] loss: 0.483
[ 91 / 95 ] loss: 0.525
[ 92 / 95 ] loss: 0.633
[ 93 / 95 ] loss: 0.585
[ 94 / 95 ] loss: 0.504
[ 95 / 95 ] loss: 0.611
0.6219661700098138
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.582200
========= epoch: 20 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.510
[ 2 / 95 ] loss: 0.719
[ 3 / 95 ] loss: 0.710
[ 4 / 95 ] loss: 0.527
[ 5 / 95 ] loss: 0.713
[ 6 / 95 ] loss: 0.612
[ 7 / 95 ] loss: 0.626
[ 8 / 95 ] loss: 0.614
[ 9 / 95 ] loss: 0.553
[ 10 / 95 ] loss: 0.581
[ 11 / 95 ] loss: 0.512
[ 12 / 95 ] loss: 0.537
[ 13 / 95 ] loss: 0.430
[ 14 / 95 ] loss: 0.523
[ 15 / 95 ] loss: 0.557
[ 16 / 95 ] loss: 0.519
[ 17 / 95 ] loss: 0.666
[ 18 / 95 ] loss: 0.469
[ 19 / 95 ] loss: 0.522
[ 20 / 95 ] loss: 0.645
[ 21 / 95 ] loss: 0.541
[ 22 / 95 ] loss: 0.627
[ 23 / 95 ] loss: 0.587
[ 24 / 95 ] loss: 0.855
[ 25 / 95 ] loss: 0.620
[ 26 / 95 ] loss: 0.736
[ 27 / 95 ] loss: 0.564
[ 28 / 95 ] loss: 0.507
[ 29 / 95 ] loss: 0.484
[ 30 / 95 ] loss: 0.593
[ 31 / 95 ] loss: 0.574
[ 32 / 95 ] loss: 0.451
[ 33 / 95 ] loss: 0.629
[ 34 / 95 ] loss: 0.634
[ 35 / 95 ] loss: 0.486
[ 36 / 95 ] loss: 0.753
[ 37 / 95 ] loss: 0.769
[ 38 / 95 ] loss: 0.698
[ 39 / 95 ] loss: 0.565
[ 40 / 95 ] loss: 0.594
[ 41 / 95 ] loss: 0.589
[ 42 / 95 ] loss: 0.482
[ 43 / 95 ] loss: 0.827
[ 44 / 95 ] loss: 0.753
[ 45 / 95 ] loss: 0.398
[ 46 / 95 ] loss: 0.615
[ 47 / 95 ] loss: 0.714
[ 48 / 95 ] loss: 0.514
[ 49 / 95 ] loss: 0.571
[ 50 / 95 ] loss: 0.647
[ 51 / 95 ] loss: 0.725
[ 52 / 95 ] loss: 0.498
[ 53 / 95 ] loss: 0.506
[ 54 / 95 ] loss: 0.611
[ 55 / 95 ] loss: 0.710
[ 56 / 95 ] loss: 0.613
[ 57 / 95 ] loss: 0.618
[ 58 / 95 ] loss: 0.584
[ 59 / 95 ] loss: 0.674
[ 60 / 95 ] loss: 0.542
[ 61 / 95 ] loss: 0.396
[ 62 / 95 ] loss: 0.619
[ 63 / 95 ] loss: 0.744
[ 64 / 95 ] loss: 0.613
[ 65 / 95 ] loss: 0.814
[ 66 / 95 ] loss: 0.829
[ 67 / 95 ] loss: 0.746
[ 68 / 95 ] loss: 0.652
[ 69 / 95 ] loss: 0.784
[ 70 / 95 ] loss: 0.611
[ 71 / 95 ] loss: 0.512
[ 72 / 95 ] loss: 0.715
[ 73 / 95 ] loss: 0.816
[ 74 / 95 ] loss: 0.577
[ 75 / 95 ] loss: 0.651
[ 76 / 95 ] loss: 0.573
[ 77 / 95 ] loss: 0.576
[ 78 / 95 ] loss: 0.530
[ 79 / 95 ] loss: 0.474
[ 80 / 95 ] loss: 0.501
[ 81 / 95 ] loss: 0.563
[ 82 / 95 ] loss: 0.820
[ 83 / 95 ] loss: 0.509
[ 84 / 95 ] loss: 0.692
[ 85 / 95 ] loss: 0.549
[ 86 / 95 ] loss: 0.650
[ 87 / 95 ] loss: 0.536
[ 88 / 95 ] loss: 0.651
[ 89 / 95 ] loss: 0.746
[ 90 / 95 ] loss: 0.547
[ 91 / 95 ] loss: 0.573
[ 92 / 95 ] loss: 0.598
[ 93 / 95 ] loss: 0.597
[ 94 / 95 ] loss: 0.727
[ 95 / 95 ] loss: 0.559
0.6110855262530478
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.539747
========= epoch: 21 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.620
[ 2 / 95 ] loss: 0.690
[ 3 / 95 ] loss: 0.612
[ 4 / 95 ] loss: 0.594
[ 5 / 95 ] loss: 0.582
[ 6 / 95 ] loss: 0.565
[ 7 / 95 ] loss: 0.730
[ 8 / 95 ] loss: 0.518
[ 9 / 95 ] loss: 0.686
[ 10 / 95 ] loss: 0.666
[ 11 / 95 ] loss: 0.790
[ 12 / 95 ] loss: 0.666
[ 13 / 95 ] loss: 0.657
[ 14 / 95 ] loss: 0.576
[ 15 / 95 ] loss: 0.712
[ 16 / 95 ] loss: 0.460
[ 17 / 95 ] loss: 0.520
[ 18 / 95 ] loss: 0.518
[ 19 / 95 ] loss: 0.535
[ 20 / 95 ] loss: 0.522
[ 21 / 95 ] loss: 0.533
[ 22 / 95 ] loss: 0.544
[ 23 / 95 ] loss: 0.645
[ 24 / 95 ] loss: 0.512
[ 25 / 95 ] loss: 0.554
[ 26 / 95 ] loss: 0.730
[ 27 / 95 ] loss: 0.588
[ 28 / 95 ] loss: 0.640
[ 29 / 95 ] loss: 0.520
[ 30 / 95 ] loss: 0.588
[ 31 / 95 ] loss: 0.567
[ 32 / 95 ] loss: 0.630
[ 33 / 95 ] loss: 0.684
[ 34 / 95 ] loss: 0.430
[ 35 / 95 ] loss: 0.508
[ 36 / 95 ] loss: 0.446
[ 37 / 95 ] loss: 0.698
[ 38 / 95 ] loss: 0.688
[ 39 / 95 ] loss: 0.560
[ 40 / 95 ] loss: 0.452
[ 41 / 95 ] loss: 0.642
[ 42 / 95 ] loss: 0.443
[ 43 / 95 ] loss: 0.561
[ 44 / 95 ] loss: 0.510
[ 45 / 95 ] loss: 0.548
[ 46 / 95 ] loss: 0.797
[ 47 / 95 ] loss: 0.417
[ 48 / 95 ] loss: 0.423
[ 49 / 95 ] loss: 0.608
[ 50 / 95 ] loss: 0.699
[ 51 / 95 ] loss: 0.776
[ 52 / 95 ] loss: 0.698
[ 53 / 95 ] loss: 0.647
[ 54 / 95 ] loss: 0.676
[ 55 / 95 ] loss: 0.512
[ 56 / 95 ] loss: 0.722
[ 57 / 95 ] loss: 0.571
[ 58 / 95 ] loss: 0.651
[ 59 / 95 ] loss: 0.588
[ 60 / 95 ] loss: 0.602
[ 61 / 95 ] loss: 0.621
[ 62 / 95 ] loss: 0.612
[ 63 / 95 ] loss: 0.580
[ 64 / 95 ] loss: 0.544
[ 65 / 95 ] loss: 0.688
[ 66 / 95 ] loss: 0.695
[ 67 / 95 ] loss: 0.410
[ 68 / 95 ] loss: 0.605
[ 69 / 95 ] loss: 0.573
[ 70 / 95 ] loss: 0.667
[ 71 / 95 ] loss: 0.688
[ 72 / 95 ] loss: 0.684
[ 73 / 95 ] loss: 0.679
[ 74 / 95 ] loss: 0.620
[ 75 / 95 ] loss: 0.730
[ 76 / 95 ] loss: 0.629
[ 77 / 95 ] loss: 0.528
[ 78 / 95 ] loss: 0.535
[ 79 / 95 ] loss: 0.587
[ 80 / 95 ] loss: 0.709
[ 81 / 95 ] loss: 0.606
[ 82 / 95 ] loss: 0.736
[ 83 / 95 ] loss: 0.530
[ 84 / 95 ] loss: 0.672
[ 85 / 95 ] loss: 0.781
[ 86 / 95 ] loss: 0.588
[ 87 / 95 ] loss: 0.641
[ 88 / 95 ] loss: 0.685
[ 89 / 95 ] loss: 0.639
[ 90 / 95 ] loss: 0.603
[ 91 / 95 ] loss: 0.435
[ 92 / 95 ] loss: 0.502
[ 93 / 95 ] loss: 0.611
[ 94 / 95 ] loss: 0.638
[ 95 / 95 ] loss: 0.479
0.6037688446672339
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.528471
========= epoch: 22 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.704
[ 2 / 95 ] loss: 0.612
[ 3 / 95 ] loss: 0.505
[ 4 / 95 ] loss: 0.429
[ 5 / 95 ] loss: 0.518
[ 6 / 95 ] loss: 0.894
[ 7 / 95 ] loss: 0.537
[ 8 / 95 ] loss: 0.444
[ 9 / 95 ] loss: 0.561
[ 10 / 95 ] loss: 0.622
[ 11 / 95 ] loss: 0.621
[ 12 / 95 ] loss: 0.571
[ 13 / 95 ] loss: 0.514
[ 14 / 95 ] loss: 0.603
[ 15 / 95 ] loss: 0.506
[ 16 / 95 ] loss: 0.689
[ 17 / 95 ] loss: 0.647
[ 18 / 95 ] loss: 0.692
[ 19 / 95 ] loss: 0.768
[ 20 / 95 ] loss: 0.748
[ 21 / 95 ] loss: 0.787
[ 22 / 95 ] loss: 0.492
[ 23 / 95 ] loss: 0.474
[ 24 / 95 ] loss: 0.551
[ 25 / 95 ] loss: 0.629
[ 26 / 95 ] loss: 0.768
[ 27 / 95 ] loss: 0.661
[ 28 / 95 ] loss: 0.774
[ 29 / 95 ] loss: 0.710
[ 30 / 95 ] loss: 0.625
[ 31 / 95 ] loss: 0.535
[ 32 / 95 ] loss: 0.905
[ 33 / 95 ] loss: 0.647
[ 34 / 95 ] loss: 0.590
[ 35 / 95 ] loss: 0.579
[ 36 / 95 ] loss: 0.534
[ 37 / 95 ] loss: 0.478
[ 38 / 95 ] loss: 0.593
[ 39 / 95 ] loss: 0.558
[ 40 / 95 ] loss: 0.622
[ 41 / 95 ] loss: 0.520
[ 42 / 95 ] loss: 0.653
[ 43 / 95 ] loss: 0.564
[ 44 / 95 ] loss: 0.570
[ 45 / 95 ] loss: 0.641
[ 46 / 95 ] loss: 0.788
[ 47 / 95 ] loss: 0.786
[ 48 / 95 ] loss: 0.637
[ 49 / 95 ] loss: 0.499
[ 50 / 95 ] loss: 0.518
[ 51 / 95 ] loss: 0.631
[ 52 / 95 ] loss: 0.642
[ 53 / 95 ] loss: 0.672
[ 54 / 95 ] loss: 0.635
[ 55 / 95 ] loss: 0.722
[ 56 / 95 ] loss: 0.529
[ 57 / 95 ] loss: 0.826
[ 58 / 95 ] loss: 0.503
[ 59 / 95 ] loss: 0.737
[ 60 / 95 ] loss: 0.732
[ 61 / 95 ] loss: 0.616
[ 62 / 95 ] loss: 0.603
[ 63 / 95 ] loss: 0.677
[ 64 / 95 ] loss: 0.632
[ 65 / 95 ] loss: 0.732
[ 66 / 95 ] loss: 0.607
[ 67 / 95 ] loss: 0.719
[ 68 / 95 ] loss: 0.812
[ 69 / 95 ] loss: 0.472
[ 70 / 95 ] loss: 0.613
[ 71 / 95 ] loss: 0.502
[ 72 / 95 ] loss: 0.500
[ 73 / 95 ] loss: 0.505
[ 74 / 95 ] loss: 0.577
[ 75 / 95 ] loss: 0.557
[ 76 / 95 ] loss: 0.528
[ 77 / 95 ] loss: 0.588
[ 78 / 95 ] loss: 0.638
[ 79 / 95 ] loss: 0.614
[ 80 / 95 ] loss: 0.472
[ 81 / 95 ] loss: 0.598
[ 82 / 95 ] loss: 0.598
[ 83 / 95 ] loss: 0.624
[ 84 / 95 ] loss: 0.511
[ 85 / 95 ] loss: 0.446
[ 86 / 95 ] loss: 0.430
[ 87 / 95 ] loss: 0.652
[ 88 / 95 ] loss: 0.578
[ 89 / 95 ] loss: 0.706
[ 90 / 95 ] loss: 0.554
[ 91 / 95 ] loss: 0.567
[ 92 / 95 ] loss: 0.574
[ 93 / 95 ] loss: 0.593
[ 94 / 95 ] loss: 0.579
[ 95 / 95 ] loss: 0.695
0.6123376567112772
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./src/train.py:121: RuntimeWarning: invalid value encountered in double_scalars
  f1 = 2 * prc * rc / (prc + rc)
Accuracy: 0.424403 -- Precision: 0.000000 -- Recall: 0.000000 -- F1: nan -- AUC: 0.700749
========= epoch: 23 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.610
[ 2 / 95 ] loss: 0.458
[ 3 / 95 ] loss: 0.746
[ 4 / 95 ] loss: 0.610
[ 5 / 95 ] loss: 0.434
[ 6 / 95 ] loss: 0.619
[ 7 / 95 ] loss: 0.470
[ 8 / 95 ] loss: 0.520
[ 9 / 95 ] loss: 0.654
[ 10 / 95 ] loss: 0.647
[ 11 / 95 ] loss: 0.649
[ 12 / 95 ] loss: 0.561
[ 13 / 95 ] loss: 0.579
[ 14 / 95 ] loss: 0.563
[ 15 / 95 ] loss: 0.602
[ 16 / 95 ] loss: 0.723
[ 17 / 95 ] loss: 0.487
[ 18 / 95 ] loss: 0.505
[ 19 / 95 ] loss: 0.735
[ 20 / 95 ] loss: 0.807
[ 21 / 95 ] loss: 0.395
[ 22 / 95 ] loss: 0.694
[ 23 / 95 ] loss: 0.613
[ 24 / 95 ] loss: 0.497
[ 25 / 95 ] loss: 0.616
[ 26 / 95 ] loss: 0.635
[ 27 / 95 ] loss: 0.578
[ 28 / 95 ] loss: 0.565
[ 29 / 95 ] loss: 0.593
[ 30 / 95 ] loss: 0.613
[ 31 / 95 ] loss: 0.674
[ 32 / 95 ] loss: 0.596
[ 33 / 95 ] loss: 0.579
[ 34 / 95 ] loss: 0.744
[ 35 / 95 ] loss: 0.553
[ 36 / 95 ] loss: 0.544
[ 37 / 95 ] loss: 0.649
[ 38 / 95 ] loss: 0.521
[ 39 / 95 ] loss: 0.541
[ 40 / 95 ] loss: 0.585
[ 41 / 95 ] loss: 0.672
[ 42 / 95 ] loss: 0.654
[ 43 / 95 ] loss: 0.561
[ 44 / 95 ] loss: 0.584
[ 45 / 95 ] loss: 0.471
[ 46 / 95 ] loss: 0.719
[ 47 / 95 ] loss: 0.517
[ 48 / 95 ] loss: 0.496
[ 49 / 95 ] loss: 0.517
[ 50 / 95 ] loss: 0.525
[ 51 / 95 ] loss: 0.639
[ 52 / 95 ] loss: 0.649
[ 53 / 95 ] loss: 0.547
[ 54 / 95 ] loss: 0.739
[ 55 / 95 ] loss: 0.718
[ 56 / 95 ] loss: 0.804
[ 57 / 95 ] loss: 0.425
[ 58 / 95 ] loss: 0.663
[ 59 / 95 ] loss: 0.377
[ 60 / 95 ] loss: 0.652
[ 61 / 95 ] loss: 0.651
[ 62 / 95 ] loss: 0.665
[ 63 / 95 ] loss: 0.799
[ 64 / 95 ] loss: 0.465
[ 65 / 95 ] loss: 0.564
[ 66 / 95 ] loss: 0.592
[ 67 / 95 ] loss: 0.378
[ 68 / 95 ] loss: 0.458
[ 69 / 95 ] loss: 0.609
[ 70 / 95 ] loss: 0.635
[ 71 / 95 ] loss: 0.586
[ 72 / 95 ] loss: 0.747
[ 73 / 95 ] loss: 0.391
[ 74 / 95 ] loss: 0.524
[ 75 / 95 ] loss: 0.362
[ 76 / 95 ] loss: 0.560
[ 77 / 95 ] loss: 0.528
[ 78 / 95 ] loss: 0.420
[ 79 / 95 ] loss: 0.724
[ 80 / 95 ] loss: 0.569
[ 81 / 95 ] loss: 0.729
[ 82 / 95 ] loss: 0.565
[ 83 / 95 ] loss: 0.592
[ 84 / 95 ] loss: 0.586
[ 85 / 95 ] loss: 0.596
[ 86 / 95 ] loss: 0.334
[ 87 / 95 ] loss: 0.531
[ 88 / 95 ] loss: 0.769
[ 89 / 95 ] loss: 0.511
[ 90 / 95 ] loss: 0.371
[ 91 / 95 ] loss: 0.771
[ 92 / 95 ] loss: 0.470
[ 93 / 95 ] loss: 0.542
[ 94 / 95 ] loss: 0.567
[ 95 / 95 ] loss: 0.581
0.5835376494809201
Accuracy: 0.734748 -- Precision: 0.698305 -- Recall: 0.949309 -- F1: 0.804688 -- AUC: 0.669470
========= epoch: 24 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.518
[ 2 / 95 ] loss: 0.488
[ 3 / 95 ] loss: 0.620
[ 4 / 95 ] loss: 0.619
[ 5 / 95 ] loss: 0.570
[ 6 / 95 ] loss: 0.484
[ 7 / 95 ] loss: 0.530
[ 8 / 95 ] loss: 0.521
[ 9 / 95 ] loss: 0.491
[ 10 / 95 ] loss: 0.491
[ 11 / 95 ] loss: 0.538
[ 12 / 95 ] loss: 0.670
[ 13 / 95 ] loss: 0.567
[ 14 / 95 ] loss: 0.561
[ 15 / 95 ] loss: 0.615
[ 16 / 95 ] loss: 0.656
[ 17 / 95 ] loss: 0.512
[ 18 / 95 ] loss: 0.656
[ 19 / 95 ] loss: 0.817
[ 20 / 95 ] loss: 0.629
[ 21 / 95 ] loss: 0.641
[ 22 / 95 ] loss: 0.707
[ 23 / 95 ] loss: 0.533
[ 24 / 95 ] loss: 0.518
[ 25 / 95 ] loss: 0.622
[ 26 / 95 ] loss: 0.728
[ 27 / 95 ] loss: 0.693
[ 28 / 95 ] loss: 0.733
[ 29 / 95 ] loss: 0.762
[ 30 / 95 ] loss: 0.696
[ 31 / 95 ] loss: 0.689
[ 32 / 95 ] loss: 0.694
[ 33 / 95 ] loss: 0.724
[ 34 / 95 ] loss: 0.699
[ 35 / 95 ] loss: 0.675
[ 36 / 95 ] loss: 0.714
[ 37 / 95 ] loss: 0.687
[ 38 / 95 ] loss: 0.702
[ 39 / 95 ] loss: 0.687
[ 40 / 95 ] loss: 0.687
[ 41 / 95 ] loss: 0.679
[ 42 / 95 ] loss: 0.669
[ 43 / 95 ] loss: 0.675
[ 44 / 95 ] loss: 0.663
[ 45 / 95 ] loss: 0.741
[ 46 / 95 ] loss: 0.666
[ 47 / 95 ] loss: 0.615
[ 48 / 95 ] loss: 0.658
[ 49 / 95 ] loss: 0.691
[ 50 / 95 ] loss: 0.839
[ 51 / 95 ] loss: 0.658
[ 52 / 95 ] loss: 0.634
[ 53 / 95 ] loss: 0.747
[ 54 / 95 ] loss: 0.600
[ 55 / 95 ] loss: 0.685
[ 56 / 95 ] loss: 0.714
[ 57 / 95 ] loss: 0.624
[ 58 / 95 ] loss: 0.690
[ 59 / 95 ] loss: 0.693
[ 60 / 95 ] loss: 0.618
[ 61 / 95 ] loss: 0.653
[ 62 / 95 ] loss: 0.616
[ 63 / 95 ] loss: 0.747
[ 64 / 95 ] loss: 0.630
[ 65 / 95 ] loss: 0.606
[ 66 / 95 ] loss: 0.564
[ 67 / 95 ] loss: 0.731
[ 68 / 95 ] loss: 0.663
[ 69 / 95 ] loss: 0.631
[ 70 / 95 ] loss: 0.764
[ 71 / 95 ] loss: 0.593
[ 72 / 95 ] loss: 0.679
[ 73 / 95 ] loss: 0.861
[ 74 / 95 ] loss: 0.795
[ 75 / 95 ] loss: 0.690
[ 76 / 95 ] loss: 0.610
[ 77 / 95 ] loss: 0.626
[ 78 / 95 ] loss: 0.664
[ 79 / 95 ] loss: 0.548
[ 80 / 95 ] loss: 0.570
[ 81 / 95 ] loss: 0.777
[ 82 / 95 ] loss: 0.709
[ 83 / 95 ] loss: 0.720
[ 84 / 95 ] loss: 0.638
[ 85 / 95 ] loss: 0.680
[ 86 / 95 ] loss: 0.704
[ 87 / 95 ] loss: 0.740
[ 88 / 95 ] loss: 0.681
[ 89 / 95 ] loss: 0.671
[ 90 / 95 ] loss: 0.699
[ 91 / 95 ] loss: 0.693
[ 92 / 95 ] loss: 0.696
[ 93 / 95 ] loss: 0.737
[ 94 / 95 ] loss: 0.687
[ 95 / 95 ] loss: 0.725
0.6578809135838559
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.625706
========= epoch: 25 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.699
[ 2 / 95 ] loss: 0.691
[ 3 / 95 ] loss: 0.691
[ 4 / 95 ] loss: 0.704
[ 5 / 95 ] loss: 0.733
[ 6 / 95 ] loss: 0.715
[ 7 / 95 ] loss: 0.690
[ 8 / 95 ] loss: 0.687
[ 9 / 95 ] loss: 0.682
[ 10 / 95 ] loss: 0.707
[ 11 / 95 ] loss: 0.706
[ 12 / 95 ] loss: 0.682
[ 13 / 95 ] loss: 0.700
[ 14 / 95 ] loss: 0.680
[ 15 / 95 ] loss: 0.692
[ 16 / 95 ] loss: 0.683
[ 17 / 95 ] loss: 0.679
[ 18 / 95 ] loss: 0.656
[ 19 / 95 ] loss: 0.643
[ 20 / 95 ] loss: 0.668
[ 21 / 95 ] loss: 0.699
[ 22 / 95 ] loss: 0.630
[ 23 / 95 ] loss: 0.660
[ 24 / 95 ] loss: 0.716
[ 25 / 95 ] loss: 0.771
[ 26 / 95 ] loss: 0.688
[ 27 / 95 ] loss: 0.695
[ 28 / 95 ] loss: 0.726
[ 29 / 95 ] loss: 0.691
[ 30 / 95 ] loss: 0.664
[ 31 / 95 ] loss: 0.757
[ 32 / 95 ] loss: 0.704
[ 33 / 95 ] loss: 0.639
[ 34 / 95 ] loss: 0.649
[ 35 / 95 ] loss: 0.717
[ 36 / 95 ] loss: 0.694
[ 37 / 95 ] loss: 0.643
[ 38 / 95 ] loss: 0.594
[ 39 / 95 ] loss: 0.678
[ 40 / 95 ] loss: 0.687
[ 41 / 95 ] loss: 0.609
[ 42 / 95 ] loss: 0.643
[ 43 / 95 ] loss: 0.676
[ 44 / 95 ] loss: 0.684
[ 45 / 95 ] loss: 0.747
[ 46 / 95 ] loss: 0.625
[ 47 / 95 ] loss: 0.635
[ 48 / 95 ] loss: 0.694
[ 49 / 95 ] loss: 0.672
[ 50 / 95 ] loss: 0.661
[ 51 / 95 ] loss: 0.724
[ 52 / 95 ] loss: 0.718
[ 53 / 95 ] loss: 0.722
[ 54 / 95 ] loss: 0.717
[ 55 / 95 ] loss: 0.603
[ 56 / 95 ] loss: 0.672
[ 57 / 95 ] loss: 0.690
[ 58 / 95 ] loss: 0.684
[ 59 / 95 ] loss: 0.678
[ 60 / 95 ] loss: 0.632
[ 61 / 95 ] loss: 0.684
[ 62 / 95 ] loss: 0.666
[ 63 / 95 ] loss: 0.689
[ 64 / 95 ] loss: 0.640
[ 65 / 95 ] loss: 0.677
[ 66 / 95 ] loss: 0.694
[ 67 / 95 ] loss: 0.673
[ 68 / 95 ] loss: 0.658
[ 69 / 95 ] loss: 0.662
[ 70 / 95 ] loss: 0.687
[ 71 / 95 ] loss: 0.646
[ 72 / 95 ] loss: 0.637
[ 73 / 95 ] loss: 0.638
[ 74 / 95 ] loss: 0.595
[ 75 / 95 ] loss: 0.604
[ 76 / 95 ] loss: 0.642
[ 77 / 95 ] loss: 0.668
[ 78 / 95 ] loss: 0.766
[ 79 / 95 ] loss: 0.732
[ 80 / 95 ] loss: 0.621
[ 81 / 95 ] loss: 0.687
[ 82 / 95 ] loss: 0.708
[ 83 / 95 ] loss: 0.670
[ 84 / 95 ] loss: 0.747
[ 85 / 95 ] loss: 0.687
[ 86 / 95 ] loss: 0.586
[ 87 / 95 ] loss: 0.653
[ 88 / 95 ] loss: 0.693
[ 89 / 95 ] loss: 0.665
[ 90 / 95 ] loss: 0.677
[ 91 / 95 ] loss: 0.624
[ 92 / 95 ] loss: 0.595
[ 93 / 95 ] loss: 0.714
[ 94 / 95 ] loss: 0.737
[ 95 / 95 ] loss: 0.808
0.678985321521759
Accuracy: 0.575597 -- Precision: 0.575597 -- Recall: 1.000000 -- F1: 0.730640 -- AUC: 0.772595
========= epoch: 26 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.699
[ 2 / 95 ] loss: 0.579
[ 3 / 95 ] loss: 0.743
[ 4 / 95 ] loss: 0.689
[ 5 / 95 ] loss: 0.592
[ 6 / 95 ] loss: 0.620
[ 7 / 95 ] loss: 0.580
[ 8 / 95 ] loss: 0.691
[ 9 / 95 ] loss: 0.704
[ 10 / 95 ] loss: 0.690
[ 11 / 95 ] loss: 0.616
[ 12 / 95 ] loss: 0.628
[ 13 / 95 ] loss: 0.785
[ 14 / 95 ] loss: 0.595
[ 15 / 95 ] loss: 0.798
[ 16 / 95 ] loss: 0.647
[ 17 / 95 ] loss: 0.597
[ 18 / 95 ] loss: 0.671
[ 19 / 95 ] loss: 0.706
[ 20 / 95 ] loss: 0.708
[ 21 / 95 ] loss: 0.681
[ 22 / 95 ] loss: 0.633
[ 23 / 95 ] loss: 0.643
[ 24 / 95 ] loss: 0.678
[ 25 / 95 ] loss: 0.718
[ 26 / 95 ] loss: 0.719
[ 27 / 95 ] loss: 0.696
[ 28 / 95 ] loss: 0.646
[ 29 / 95 ] loss: 0.668
[ 30 / 95 ] loss: 0.681
[ 31 / 95 ] loss: 0.635
[ 32 / 95 ] loss: 0.732
[ 33 / 95 ] loss: 0.711
[ 34 / 95 ] loss: 0.700
[ 35 / 95 ] loss: 0.676
[ 36 / 95 ] loss: 0.671
[ 37 / 95 ] loss: 0.678
[ 38 / 95 ] loss: 0.657
[ 39 / 95 ] loss: 0.676
[ 40 / 95 ] loss: 0.686
[ 41 / 95 ] loss: 0.686
[ 42 / 95 ] loss: 0.673
[ 43 / 95 ] loss: 0.665
[ 44 / 95 ] loss: 0.673
[ 45 / 95 ] loss: 0.715
[ 46 / 95 ] loss: 0.653
[ 47 / 95 ] loss: 0.661
[ 48 / 95 ] loss: 0.748
[ 49 / 95 ] loss: 0.651
[ 50 / 95 ] loss: 0.658
[ 51 / 95 ] loss: 0.681
[ 52 / 95 ] loss: 0.647
[ 53 / 95 ] loss: 0.684
[ 54 / 95 ] loss: 0.614
[ 55 / 95 ] loss: 0.615
[ 56 / 95 ] loss: 0.633
[ 57 / 95 ] loss: 0.689
[ 58 / 95 ] loss: 0.598
[ 59 / 95 ] loss: 0.580
[ 60 / 95 ] loss: 0.697
[ 61 / 95 ] loss: 0.640
[ 62 / 95 ] loss: 0.665
[ 63 / 95 ] loss: 0.562
[ 64 / 95 ] loss: 0.646
[ 65 / 95 ] loss: 0.631
[ 66 / 95 ] loss: 0.637
[ 67 / 95 ] loss: 0.680
[ 68 / 95 ] loss: 0.665
[ 69 / 95 ] loss: 0.573
[ 70 / 95 ] loss: 0.595
[ 71 / 95 ] loss: 0.654
[ 72 / 95 ] loss: 0.760
[ 73 / 95 ] loss: 0.501
[ 74 / 95 ] loss: 0.641
[ 75 / 95 ] loss: 0.502
[ 76 / 95 ] loss: 0.551
[ 77 / 95 ] loss: 0.611
[ 78 / 95 ] loss: 0.640
[ 79 / 95 ] loss: 0.529
[ 80 / 95 ] loss: 0.712
[ 81 / 95 ] loss: 0.631
[ 82 / 95 ] loss: 0.754
[ 83 / 95 ] loss: 0.547
[ 84 / 95 ] loss: 0.671
[ 85 / 95 ] loss: 0.720
[ 86 / 95 ] loss: 0.593
[ 87 / 95 ] loss: 0.538
[ 88 / 95 ] loss: 0.649
[ 89 / 95 ] loss: 0.612
[ 90 / 95 ] loss: 0.594
[ 91 / 95 ] loss: 0.677
[ 92 / 95 ] loss: 0.647
[ 93 / 95 ] loss: 0.653
[ 94 / 95 ] loss: 0.665
[ 95 / 95 ] loss: 0.397
0.651450208613747
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./src/train.py:121: RuntimeWarning: invalid value encountered in double_scalars
  f1 = 2 * prc * rc / (prc + rc)
Accuracy: 0.424403 -- Precision: 0.000000 -- Recall: 0.000000 -- F1: nan -- AUC: 0.776512
========= epoch: 27 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.533
[ 2 / 95 ] loss: 0.539
[ 3 / 95 ] loss: 0.585
[ 4 / 95 ] loss: 0.605
[ 5 / 95 ] loss: 0.664
[ 6 / 95 ] loss: 0.477
[ 7 / 95 ] loss: 0.501
[ 8 / 95 ] loss: 0.606
[ 9 / 95 ] loss: 0.686
[ 10 / 95 ] loss: 0.598
[ 11 / 95 ] loss: 0.616
[ 12 / 95 ] loss: 0.821
[ 13 / 95 ] loss: 0.621
[ 14 / 95 ] loss: 0.631
[ 15 / 95 ] loss: 0.771
[ 16 / 95 ] loss: 0.839
[ 17 / 95 ] loss: 0.965
[ 18 / 95 ] loss: 0.566
[ 19 / 95 ] loss: 0.505
[ 20 / 95 ] loss: 0.734
[ 21 / 95 ] loss: 0.656
[ 22 / 95 ] loss: 0.758
[ 23 / 95 ] loss: 0.844
[ 24 / 95 ] loss: 0.622
[ 25 / 95 ] loss: 0.594
[ 26 / 95 ] loss: 0.609
[ 27 / 95 ] loss: 0.649
[ 28 / 95 ] loss: 0.631
[ 29 / 95 ] loss: 0.708
[ 30 / 95 ] loss: 0.677
[ 31 / 95 ] loss: 0.646
[ 32 / 95 ] loss: 0.596
[ 33 / 95 ] loss: 0.661
[ 34 / 95 ] loss: 0.620
[ 35 / 95 ] loss: 0.612
[ 36 / 95 ] loss: 0.626
[ 37 / 95 ] loss: 0.627
[ 38 / 95 ] loss: 0.753
[ 39 / 95 ] loss: 0.633
[ 40 / 95 ] loss: 0.619
[ 41 / 95 ] loss: 0.547
[ 42 / 95 ] loss: 0.621
[ 43 / 95 ] loss: 0.536
[ 44 / 95 ] loss: 0.545
[ 45 / 95 ] loss: 0.687
[ 46 / 95 ] loss: 0.653
[ 47 / 95 ] loss: 0.756
[ 48 / 95 ] loss: 0.673
[ 49 / 95 ] loss: 0.641
[ 50 / 95 ] loss: 0.685
[ 51 / 95 ] loss: 0.731
[ 52 / 95 ] loss: 0.652
[ 53 / 95 ] loss: 0.561
[ 54 / 95 ] loss: 0.698
[ 55 / 95 ] loss: 0.639
[ 56 / 95 ] loss: 0.596
[ 57 / 95 ] loss: 0.751
[ 58 / 95 ] loss: 0.553
[ 59 / 95 ] loss: 0.641
[ 60 / 95 ] loss: 0.586
[ 61 / 95 ] loss: 0.694
[ 62 / 95 ] loss: 0.533
[ 63 / 95 ] loss: 0.593
[ 64 / 95 ] loss: 0.483
[ 65 / 95 ] loss: 0.572
[ 66 / 95 ] loss: 0.596
[ 67 / 95 ] loss: 0.484
[ 68 / 95 ] loss: 0.509
[ 69 / 95 ] loss: 0.588
[ 70 / 95 ] loss: 0.642
[ 71 / 95 ] loss: 0.578
[ 72 / 95 ] loss: 0.593
[ 73 / 95 ] loss: 0.670
[ 74 / 95 ] loss: 0.584
[ 75 / 95 ] loss: 0.547
[ 76 / 95 ] loss: 0.425
[ 77 / 95 ] loss: 0.542
[ 78 / 95 ] loss: 0.676
[ 79 / 95 ] loss: 0.537
[ 80 / 95 ] loss: 0.890
[ 81 / 95 ] loss: 0.588
[ 82 / 95 ] loss: 0.677
[ 83 / 95 ] loss: 0.320
[ 84 / 95 ] loss: 0.726
[ 85 / 95 ] loss: 0.788
[ 86 / 95 ] loss: 0.542
[ 87 / 95 ] loss: 0.579
[ 88 / 95 ] loss: 0.670
[ 89 / 95 ] loss: 0.587
[ 90 / 95 ] loss: 0.645
[ 91 / 95 ] loss: 0.607
[ 92 / 95 ] loss: 0.694
[ 93 / 95 ] loss: 0.661
[ 94 / 95 ] loss: 0.604
[ 95 / 95 ] loss: 0.696
0.6302771656136764
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./src/train.py:121: RuntimeWarning: invalid value encountered in double_scalars
  f1 = 2 * prc * rc / (prc + rc)
Accuracy: 0.424403 -- Precision: 0.000000 -- Recall: 0.000000 -- F1: nan -- AUC: 0.780400
========= epoch: 28 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.662
[ 2 / 95 ] loss: 0.675
[ 3 / 95 ] loss: 0.651
[ 4 / 95 ] loss: 0.611
[ 5 / 95 ] loss: 0.494
[ 6 / 95 ] loss: 0.555
[ 7 / 95 ] loss: 0.637
[ 8 / 95 ] loss: 0.502
[ 9 / 95 ] loss: 0.717
[ 10 / 95 ] loss: 0.665
[ 11 / 95 ] loss: 0.711
[ 12 / 95 ] loss: 0.536
[ 13 / 95 ] loss: 0.945
[ 14 / 95 ] loss: 0.663
[ 15 / 95 ] loss: 0.739
[ 16 / 95 ] loss: 0.757
[ 17 / 95 ] loss: 0.625
[ 18 / 95 ] loss: 0.605
[ 19 / 95 ] loss: 0.531
[ 20 / 95 ] loss: 0.481
[ 21 / 95 ] loss: 0.646
[ 22 / 95 ] loss: 0.683
[ 23 / 95 ] loss: 0.638
[ 24 / 95 ] loss: 0.608
[ 25 / 95 ] loss: 0.585
[ 26 / 95 ] loss: 0.625
[ 27 / 95 ] loss: 0.634
[ 28 / 95 ] loss: 0.642
[ 29 / 95 ] loss: 0.637
[ 30 / 95 ] loss: 0.586
[ 31 / 95 ] loss: 0.719
[ 32 / 95 ] loss: 0.602
[ 33 / 95 ] loss: 0.646
[ 34 / 95 ] loss: 0.674
[ 35 / 95 ] loss: 0.660
[ 36 / 95 ] loss: 0.590
[ 37 / 95 ] loss: 0.674
[ 38 / 95 ] loss: 0.680
[ 39 / 95 ] loss: 0.619
[ 40 / 95 ] loss: 0.656
[ 41 / 95 ] loss: 0.577
[ 42 / 95 ] loss: 0.634
[ 43 / 95 ] loss: 0.678
[ 44 / 95 ] loss: 0.654
[ 45 / 95 ] loss: 0.499
[ 46 / 95 ] loss: 0.670
[ 47 / 95 ] loss: 0.601
[ 48 / 95 ] loss: 0.650
[ 49 / 95 ] loss: 0.609
[ 50 / 95 ] loss: 0.635
[ 51 / 95 ] loss: 0.564
[ 52 / 95 ] loss: 0.573
[ 53 / 95 ] loss: 0.480
[ 54 / 95 ] loss: 0.695
[ 55 / 95 ] loss: 0.532
[ 56 / 95 ] loss: 0.603
[ 57 / 95 ] loss: 0.571
[ 58 / 95 ] loss: 0.648
[ 59 / 95 ] loss: 0.772
[ 60 / 95 ] loss: 0.643
[ 61 / 95 ] loss: 0.548
[ 62 / 95 ] loss: 0.683
[ 63 / 95 ] loss: 0.687
[ 64 / 95 ] loss: 0.450
[ 65 / 95 ] loss: 0.660
[ 66 / 95 ] loss: 0.563
[ 67 / 95 ] loss: 0.699
[ 68 / 95 ] loss: 0.508
[ 69 / 95 ] loss: 0.606
[ 70 / 95 ] loss: 0.754
[ 71 / 95 ] loss: 0.668
[ 72 / 95 ] loss: 0.494
[ 73 / 95 ] loss: 0.556
[ 74 / 95 ] loss: 0.607
[ 75 / 95 ] loss: 0.505
[ 76 / 95 ] loss: 0.639
[ 77 / 95 ] loss: 0.534
[ 78 / 95 ] loss: 0.788
[ 79 / 95 ] loss: 0.637
[ 80 / 95 ] loss: 0.711
[ 81 / 95 ] loss: 0.453
[ 82 / 95 ] loss: 0.687
[ 83 / 95 ] loss: 0.614
[ 84 / 95 ] loss: 0.497
[ 85 / 95 ] loss: 0.690
[ 86 / 95 ] loss: 0.655
[ 87 / 95 ] loss: 0.630
[ 88 / 95 ] loss: 0.601
[ 89 / 95 ] loss: 0.668
[ 90 / 95 ] loss: 0.645
[ 91 / 95 ] loss: 0.673
[ 92 / 95 ] loss: 0.533
[ 93 / 95 ] loss: 0.758
[ 94 / 95 ] loss: 0.681
[ 95 / 95 ] loss: 0.522
0.6258738593051308
Accuracy: 0.580902 -- Precision: 0.578667 -- Recall: 1.000000 -- F1: 0.733108 -- AUC: 0.768419
========= epoch: 29 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.656
[ 2 / 95 ] loss: 0.524
[ 3 / 95 ] loss: 0.704
[ 4 / 95 ] loss: 0.797
[ 5 / 95 ] loss: 0.590
[ 6 / 95 ] loss: 0.649
[ 7 / 95 ] loss: 0.644
[ 8 / 95 ] loss: 0.627
[ 9 / 95 ] loss: 0.562
[ 10 / 95 ] loss: 0.589
[ 11 / 95 ] loss: 0.626
[ 12 / 95 ] loss: 0.577
[ 13 / 95 ] loss: 0.747
[ 14 / 95 ] loss: 0.571
[ 15 / 95 ] loss: 0.593
[ 16 / 95 ] loss: 0.593
[ 17 / 95 ] loss: 0.655
[ 18 / 95 ] loss: 0.643
[ 19 / 95 ] loss: 0.596
[ 20 / 95 ] loss: 0.613
[ 21 / 95 ] loss: 0.559
[ 22 / 95 ] loss: 0.612
[ 23 / 95 ] loss: 0.662
[ 24 / 95 ] loss: 0.582
[ 25 / 95 ] loss: 0.615
[ 26 / 95 ] loss: 0.650
[ 27 / 95 ] loss: 0.552
[ 28 / 95 ] loss: 0.553
[ 29 / 95 ] loss: 0.481
[ 30 / 95 ] loss: 0.510
[ 31 / 95 ] loss: 0.784
[ 32 / 95 ] loss: 0.646
[ 33 / 95 ] loss: 0.587
[ 34 / 95 ] loss: 0.557
[ 35 / 95 ] loss: 0.635
[ 36 / 95 ] loss: 0.615
[ 37 / 95 ] loss: 0.742
[ 38 / 95 ] loss: 0.560
[ 39 / 95 ] loss: 0.733
[ 40 / 95 ] loss: 0.685
[ 41 / 95 ] loss: 0.514
[ 42 / 95 ] loss: 0.497
[ 43 / 95 ] loss: 0.609
[ 44 / 95 ] loss: 0.605
[ 45 / 95 ] loss: 0.444
[ 46 / 95 ] loss: 0.484
[ 47 / 95 ] loss: 0.514
[ 48 / 95 ] loss: 0.499
[ 49 / 95 ] loss: 0.736
[ 50 / 95 ] loss: 0.611
[ 51 / 95 ] loss: 0.653
[ 52 / 95 ] loss: 0.744
[ 53 / 95 ] loss: 0.543
[ 54 / 95 ] loss: 0.664
[ 55 / 95 ] loss: 0.633
[ 56 / 95 ] loss: 0.490
[ 57 / 95 ] loss: 0.565
[ 58 / 95 ] loss: 0.668
[ 59 / 95 ] loss: 0.848
[ 60 / 95 ] loss: 0.723
[ 61 / 95 ] loss: 0.554
[ 62 / 95 ] loss: 0.597
[ 63 / 95 ] loss: 0.649
[ 64 / 95 ] loss: 0.641
[ 65 / 95 ] loss: 0.539
[ 66 / 95 ] loss: 0.697
[ 67 / 95 ] loss: 0.596
[ 68 / 95 ] loss: 0.472
[ 69 / 95 ] loss: 0.443
[ 70 / 95 ] loss: 0.500
[ 71 / 95 ] loss: 0.629
[ 72 / 95 ] loss: 0.551
[ 73 / 95 ] loss: 0.492
[ 74 / 95 ] loss: 0.608
[ 75 / 95 ] loss: 0.598
[ 76 / 95 ] loss: 0.493
[ 77 / 95 ] loss: 0.652
[ 78 / 95 ] loss: 0.552
[ 79 / 95 ] loss: 0.547
[ 80 / 95 ] loss: 0.572
[ 81 / 95 ] loss: 0.629
[ 82 / 95 ] loss: 0.654
[ 83 / 95 ] loss: 0.705
[ 84 / 95 ] loss: 0.648
[ 85 / 95 ] loss: 0.434
[ 86 / 95 ] loss: 0.688
[ 87 / 95 ] loss: 0.885
[ 88 / 95 ] loss: 0.487
[ 89 / 95 ] loss: 0.549
[ 90 / 95 ] loss: 0.550
[ 91 / 95 ] loss: 0.521
[ 92 / 95 ] loss: 0.591
[ 93 / 95 ] loss: 0.566
[ 94 / 95 ] loss: 0.651
[ 95 / 95 ] loss: 0.454
0.6033334675588106
Accuracy: 0.586207 -- Precision: 0.581769 -- Recall: 1.000000 -- F1: 0.735593 -- AUC: 0.761305
========= epoch: 30 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.520
[ 2 / 95 ] loss: 0.596
[ 3 / 95 ] loss: 0.817
[ 4 / 95 ] loss: 0.348
[ 5 / 95 ] loss: 0.898
[ 6 / 95 ] loss: 0.486
[ 7 / 95 ] loss: 0.581
[ 8 / 95 ] loss: 0.556
[ 9 / 95 ] loss: 0.492
[ 10 / 95 ] loss: 0.519
[ 11 / 95 ] loss: 0.475
[ 12 / 95 ] loss: 0.639
[ 13 / 95 ] loss: 0.497
[ 14 / 95 ] loss: 0.501
[ 15 / 95 ] loss: 0.621
[ 16 / 95 ] loss: 0.464
[ 17 / 95 ] loss: 0.565
[ 18 / 95 ] loss: 0.799
[ 19 / 95 ] loss: 0.779
[ 20 / 95 ] loss: 0.697
[ 21 / 95 ] loss: 0.544
[ 22 / 95 ] loss: 0.548
[ 23 / 95 ] loss: 0.568
[ 24 / 95 ] loss: 0.473
[ 25 / 95 ] loss: 0.567
[ 26 / 95 ] loss: 0.511
[ 27 / 95 ] loss: 0.434
[ 28 / 95 ] loss: 0.589
[ 29 / 95 ] loss: 0.655
[ 30 / 95 ] loss: 0.428
[ 31 / 95 ] loss: 1.173
[ 32 / 95 ] loss: 0.854
[ 33 / 95 ] loss: 0.561
[ 34 / 95 ] loss: 0.646
[ 35 / 95 ] loss: 0.566
[ 36 / 95 ] loss: 0.596
[ 37 / 95 ] loss: 0.535
[ 38 / 95 ] loss: 0.467
[ 39 / 95 ] loss: 0.529
[ 40 / 95 ] loss: 0.645
[ 41 / 95 ] loss: 0.599
[ 42 / 95 ] loss: 0.694
[ 43 / 95 ] loss: 0.573
[ 44 / 95 ] loss: 0.591
[ 45 / 95 ] loss: 0.606
[ 46 / 95 ] loss: 0.615
[ 47 / 95 ] loss: 0.432
[ 48 / 95 ] loss: 0.615
[ 49 / 95 ] loss: 0.600
[ 50 / 95 ] loss: 0.727
[ 51 / 95 ] loss: 0.600
[ 52 / 95 ] loss: 0.541
[ 53 / 95 ] loss: 0.456
[ 54 / 95 ] loss: 0.549
[ 55 / 95 ] loss: 0.594
[ 56 / 95 ] loss: 0.563
[ 57 / 95 ] loss: 0.584
[ 58 / 95 ] loss: 0.676
[ 59 / 95 ] loss: 0.552
[ 60 / 95 ] loss: 0.664
[ 61 / 95 ] loss: 0.655
[ 62 / 95 ] loss: 0.780
[ 63 / 95 ] loss: 0.518
[ 64 / 95 ] loss: 0.553
[ 65 / 95 ] loss: 0.505
[ 66 / 95 ] loss: 0.546
[ 67 / 95 ] loss: 0.685
[ 68 / 95 ] loss: 0.715
[ 69 / 95 ] loss: 0.634
[ 70 / 95 ] loss: 0.730
[ 71 / 95 ] loss: 0.552
[ 72 / 95 ] loss: 0.615
[ 73 / 95 ] loss: 0.582
[ 74 / 95 ] loss: 0.666
[ 75 / 95 ] loss: 0.625
[ 76 / 95 ] loss: 0.564
[ 77 / 95 ] loss: 0.656
[ 78 / 95 ] loss: 0.540
[ 79 / 95 ] loss: 0.673
[ 80 / 95 ] loss: 0.622
[ 81 / 95 ] loss: 0.549
[ 82 / 95 ] loss: 0.652
[ 83 / 95 ] loss: 0.818
[ 84 / 95 ] loss: 0.735
[ 85 / 95 ] loss: 0.696
[ 86 / 95 ] loss: 0.648
[ 87 / 95 ] loss: 0.748
[ 88 / 95 ] loss: 0.542
[ 89 / 95 ] loss: 0.693
[ 90 / 95 ] loss: 0.616
[ 91 / 95 ] loss: 0.525
[ 92 / 95 ] loss: 0.580
[ 93 / 95 ] loss: 0.597
[ 94 / 95 ] loss: 0.476
[ 95 / 95 ] loss: 0.596
0.60488232907496
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./src/train.py:121: RuntimeWarning: invalid value encountered in double_scalars
  f1 = 2 * prc * rc / (prc + rc)
Accuracy: 0.424403 -- Precision: 0.000000 -- Recall: 0.000000 -- F1: nan -- AUC: 0.755040
========= epoch: 31 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.630
[ 2 / 95 ] loss: 0.620
[ 3 / 95 ] loss: 0.624
[ 4 / 95 ] loss: 0.526
[ 5 / 95 ] loss: 0.533
[ 6 / 95 ] loss: 0.511
[ 7 / 95 ] loss: 0.520
[ 8 / 95 ] loss: 0.702
[ 9 / 95 ] loss: 0.708
[ 10 / 95 ] loss: 0.525
[ 11 / 95 ] loss: 0.579
[ 12 / 95 ] loss: 0.451
[ 13 / 95 ] loss: 0.800
[ 14 / 95 ] loss: 0.550
[ 15 / 95 ] loss: 0.660
[ 16 / 95 ] loss: 0.579
[ 17 / 95 ] loss: 0.599
[ 18 / 95 ] loss: 0.776
[ 19 / 95 ] loss: 0.799
[ 20 / 95 ] loss: 0.589
[ 21 / 95 ] loss: 0.632
[ 22 / 95 ] loss: 0.738
[ 23 / 95 ] loss: 0.508
[ 24 / 95 ] loss: 0.548
[ 25 / 95 ] loss: 0.666
[ 26 / 95 ] loss: 0.759
[ 27 / 95 ] loss: 0.713
[ 28 / 95 ] loss: 0.526
[ 29 / 95 ] loss: 0.734
[ 30 / 95 ] loss: 0.711
[ 31 / 95 ] loss: 0.481
[ 32 / 95 ] loss: 0.550
[ 33 / 95 ] loss: 0.595
[ 34 / 95 ] loss: 0.763
[ 35 / 95 ] loss: 0.507
[ 36 / 95 ] loss: 0.579
[ 37 / 95 ] loss: 0.724
[ 38 / 95 ] loss: 0.759
[ 39 / 95 ] loss: 0.623
[ 40 / 95 ] loss: 0.628
[ 41 / 95 ] loss: 0.594
[ 42 / 95 ] loss: 0.616
[ 43 / 95 ] loss: 0.515
[ 44 / 95 ] loss: 0.460
[ 45 / 95 ] loss: 0.537
[ 46 / 95 ] loss: 0.623
[ 47 / 95 ] loss: 0.557
[ 48 / 95 ] loss: 0.722
[ 49 / 95 ] loss: 0.574
[ 50 / 95 ] loss: 0.639
[ 51 / 95 ] loss: 0.602
[ 52 / 95 ] loss: 0.516
[ 53 / 95 ] loss: 0.618
[ 54 / 95 ] loss: 0.590
[ 55 / 95 ] loss: 0.672
[ 56 / 95 ] loss: 0.621
[ 57 / 95 ] loss: 0.620
[ 58 / 95 ] loss: 0.587
[ 59 / 95 ] loss: 0.738
[ 60 / 95 ] loss: 0.790
[ 61 / 95 ] loss: 0.501
[ 62 / 95 ] loss: 0.558
[ 63 / 95 ] loss: 0.607
[ 64 / 95 ] loss: 0.682
[ 65 / 95 ] loss: 0.745
[ 66 / 95 ] loss: 0.640
[ 67 / 95 ] loss: 0.519
[ 68 / 95 ] loss: 0.733
[ 69 / 95 ] loss: 0.611
[ 70 / 95 ] loss: 0.442
[ 71 / 95 ] loss: 0.648
[ 72 / 95 ] loss: 0.531
[ 73 / 95 ] loss: 0.617
[ 74 / 95 ] loss: 0.624
[ 75 / 95 ] loss: 0.661
[ 76 / 95 ] loss: 0.658
[ 77 / 95 ] loss: 0.764
[ 78 / 95 ] loss: 0.764
[ 79 / 95 ] loss: 0.573
[ 80 / 95 ] loss: 0.420
[ 81 / 95 ] loss: 0.531
[ 82 / 95 ] loss: 0.692
[ 83 / 95 ] loss: 0.651
[ 84 / 95 ] loss: 0.664
[ 85 / 95 ] loss: 0.646
[ 86 / 95 ] loss: 0.689
[ 87 / 95 ] loss: 0.541
[ 88 / 95 ] loss: 0.515
[ 89 / 95 ] loss: 0.610
[ 90 / 95 ] loss: 0.488
[ 91 / 95 ] loss: 0.759
[ 92 / 95 ] loss: 0.621
[ 93 / 95 ] loss: 0.596
[ 94 / 95 ] loss: 0.681
[ 95 / 95 ] loss: 0.595
0.618898375097074
Accuracy: 0.429708 -- Precision: 1.000000 -- Recall: 0.009217 -- F1: 0.018265 -- AUC: 0.826973
========= epoch: 32 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.804
[ 2 / 95 ] loss: 0.593
[ 3 / 95 ] loss: 0.653
[ 4 / 95 ] loss: 0.524
[ 5 / 95 ] loss: 0.610
[ 6 / 95 ] loss: 0.578
[ 7 / 95 ] loss: 0.662
[ 8 / 95 ] loss: 0.523
[ 9 / 95 ] loss: 0.865
[ 10 / 95 ] loss: 0.522
[ 11 / 95 ] loss: 0.648
[ 12 / 95 ] loss: 0.471
[ 13 / 95 ] loss: 0.555
[ 14 / 95 ] loss: 0.758
[ 15 / 95 ] loss: 0.421
[ 16 / 95 ] loss: 0.565
[ 17 / 95 ] loss: 0.598
[ 18 / 95 ] loss: 0.657
[ 19 / 95 ] loss: 0.434
[ 20 / 95 ] loss: 0.494
[ 21 / 95 ] loss: 0.525
[ 22 / 95 ] loss: 0.618
[ 23 / 95 ] loss: 0.488
[ 24 / 95 ] loss: 0.489
[ 25 / 95 ] loss: 0.592
[ 26 / 95 ] loss: 0.574
[ 27 / 95 ] loss: 0.467
[ 28 / 95 ] loss: 0.352
[ 29 / 95 ] loss: 0.583
[ 30 / 95 ] loss: 0.522
[ 31 / 95 ] loss: 0.601
[ 32 / 95 ] loss: 0.647
[ 33 / 95 ] loss: 0.365
[ 34 / 95 ] loss: 0.633
[ 35 / 95 ] loss: 0.456
[ 36 / 95 ] loss: 0.427
[ 37 / 95 ] loss: 0.483
[ 38 / 95 ] loss: 0.507
[ 39 / 95 ] loss: 0.689
[ 40 / 95 ] loss: 0.471
[ 41 / 95 ] loss: 0.961
[ 42 / 95 ] loss: 0.731
[ 43 / 95 ] loss: 0.680
[ 44 / 95 ] loss: 0.530
[ 45 / 95 ] loss: 0.678
[ 46 / 95 ] loss: 0.423
[ 47 / 95 ] loss: 0.361
[ 48 / 95 ] loss: 0.654
[ 49 / 95 ] loss: 0.522
[ 50 / 95 ] loss: 0.611
[ 51 / 95 ] loss: 0.439
[ 52 / 95 ] loss: 0.659
[ 53 / 95 ] loss: 0.463
[ 54 / 95 ] loss: 0.443
[ 55 / 95 ] loss: 0.539
[ 56 / 95 ] loss: 0.698
[ 57 / 95 ] loss: 0.371
[ 58 / 95 ] loss: 0.452
[ 59 / 95 ] loss: 0.543
[ 60 / 95 ] loss: 0.388
[ 61 / 95 ] loss: 0.439
[ 62 / 95 ] loss: 0.667
[ 63 / 95 ] loss: 0.647
[ 64 / 95 ] loss: 0.492
[ 65 / 95 ] loss: 0.621
[ 66 / 95 ] loss: 0.415
[ 67 / 95 ] loss: 0.618
[ 68 / 95 ] loss: 0.653
[ 69 / 95 ] loss: 0.440
[ 70 / 95 ] loss: 0.400
[ 71 / 95 ] loss: 0.611
[ 72 / 95 ] loss: 0.442
[ 73 / 95 ] loss: 0.531
[ 74 / 95 ] loss: 0.645
[ 75 / 95 ] loss: 0.384
[ 76 / 95 ] loss: 0.443
[ 77 / 95 ] loss: 0.386
[ 78 / 95 ] loss: 0.386
[ 79 / 95 ] loss: 0.575
[ 80 / 95 ] loss: 0.619
[ 81 / 95 ] loss: 0.569
[ 82 / 95 ] loss: 0.593
[ 83 / 95 ] loss: 0.440
[ 84 / 95 ] loss: 0.695
[ 85 / 95 ] loss: 0.521
[ 86 / 95 ] loss: 0.429
[ 87 / 95 ] loss: 0.574
[ 88 / 95 ] loss: 0.453
[ 89 / 95 ] loss: 0.424
[ 90 / 95 ] loss: 0.319
[ 91 / 95 ] loss: 0.439
[ 92 / 95 ] loss: 0.552
[ 93 / 95 ] loss: 0.514
[ 94 / 95 ] loss: 0.377
[ 95 / 95 ] loss: 0.341
0.5392112063734155
Accuracy: 0.806366 -- Precision: 0.774809 -- Recall: 0.935484 -- F1: 0.847599 -- AUC: 0.831293
========= epoch: 33 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.276
[ 2 / 95 ] loss: 0.636
[ 3 / 95 ] loss: 0.544
[ 4 / 95 ] loss: 0.664
[ 5 / 95 ] loss: 0.287
[ 6 / 95 ] loss: 0.666
[ 7 / 95 ] loss: 0.464
[ 8 / 95 ] loss: 0.660
[ 9 / 95 ] loss: 0.401
[ 10 / 95 ] loss: 0.563
[ 11 / 95 ] loss: 0.310
[ 12 / 95 ] loss: 0.277
[ 13 / 95 ] loss: 0.552
[ 14 / 95 ] loss: 0.277
[ 15 / 95 ] loss: 0.723
[ 16 / 95 ] loss: 0.291
[ 17 / 95 ] loss: 0.538
[ 18 / 95 ] loss: 0.526
[ 19 / 95 ] loss: 0.325
[ 20 / 95 ] loss: 0.466
[ 21 / 95 ] loss: 0.415
[ 22 / 95 ] loss: 0.601
[ 23 / 95 ] loss: 0.507
[ 24 / 95 ] loss: 0.443
[ 25 / 95 ] loss: 0.550
[ 26 / 95 ] loss: 0.280
[ 27 / 95 ] loss: 0.585
[ 28 / 95 ] loss: 0.607
[ 29 / 95 ] loss: 0.302
[ 30 / 95 ] loss: 0.571
[ 31 / 95 ] loss: 0.645
[ 32 / 95 ] loss: 0.502
[ 33 / 95 ] loss: 0.412
[ 34 / 95 ] loss: 0.454
[ 35 / 95 ] loss: 0.465
[ 36 / 95 ] loss: 0.457
[ 37 / 95 ] loss: 0.450
[ 38 / 95 ] loss: 0.366
[ 39 / 95 ] loss: 0.505
[ 40 / 95 ] loss: 0.765
[ 41 / 95 ] loss: 0.732
[ 42 / 95 ] loss: 0.584
[ 43 / 95 ] loss: 0.467
[ 44 / 95 ] loss: 0.500
[ 45 / 95 ] loss: 0.561
[ 46 / 95 ] loss: 0.513
[ 47 / 95 ] loss: 0.297
[ 48 / 95 ] loss: 0.315
[ 49 / 95 ] loss: 0.452
[ 50 / 95 ] loss: 0.304
[ 51 / 95 ] loss: 0.293
[ 52 / 95 ] loss: 0.485
[ 53 / 95 ] loss: 0.340
[ 54 / 95 ] loss: 0.652
[ 55 / 95 ] loss: 0.854
[ 56 / 95 ] loss: 0.519
[ 57 / 95 ] loss: 0.448
[ 58 / 95 ] loss: 0.472
[ 59 / 95 ] loss: 0.470
[ 60 / 95 ] loss: 0.372
[ 61 / 95 ] loss: 0.750
[ 62 / 95 ] loss: 0.574
[ 63 / 95 ] loss: 0.525
[ 64 / 95 ] loss: 0.526
[ 65 / 95 ] loss: 0.606
[ 66 / 95 ] loss: 0.702
[ 67 / 95 ] loss: 0.529
[ 68 / 95 ] loss: 0.547
[ 69 / 95 ] loss: 0.585
[ 70 / 95 ] loss: 0.308
[ 71 / 95 ] loss: 0.397
[ 72 / 95 ] loss: 0.537
[ 73 / 95 ] loss: 0.471
[ 74 / 95 ] loss: 0.398
[ 75 / 95 ] loss: 0.316
[ 76 / 95 ] loss: 0.561
[ 77 / 95 ] loss: 0.563
[ 78 / 95 ] loss: 0.578
[ 79 / 95 ] loss: 0.365
[ 80 / 95 ] loss: 0.520
[ 81 / 95 ] loss: 0.495
[ 82 / 95 ] loss: 0.454
[ 83 / 95 ] loss: 0.263
[ 84 / 95 ] loss: 0.487
[ 85 / 95 ] loss: 0.625
[ 86 / 95 ] loss: 0.477
[ 87 / 95 ] loss: 0.546
[ 88 / 95 ] loss: 0.303
[ 89 / 95 ] loss: 0.441
[ 90 / 95 ] loss: 0.721
[ 91 / 95 ] loss: 0.349
[ 92 / 95 ] loss: 0.320
[ 93 / 95 ] loss: 0.563
[ 94 / 95 ] loss: 0.562
[ 95 / 95 ] loss: 0.350
0.487114806865391
Accuracy: 0.482759 -- Precision: 0.866667 -- Recall: 0.119816 -- F1: 0.210526 -- AUC: 0.770147
========= epoch: 34 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.582
[ 2 / 95 ] loss: 0.569
[ 3 / 95 ] loss: 0.246
[ 4 / 95 ] loss: 0.848
[ 5 / 95 ] loss: 0.698
[ 6 / 95 ] loss: 0.532
[ 7 / 95 ] loss: 0.684
[ 8 / 95 ] loss: 0.546
[ 9 / 95 ] loss: 0.690
[ 10 / 95 ] loss: 0.667
[ 11 / 95 ] loss: 0.385
[ 12 / 95 ] loss: 0.651
[ 13 / 95 ] loss: 0.668
[ 14 / 95 ] loss: 0.669
[ 15 / 95 ] loss: 0.511
[ 16 / 95 ] loss: 0.521
[ 17 / 95 ] loss: 0.630
[ 18 / 95 ] loss: 0.585
[ 19 / 95 ] loss: 0.574
[ 20 / 95 ] loss: 0.655
[ 21 / 95 ] loss: 0.593
[ 22 / 95 ] loss: 0.576
[ 23 / 95 ] loss: 0.683
[ 24 / 95 ] loss: 0.649
[ 25 / 95 ] loss: 0.578
[ 26 / 95 ] loss: 0.427
[ 27 / 95 ] loss: 0.593
[ 28 / 95 ] loss: 0.503
[ 29 / 95 ] loss: 0.659
[ 30 / 95 ] loss: 0.571
[ 31 / 95 ] loss: 0.318
[ 32 / 95 ] loss: 0.550
[ 33 / 95 ] loss: 0.375
[ 34 / 95 ] loss: 0.618
[ 35 / 95 ] loss: 0.601
[ 36 / 95 ] loss: 0.674
[ 37 / 95 ] loss: 0.605
[ 38 / 95 ] loss: 0.604
[ 39 / 95 ] loss: 0.595
[ 40 / 95 ] loss: 0.541
[ 41 / 95 ] loss: 0.908
[ 42 / 95 ] loss: 0.659
[ 43 / 95 ] loss: 0.596
[ 44 / 95 ] loss: 0.429
[ 45 / 95 ] loss: 0.497
[ 46 / 95 ] loss: 0.448
[ 47 / 95 ] loss: 0.684
[ 48 / 95 ] loss: 0.718
[ 49 / 95 ] loss: 0.529
[ 50 / 95 ] loss: 0.664
[ 51 / 95 ] loss: 0.552
[ 52 / 95 ] loss: 0.381
[ 53 / 95 ] loss: 0.526
[ 54 / 95 ] loss: 0.792
[ 55 / 95 ] loss: 0.672
[ 56 / 95 ] loss: 0.411
[ 57 / 95 ] loss: 0.717
[ 58 / 95 ] loss: 0.674
[ 59 / 95 ] loss: 0.528
[ 60 / 95 ] loss: 0.474
[ 61 / 95 ] loss: 0.660
[ 62 / 95 ] loss: 0.555
[ 63 / 95 ] loss: 0.440
[ 64 / 95 ] loss: 0.575
[ 65 / 95 ] loss: 0.711
[ 66 / 95 ] loss: 0.443
[ 67 / 95 ] loss: 0.393
[ 68 / 95 ] loss: 0.646
[ 69 / 95 ] loss: 0.450
[ 70 / 95 ] loss: 0.560
[ 71 / 95 ] loss: 0.514
[ 72 / 95 ] loss: 0.437
[ 73 / 95 ] loss: 0.607
[ 74 / 95 ] loss: 0.426
[ 75 / 95 ] loss: 0.595
[ 76 / 95 ] loss: 0.513
[ 77 / 95 ] loss: 0.493
[ 78 / 95 ] loss: 0.507
[ 79 / 95 ] loss: 0.461
[ 80 / 95 ] loss: 0.464
[ 81 / 95 ] loss: 0.312
[ 82 / 95 ] loss: 0.726
[ 83 / 95 ] loss: 0.469
[ 84 / 95 ] loss: 0.504
[ 85 / 95 ] loss: 0.612
[ 86 / 95 ] loss: 0.455
[ 87 / 95 ] loss: 0.534
[ 88 / 95 ] loss: 0.474
[ 89 / 95 ] loss: 0.473
[ 90 / 95 ] loss: 0.442
[ 91 / 95 ] loss: 0.347
[ 92 / 95 ] loss: 0.370
[ 93 / 95 ] loss: 0.362
[ 94 / 95 ] loss: 0.474
[ 95 / 95 ] loss: 0.473
0.5532697241557272
Accuracy: 0.456233 -- Precision: 0.800000 -- Recall: 0.073733 -- F1: 0.135021 -- AUC: 0.443779
========= epoch: 35 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.662
[ 2 / 95 ] loss: 0.493
[ 3 / 95 ] loss: 0.692
[ 4 / 95 ] loss: 0.503
[ 5 / 95 ] loss: 0.526
[ 6 / 95 ] loss: 0.664
[ 7 / 95 ] loss: 0.633
[ 8 / 95 ] loss: 0.523
[ 9 / 95 ] loss: 0.510
[ 10 / 95 ] loss: 1.164
[ 11 / 95 ] loss: 0.550
[ 12 / 95 ] loss: 0.725
[ 13 / 95 ] loss: 0.300
[ 14 / 95 ] loss: 0.844
[ 15 / 95 ] loss: 0.335
[ 16 / 95 ] loss: 0.546
[ 17 / 95 ] loss: 0.730
[ 18 / 95 ] loss: 0.647
[ 19 / 95 ] loss: 0.830
[ 20 / 95 ] loss: 0.740
[ 21 / 95 ] loss: 0.672
[ 22 / 95 ] loss: 0.869
[ 23 / 95 ] loss: 0.542
[ 24 / 95 ] loss: 0.594
[ 25 / 95 ] loss: 0.591
[ 26 / 95 ] loss: 0.466
[ 27 / 95 ] loss: 0.654
[ 28 / 95 ] loss: 0.629
[ 29 / 95 ] loss: 0.562
[ 30 / 95 ] loss: 0.591
[ 31 / 95 ] loss: 0.689
[ 32 / 95 ] loss: 0.892
[ 33 / 95 ] loss: 0.505
[ 34 / 95 ] loss: 0.757
[ 35 / 95 ] loss: 0.526
[ 36 / 95 ] loss: 0.582
[ 37 / 95 ] loss: 0.570
[ 38 / 95 ] loss: 0.500
[ 39 / 95 ] loss: 0.706
[ 40 / 95 ] loss: 0.812
[ 41 / 95 ] loss: 0.514
[ 42 / 95 ] loss: 0.792
[ 43 / 95 ] loss: 0.528
[ 44 / 95 ] loss: 0.770
[ 45 / 95 ] loss: 0.678
[ 46 / 95 ] loss: 0.642
[ 47 / 95 ] loss: 0.625
[ 48 / 95 ] loss: 0.628
[ 49 / 95 ] loss: 0.462
[ 50 / 95 ] loss: 0.541
[ 51 / 95 ] loss: 0.601
[ 52 / 95 ] loss: 0.407
[ 53 / 95 ] loss: 0.545
[ 54 / 95 ] loss: 0.582
[ 55 / 95 ] loss: 0.453
[ 56 / 95 ] loss: 0.504
[ 57 / 95 ] loss: 0.534
[ 58 / 95 ] loss: 0.605
[ 59 / 95 ] loss: 0.615
[ 60 / 95 ] loss: 0.516
[ 61 / 95 ] loss: 0.494
[ 62 / 95 ] loss: 0.692
[ 63 / 95 ] loss: 0.363
[ 64 / 95 ] loss: 0.608
[ 65 / 95 ] loss: 0.678
[ 66 / 95 ] loss: 0.578
[ 67 / 95 ] loss: 0.379
[ 68 / 95 ] loss: 0.644
[ 69 / 95 ] loss: 0.565
[ 70 / 95 ] loss: 0.603
[ 71 / 95 ] loss: 0.431
[ 72 / 95 ] loss: 0.489
[ 73 / 95 ] loss: 0.700
[ 74 / 95 ] loss: 0.498
[ 75 / 95 ] loss: 0.421
[ 76 / 95 ] loss: 0.592
[ 77 / 95 ] loss: 0.539
[ 78 / 95 ] loss: 0.496
[ 79 / 95 ] loss: 0.487
[ 80 / 95 ] loss: 0.450
[ 81 / 95 ] loss: 0.677
[ 82 / 95 ] loss: 0.452
[ 83 / 95 ] loss: 0.447
[ 84 / 95 ] loss: 0.525
[ 85 / 95 ] loss: 0.571
[ 86 / 95 ] loss: 0.499
[ 87 / 95 ] loss: 0.847
[ 88 / 95 ] loss: 0.592
[ 89 / 95 ] loss: 0.429
[ 90 / 95 ] loss: 0.593
[ 91 / 95 ] loss: 0.359
[ 92 / 95 ] loss: 0.488
[ 93 / 95 ] loss: 0.655
[ 94 / 95 ] loss: 0.358
[ 95 / 95 ] loss: 0.248
0.5823027380202946
Accuracy: 0.740053 -- Precision: 0.865031 -- Recall: 0.649770 -- F1: 0.742105 -- AUC: 0.760585
========= epoch: 36 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.454
[ 2 / 95 ] loss: 0.546
[ 3 / 95 ] loss: 0.402
[ 4 / 95 ] loss: 0.720
[ 5 / 95 ] loss: 0.442
[ 6 / 95 ] loss: 0.515
[ 7 / 95 ] loss: 0.416
[ 8 / 95 ] loss: 0.495
[ 9 / 95 ] loss: 0.497
[ 10 / 95 ] loss: 0.472
[ 11 / 95 ] loss: 0.485
[ 12 / 95 ] loss: 0.511
[ 13 / 95 ] loss: 0.643
[ 14 / 95 ] loss: 0.448
[ 15 / 95 ] loss: 0.699
[ 16 / 95 ] loss: 0.440
[ 17 / 95 ] loss: 0.754
[ 18 / 95 ] loss: 0.534
[ 19 / 95 ] loss: 0.394
[ 20 / 95 ] loss: 0.537
[ 21 / 95 ] loss: 0.445
[ 22 / 95 ] loss: 0.772
[ 23 / 95 ] loss: 0.406
[ 24 / 95 ] loss: 0.366
[ 25 / 95 ] loss: 0.364
[ 26 / 95 ] loss: 0.320
[ 27 / 95 ] loss: 0.378
[ 28 / 95 ] loss: 0.521
[ 29 / 95 ] loss: 0.415
[ 30 / 95 ] loss: 0.472
[ 31 / 95 ] loss: 0.551
[ 32 / 95 ] loss: 0.329
[ 33 / 95 ] loss: 0.616
[ 34 / 95 ] loss: 0.476
[ 35 / 95 ] loss: 0.549
[ 36 / 95 ] loss: 0.353
[ 37 / 95 ] loss: 0.510
[ 38 / 95 ] loss: 0.532
[ 39 / 95 ] loss: 0.358
[ 40 / 95 ] loss: 0.447
[ 41 / 95 ] loss: 0.362
[ 42 / 95 ] loss: 0.291
[ 43 / 95 ] loss: 0.605
[ 44 / 95 ] loss: 0.170
[ 45 / 95 ] loss: 0.556
[ 46 / 95 ] loss: 0.460
[ 47 / 95 ] loss: 0.320
[ 48 / 95 ] loss: 0.396
[ 49 / 95 ] loss: 0.619
[ 50 / 95 ] loss: 0.484
[ 51 / 95 ] loss: 0.557
[ 52 / 95 ] loss: 0.420
[ 53 / 95 ] loss: 0.427
[ 54 / 95 ] loss: 0.411
[ 55 / 95 ] loss: 0.672
[ 56 / 95 ] loss: 0.428
[ 57 / 95 ] loss: 0.701
[ 58 / 95 ] loss: 0.677
[ 59 / 95 ] loss: 0.515
[ 60 / 95 ] loss: 0.524
[ 61 / 95 ] loss: 0.243
[ 62 / 95 ] loss: 0.488
[ 63 / 95 ] loss: 0.453
[ 64 / 95 ] loss: 0.739
[ 65 / 95 ] loss: 0.646
[ 66 / 95 ] loss: 0.684
[ 67 / 95 ] loss: 0.310
[ 68 / 95 ] loss: 0.524
[ 69 / 95 ] loss: 0.288
[ 70 / 95 ] loss: 0.580
[ 71 / 95 ] loss: 0.404
[ 72 / 95 ] loss: 0.641
[ 73 / 95 ] loss: 0.430
[ 74 / 95 ] loss: 0.476
[ 75 / 95 ] loss: 0.608
[ 76 / 95 ] loss: 0.457
[ 77 / 95 ] loss: 0.361
[ 78 / 95 ] loss: 0.508
[ 79 / 95 ] loss: 0.612
[ 80 / 95 ] loss: 0.456
[ 81 / 95 ] loss: 0.322
[ 82 / 95 ] loss: 0.792
[ 83 / 95 ] loss: 0.765
[ 84 / 95 ] loss: 0.532
[ 85 / 95 ] loss: 0.822
[ 86 / 95 ] loss: 0.390
[ 87 / 95 ] loss: 0.549
[ 88 / 95 ] loss: 0.532
[ 89 / 95 ] loss: 0.478
[ 90 / 95 ] loss: 0.548
[ 91 / 95 ] loss: 0.449
[ 92 / 95 ] loss: 0.613
[ 93 / 95 ] loss: 0.399
[ 94 / 95 ] loss: 0.440
[ 95 / 95 ] loss: 0.737
0.49952798611239385
Accuracy: 0.718833 -- Precision: 0.672897 -- Recall: 0.995392 -- F1: 0.802974 -- AUC: 0.679925
========= epoch: 37 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.594
[ 2 / 95 ] loss: 0.446
[ 3 / 95 ] loss: 0.537
[ 4 / 95 ] loss: 0.523
[ 5 / 95 ] loss: 0.338
[ 6 / 95 ] loss: 0.692
[ 7 / 95 ] loss: 0.838
[ 8 / 95 ] loss: 0.610
[ 9 / 95 ] loss: 0.609
[ 10 / 95 ] loss: 0.589
[ 11 / 95 ] loss: 0.659
[ 12 / 95 ] loss: 0.680
[ 13 / 95 ] loss: 0.559
[ 14 / 95 ] loss: 0.581
[ 15 / 95 ] loss: 0.337
[ 16 / 95 ] loss: 0.616
[ 17 / 95 ] loss: 0.520
[ 18 / 95 ] loss: 0.536
[ 19 / 95 ] loss: 0.594
[ 20 / 95 ] loss: 0.569
[ 21 / 95 ] loss: 0.778
[ 22 / 95 ] loss: 0.600
[ 23 / 95 ] loss: 0.431
[ 24 / 95 ] loss: 0.553
[ 25 / 95 ] loss: 0.534
[ 26 / 95 ] loss: 0.531
[ 27 / 95 ] loss: 0.575
[ 28 / 95 ] loss: 0.582
[ 29 / 95 ] loss: 0.691
[ 30 / 95 ] loss: 0.526
[ 31 / 95 ] loss: 0.790
[ 32 / 95 ] loss: 0.603
[ 33 / 95 ] loss: 0.509
[ 34 / 95 ] loss: 0.587
[ 35 / 95 ] loss: 0.626
[ 36 / 95 ] loss: 0.554
[ 37 / 95 ] loss: 0.574
[ 38 / 95 ] loss: 0.712
[ 39 / 95 ] loss: 0.597
[ 40 / 95 ] loss: 0.639
[ 41 / 95 ] loss: 0.519
[ 42 / 95 ] loss: 0.592
[ 43 / 95 ] loss: 0.793
[ 44 / 95 ] loss: 0.484
[ 45 / 95 ] loss: 0.652
[ 46 / 95 ] loss: 0.618
[ 47 / 95 ] loss: 0.622
[ 48 / 95 ] loss: 0.518
[ 49 / 95 ] loss: 0.696
[ 50 / 95 ] loss: 0.628
[ 51 / 95 ] loss: 0.577
[ 52 / 95 ] loss: 0.602
[ 53 / 95 ] loss: 0.566
[ 54 / 95 ] loss: 0.668
[ 55 / 95 ] loss: 0.453
[ 56 / 95 ] loss: 0.570
[ 57 / 95 ] loss: 0.597
[ 58 / 95 ] loss: 0.569
[ 59 / 95 ] loss: 0.520
[ 60 / 95 ] loss: 0.595
[ 61 / 95 ] loss: 0.715
[ 62 / 95 ] loss: 0.614
[ 63 / 95 ] loss: 0.739
[ 64 / 95 ] loss: 0.685
[ 65 / 95 ] loss: 0.601
[ 66 / 95 ] loss: 0.772
[ 67 / 95 ] loss: 0.703
[ 68 / 95 ] loss: 0.569
[ 69 / 95 ] loss: 0.558
[ 70 / 95 ] loss: 0.546
[ 71 / 95 ] loss: 0.485
[ 72 / 95 ] loss: 0.700
[ 73 / 95 ] loss: 0.648
[ 74 / 95 ] loss: 0.544
[ 75 / 95 ] loss: 0.555
[ 76 / 95 ] loss: 0.526
[ 77 / 95 ] loss: 0.627
[ 78 / 95 ] loss: 0.568
[ 79 / 95 ] loss: 0.624
[ 80 / 95 ] loss: 0.585
[ 81 / 95 ] loss: 0.495
[ 82 / 95 ] loss: 0.558
[ 83 / 95 ] loss: 0.597
[ 84 / 95 ] loss: 0.550
[ 85 / 95 ] loss: 0.518
[ 86 / 95 ] loss: 0.617
[ 87 / 95 ] loss: 0.563
[ 88 / 95 ] loss: 0.766
[ 89 / 95 ] loss: 0.545
[ 90 / 95 ] loss: 0.523
[ 91 / 95 ] loss: 0.477
[ 92 / 95 ] loss: 0.507
[ 93 / 95 ] loss: 0.805
[ 94 / 95 ] loss: 0.538
[ 95 / 95 ] loss: 0.532
0.5917379831012927
Accuracy: 0.702918 -- Precision: 0.660550 -- Recall: 0.995392 -- F1: 0.794118 -- AUC: 0.648891
========= epoch: 38 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.532
[ 2 / 95 ] loss: 0.682
[ 3 / 95 ] loss: 0.668
[ 4 / 95 ] loss: 0.585
[ 5 / 95 ] loss: 0.586
[ 6 / 95 ] loss: 0.401
[ 7 / 95 ] loss: 0.473
[ 8 / 95 ] loss: 0.497
[ 9 / 95 ] loss: 0.669
[ 10 / 95 ] loss: 0.601
[ 11 / 95 ] loss: 0.720
[ 12 / 95 ] loss: 0.655
[ 13 / 95 ] loss: 0.680
[ 14 / 95 ] loss: 0.546
[ 15 / 95 ] loss: 0.691
[ 16 / 95 ] loss: 0.591
[ 17 / 95 ] loss: 0.643
[ 18 / 95 ] loss: 0.707
[ 19 / 95 ] loss: 0.741
[ 20 / 95 ] loss: 0.534
[ 21 / 95 ] loss: 0.508
[ 22 / 95 ] loss: 0.636
[ 23 / 95 ] loss: 0.564
[ 24 / 95 ] loss: 0.629
[ 25 / 95 ] loss: 0.731
[ 26 / 95 ] loss: 0.413
[ 27 / 95 ] loss: 0.731
[ 28 / 95 ] loss: 0.443
[ 29 / 95 ] loss: 0.587
[ 30 / 95 ] loss: 0.375
[ 31 / 95 ] loss: 0.403
[ 32 / 95 ] loss: 0.559
[ 33 / 95 ] loss: 0.483
[ 34 / 95 ] loss: 0.633
[ 35 / 95 ] loss: 0.684
[ 36 / 95 ] loss: 0.534
[ 37 / 95 ] loss: 0.549
[ 38 / 95 ] loss: 0.665
[ 39 / 95 ] loss: 0.597
[ 40 / 95 ] loss: 0.518
[ 41 / 95 ] loss: 0.635
[ 42 / 95 ] loss: 0.800
[ 43 / 95 ] loss: 0.621
[ 44 / 95 ] loss: 0.674
[ 45 / 95 ] loss: 0.509
[ 46 / 95 ] loss: 0.506
[ 47 / 95 ] loss: 0.646
[ 48 / 95 ] loss: 0.670
[ 49 / 95 ] loss: 0.603
[ 50 / 95 ] loss: 0.584
[ 51 / 95 ] loss: 0.496
[ 52 / 95 ] loss: 0.561
[ 53 / 95 ] loss: 0.492
[ 54 / 95 ] loss: 0.649
[ 55 / 95 ] loss: 0.883
[ 56 / 95 ] loss: 0.565
[ 57 / 95 ] loss: 0.470
[ 58 / 95 ] loss: 0.420
[ 59 / 95 ] loss: 0.705
[ 60 / 95 ] loss: 0.721
[ 61 / 95 ] loss: 0.558
[ 62 / 95 ] loss: 0.540
[ 63 / 95 ] loss: 0.593
[ 64 / 95 ] loss: 0.405
[ 65 / 95 ] loss: 0.498
[ 66 / 95 ] loss: 0.416
[ 67 / 95 ] loss: 0.454
[ 68 / 95 ] loss: 0.538
[ 69 / 95 ] loss: 0.594
[ 70 / 95 ] loss: 0.544
[ 71 / 95 ] loss: 0.669
[ 72 / 95 ] loss: 0.547
[ 73 / 95 ] loss: 0.555
[ 74 / 95 ] loss: 0.591
[ 75 / 95 ] loss: 0.668
[ 76 / 95 ] loss: 0.554
[ 77 / 95 ] loss: 0.585
[ 78 / 95 ] loss: 0.449
[ 79 / 95 ] loss: 0.672
[ 80 / 95 ] loss: 0.629
[ 81 / 95 ] loss: 0.529
[ 82 / 95 ] loss: 0.348
[ 83 / 95 ] loss: 0.592
[ 84 / 95 ] loss: 0.754
[ 85 / 95 ] loss: 0.547
[ 86 / 95 ] loss: 0.542
[ 87 / 95 ] loss: 0.612
[ 88 / 95 ] loss: 0.631
[ 89 / 95 ] loss: 0.822
[ 90 / 95 ] loss: 0.491
[ 91 / 95 ] loss: 0.623
[ 92 / 95 ] loss: 0.681
[ 93 / 95 ] loss: 0.751
[ 94 / 95 ] loss: 0.638
[ 95 / 95 ] loss: 0.582
0.587929505109787
Accuracy: 0.700265 -- Precision: 0.659509 -- Recall: 0.990783 -- F1: 0.791897 -- AUC: 0.649798
========= epoch: 39 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.628
[ 2 / 95 ] loss: 0.518
[ 3 / 95 ] loss: 0.667
[ 4 / 95 ] loss: 0.838
[ 5 / 95 ] loss: 0.582
[ 6 / 95 ] loss: 0.534
[ 7 / 95 ] loss: 0.555
[ 8 / 95 ] loss: 0.624
[ 9 / 95 ] loss: 0.367
[ 10 / 95 ] loss: 0.609
[ 11 / 95 ] loss: 0.525
[ 12 / 95 ] loss: 0.644
[ 13 / 95 ] loss: 0.662
[ 14 / 95 ] loss: 0.600
[ 15 / 95 ] loss: 0.605
[ 16 / 95 ] loss: 0.502
[ 17 / 95 ] loss: 0.609
[ 18 / 95 ] loss: 0.402
[ 19 / 95 ] loss: 0.463
[ 20 / 95 ] loss: 0.619
[ 21 / 95 ] loss: 0.523
[ 22 / 95 ] loss: 0.745
[ 23 / 95 ] loss: 0.786
[ 24 / 95 ] loss: 0.805
[ 25 / 95 ] loss: 0.530
[ 26 / 95 ] loss: 0.525
[ 27 / 95 ] loss: 0.507
[ 28 / 95 ] loss: 0.586
[ 29 / 95 ] loss: 0.729
[ 30 / 95 ] loss: 0.425
[ 31 / 95 ] loss: 0.515
[ 32 / 95 ] loss: 0.466
[ 33 / 95 ] loss: 0.524
[ 34 / 95 ] loss: 0.823
[ 35 / 95 ] loss: 0.707
[ 36 / 95 ] loss: 0.481
[ 37 / 95 ] loss: 0.726
[ 38 / 95 ] loss: 0.665
[ 39 / 95 ] loss: 0.461
[ 40 / 95 ] loss: 0.606
[ 41 / 95 ] loss: 0.444
[ 42 / 95 ] loss: 0.738
[ 43 / 95 ] loss: 0.647
[ 44 / 95 ] loss: 0.470
[ 45 / 95 ] loss: 0.447
[ 46 / 95 ] loss: 0.767
[ 47 / 95 ] loss: 0.454
[ 48 / 95 ] loss: 0.489
[ 49 / 95 ] loss: 0.432
[ 50 / 95 ] loss: 0.672
[ 51 / 95 ] loss: 0.757
[ 52 / 95 ] loss: 0.532
[ 53 / 95 ] loss: 0.820
[ 54 / 95 ] loss: 0.527
[ 55 / 95 ] loss: 0.644
[ 56 / 95 ] loss: 0.448
[ 57 / 95 ] loss: 0.597
[ 58 / 95 ] loss: 0.538
[ 59 / 95 ] loss: 0.490
[ 60 / 95 ] loss: 0.513
[ 61 / 95 ] loss: 0.561
[ 62 / 95 ] loss: 0.615
[ 63 / 95 ] loss: 0.732
[ 64 / 95 ] loss: 0.443
[ 65 / 95 ] loss: 0.544
[ 66 / 95 ] loss: 0.541
[ 67 / 95 ] loss: 0.663
[ 68 / 95 ] loss: 0.484
[ 69 / 95 ] loss: 0.554
[ 70 / 95 ] loss: 0.425
[ 71 / 95 ] loss: 0.655
[ 72 / 95 ] loss: 0.603
[ 73 / 95 ] loss: 0.700
[ 74 / 95 ] loss: 0.434
[ 75 / 95 ] loss: 0.535
[ 76 / 95 ] loss: 0.667
[ 77 / 95 ] loss: 0.709
[ 78 / 95 ] loss: 0.435
[ 79 / 95 ] loss: 0.699
[ 80 / 95 ] loss: 0.685
[ 81 / 95 ] loss: 0.510
[ 82 / 95 ] loss: 0.556
[ 83 / 95 ] loss: 0.672
[ 84 / 95 ] loss: 0.527
[ 85 / 95 ] loss: 0.557
[ 86 / 95 ] loss: 0.631
[ 87 / 95 ] loss: 0.565
[ 88 / 95 ] loss: 0.395
[ 89 / 95 ] loss: 0.482
[ 90 / 95 ] loss: 0.811
[ 91 / 95 ] loss: 0.574
[ 92 / 95 ] loss: 0.486
[ 93 / 95 ] loss: 0.611
[ 94 / 95 ] loss: 0.791
[ 95 / 95 ] loss: 0.523
0.5840932118265252
Accuracy: 0.705570 -- Precision: 0.663580 -- Recall: 0.990783 -- F1: 0.794824 -- AUC: 0.656106
========= epoch: 40 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.464
[ 2 / 95 ] loss: 0.655
[ 3 / 95 ] loss: 0.586
[ 4 / 95 ] loss: 0.713
[ 5 / 95 ] loss: 0.628
[ 6 / 95 ] loss: 0.635
[ 7 / 95 ] loss: 0.629
[ 8 / 95 ] loss: 0.574
[ 9 / 95 ] loss: 0.581
[ 10 / 95 ] loss: 0.479
[ 11 / 95 ] loss: 0.423
[ 12 / 95 ] loss: 0.594
[ 13 / 95 ] loss: 0.470
[ 14 / 95 ] loss: 0.694
[ 15 / 95 ] loss: 0.481
[ 16 / 95 ] loss: 0.497
[ 17 / 95 ] loss: 0.556
[ 18 / 95 ] loss: 0.580
[ 19 / 95 ] loss: 0.807
[ 20 / 95 ] loss: 0.466
[ 21 / 95 ] loss: 0.556
[ 22 / 95 ] loss: 0.683
[ 23 / 95 ] loss: 0.538
[ 24 / 95 ] loss: 0.450
[ 25 / 95 ] loss: 0.562
[ 26 / 95 ] loss: 0.786
[ 27 / 95 ] loss: 0.642
[ 28 / 95 ] loss: 0.368
[ 29 / 95 ] loss: 0.529
[ 30 / 95 ] loss: 0.518
[ 31 / 95 ] loss: 0.698
[ 32 / 95 ] loss: 0.599
[ 33 / 95 ] loss: 0.751
[ 34 / 95 ] loss: 0.855
[ 35 / 95 ] loss: 0.580
[ 36 / 95 ] loss: 0.608
[ 37 / 95 ] loss: 0.697
[ 38 / 95 ] loss: 0.604
[ 39 / 95 ] loss: 0.582
[ 40 / 95 ] loss: 0.512
[ 41 / 95 ] loss: 0.572
[ 42 / 95 ] loss: 0.550
[ 43 / 95 ] loss: 0.504
[ 44 / 95 ] loss: 0.677
[ 45 / 95 ] loss: 0.602
[ 46 / 95 ] loss: 0.563
[ 47 / 95 ] loss: 0.425
[ 48 / 95 ] loss: 0.705
[ 49 / 95 ] loss: 0.534
[ 50 / 95 ] loss: 0.468
[ 51 / 95 ] loss: 0.669
[ 52 / 95 ] loss: 0.457
[ 53 / 95 ] loss: 0.469
[ 54 / 95 ] loss: 0.660
[ 55 / 95 ] loss: 0.684
[ 56 / 95 ] loss: 0.511
[ 57 / 95 ] loss: 0.539
[ 58 / 95 ] loss: 0.465
[ 59 / 95 ] loss: 0.651
[ 60 / 95 ] loss: 0.612
[ 61 / 95 ] loss: 0.510
[ 62 / 95 ] loss: 0.490
[ 63 / 95 ] loss: 0.543
[ 64 / 95 ] loss: 0.695
[ 65 / 95 ] loss: 0.643
[ 66 / 95 ] loss: 0.585
[ 67 / 95 ] loss: 0.496
[ 68 / 95 ] loss: 0.478
[ 69 / 95 ] loss: 0.624
[ 70 / 95 ] loss: 0.555
[ 71 / 95 ] loss: 0.558
[ 72 / 95 ] loss: 0.430
[ 73 / 95 ] loss: 0.662
[ 74 / 95 ] loss: 0.517
[ 75 / 95 ] loss: 0.587
[ 76 / 95 ] loss: 0.591
[ 77 / 95 ] loss: 0.666
[ 78 / 95 ] loss: 0.665
[ 79 / 95 ] loss: 0.701
[ 80 / 95 ] loss: 0.445
[ 81 / 95 ] loss: 0.419
[ 82 / 95 ] loss: 0.644
[ 83 / 95 ] loss: 0.488
[ 84 / 95 ] loss: 0.914
[ 85 / 95 ] loss: 0.513
[ 86 / 95 ] loss: 0.558
[ 87 / 95 ] loss: 0.662
[ 88 / 95 ] loss: 0.553
[ 89 / 95 ] loss: 0.659
[ 90 / 95 ] loss: 0.526
[ 91 / 95 ] loss: 0.664
[ 92 / 95 ] loss: 0.650
[ 93 / 95 ] loss: 0.644
[ 94 / 95 ] loss: 0.560
[ 95 / 95 ] loss: 0.554
0.5831234467656989
Accuracy: 0.705570 -- Precision: 0.663580 -- Recall: 0.990783 -- F1: 0.794824 -- AUC: 0.655588
========= epoch: 41 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.470
[ 2 / 95 ] loss: 0.481
[ 3 / 95 ] loss: 0.575
[ 4 / 95 ] loss: 0.555
[ 5 / 95 ] loss: 0.694
[ 6 / 95 ] loss: 0.518
[ 7 / 95 ] loss: 0.481
[ 8 / 95 ] loss: 0.583
[ 9 / 95 ] loss: 0.537
[ 10 / 95 ] loss: 0.501
[ 11 / 95 ] loss: 0.485
[ 12 / 95 ] loss: 0.768
[ 13 / 95 ] loss: 0.541
[ 14 / 95 ] loss: 0.661
[ 15 / 95 ] loss: 0.937
[ 16 / 95 ] loss: 0.654
[ 17 / 95 ] loss: 0.513
[ 18 / 95 ] loss: 0.491
[ 19 / 95 ] loss: 0.574
[ 20 / 95 ] loss: 0.426
[ 21 / 95 ] loss: 0.555
[ 22 / 95 ] loss: 0.644
[ 23 / 95 ] loss: 0.599
[ 24 / 95 ] loss: 0.339
[ 25 / 95 ] loss: 0.935
[ 26 / 95 ] loss: 0.603
[ 27 / 95 ] loss: 0.527
[ 28 / 95 ] loss: 0.408
[ 29 / 95 ] loss: 0.548
[ 30 / 95 ] loss: 0.640
[ 31 / 95 ] loss: 0.714
[ 32 / 95 ] loss: 0.638
[ 33 / 95 ] loss: 0.472
[ 34 / 95 ] loss: 0.663
[ 35 / 95 ] loss: 0.602
[ 36 / 95 ] loss: 0.599
[ 37 / 95 ] loss: 0.708
[ 38 / 95 ] loss: 0.531
[ 39 / 95 ] loss: 0.525
[ 40 / 95 ] loss: 0.659
[ 41 / 95 ] loss: 0.602
[ 42 / 95 ] loss: 0.468
[ 43 / 95 ] loss: 0.737
[ 44 / 95 ] loss: 0.603
[ 45 / 95 ] loss: 0.520
[ 46 / 95 ] loss: 0.767
[ 47 / 95 ] loss: 0.700
[ 48 / 95 ] loss: 0.593
[ 49 / 95 ] loss: 0.559
[ 50 / 95 ] loss: 0.400
[ 51 / 95 ] loss: 0.598
[ 52 / 95 ] loss: 0.583
[ 53 / 95 ] loss: 0.592
[ 54 / 95 ] loss: 0.586
[ 55 / 95 ] loss: 0.646
[ 56 / 95 ] loss: 0.613
[ 57 / 95 ] loss: 0.628
[ 58 / 95 ] loss: 0.632
[ 59 / 95 ] loss: 0.374
[ 60 / 95 ] loss: 0.701
[ 61 / 95 ] loss: 0.563
[ 62 / 95 ] loss: 0.578
[ 63 / 95 ] loss: 0.456
[ 64 / 95 ] loss: 0.503
[ 65 / 95 ] loss: 0.568
[ 66 / 95 ] loss: 0.473
[ 67 / 95 ] loss: 0.544
[ 68 / 95 ] loss: 0.626
[ 69 / 95 ] loss: 0.568
[ 70 / 95 ] loss: 0.472
[ 71 / 95 ] loss: 0.689
[ 72 / 95 ] loss: 0.807
[ 73 / 95 ] loss: 0.623
[ 74 / 95 ] loss: 0.642
[ 75 / 95 ] loss: 0.581
[ 76 / 95 ] loss: 0.422
[ 77 / 95 ] loss: 0.411
[ 78 / 95 ] loss: 0.504
[ 79 / 95 ] loss: 0.603
[ 80 / 95 ] loss: 0.503
[ 81 / 95 ] loss: 0.545
[ 82 / 95 ] loss: 0.730
[ 83 / 95 ] loss: 0.744
[ 84 / 95 ] loss: 0.556
[ 85 / 95 ] loss: 0.481
[ 86 / 95 ] loss: 0.584
[ 87 / 95 ] loss: 0.789
[ 88 / 95 ] loss: 0.753
[ 89 / 95 ] loss: 0.623
[ 90 / 95 ] loss: 0.433
[ 91 / 95 ] loss: 0.551
[ 92 / 95 ] loss: 0.548
[ 93 / 95 ] loss: 0.489
[ 94 / 95 ] loss: 0.538
[ 95 / 95 ] loss: 0.538
0.5820563344578994
Accuracy: 0.705570 -- Precision: 0.663580 -- Recall: 0.990783 -- F1: 0.794824 -- AUC: 0.648272
========= epoch: 42 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.774
[ 2 / 95 ] loss: 0.422
[ 3 / 95 ] loss: 0.559
[ 4 / 95 ] loss: 0.667
[ 5 / 95 ] loss: 0.500
[ 6 / 95 ] loss: 0.487
[ 7 / 95 ] loss: 0.723
[ 8 / 95 ] loss: 0.625
[ 9 / 95 ] loss: 0.514
[ 10 / 95 ] loss: 0.781
[ 11 / 95 ] loss: 0.496
[ 12 / 95 ] loss: 0.518
[ 13 / 95 ] loss: 0.474
[ 14 / 95 ] loss: 0.482
[ 15 / 95 ] loss: 0.478
[ 16 / 95 ] loss: 0.520
[ 17 / 95 ] loss: 0.658
[ 18 / 95 ] loss: 0.531
[ 19 / 95 ] loss: 0.671
[ 20 / 95 ] loss: 0.566
[ 21 / 95 ] loss: 0.584
[ 22 / 95 ] loss: 0.664
[ 23 / 95 ] loss: 0.548
[ 24 / 95 ] loss: 0.577
[ 25 / 95 ] loss: 0.653
[ 26 / 95 ] loss: 0.683
[ 27 / 95 ] loss: 0.445
[ 28 / 95 ] loss: 0.832
[ 29 / 95 ] loss: 0.459
[ 30 / 95 ] loss: 0.557
[ 31 / 95 ] loss: 0.493
[ 32 / 95 ] loss: 0.539
[ 33 / 95 ] loss: 0.666
[ 34 / 95 ] loss: 0.539
[ 35 / 95 ] loss: 0.608
[ 36 / 95 ] loss: 0.834
[ 37 / 95 ] loss: 0.551
[ 38 / 95 ] loss: 0.480
[ 39 / 95 ] loss: 0.650
[ 40 / 95 ] loss: 0.479
[ 41 / 95 ] loss: 0.638
[ 42 / 95 ] loss: 0.614
[ 43 / 95 ] loss: 0.532
[ 44 / 95 ] loss: 0.500
[ 45 / 95 ] loss: 0.534
[ 46 / 95 ] loss: 0.629
[ 47 / 95 ] loss: 0.818
[ 48 / 95 ] loss: 0.466
[ 49 / 95 ] loss: 0.600
[ 50 / 95 ] loss: 0.562
[ 51 / 95 ] loss: 0.475
[ 52 / 95 ] loss: 0.796
[ 53 / 95 ] loss: 0.572
[ 54 / 95 ] loss: 0.334
[ 55 / 95 ] loss: 0.717
[ 56 / 95 ] loss: 0.454
[ 57 / 95 ] loss: 0.490
[ 58 / 95 ] loss: 0.791
[ 59 / 95 ] loss: 0.572
[ 60 / 95 ] loss: 0.545
[ 61 / 95 ] loss: 0.512
[ 62 / 95 ] loss: 0.622
[ 63 / 95 ] loss: 0.569
[ 64 / 95 ] loss: 0.423
[ 65 / 95 ] loss: 0.720
[ 66 / 95 ] loss: 0.609
[ 67 / 95 ] loss: 0.833
[ 68 / 95 ] loss: 0.340
[ 69 / 95 ] loss: 0.515
[ 70 / 95 ] loss: 0.591
[ 71 / 95 ] loss: 0.462
[ 72 / 95 ] loss: 0.608
[ 73 / 95 ] loss: 0.511
[ 74 / 95 ] loss: 0.735
[ 75 / 95 ] loss: 0.583
[ 76 / 95 ] loss: 0.663
[ 77 / 95 ] loss: 0.652
[ 78 / 95 ] loss: 0.412
[ 79 / 95 ] loss: 0.454
[ 80 / 95 ] loss: 0.641
[ 81 / 95 ] loss: 0.583
[ 82 / 95 ] loss: 0.531
[ 83 / 95 ] loss: 0.540
[ 84 / 95 ] loss: 0.719
[ 85 / 95 ] loss: 0.639
[ 86 / 95 ] loss: 0.626
[ 87 / 95 ] loss: 0.557
[ 88 / 95 ] loss: 0.491
[ 89 / 95 ] loss: 0.601
[ 90 / 95 ] loss: 0.488
[ 91 / 95 ] loss: 0.627
[ 92 / 95 ] loss: 0.477
[ 93 / 95 ] loss: 0.701
[ 94 / 95 ] loss: 0.538
[ 95 / 95 ] loss: 0.728
0.5812819904402683
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.652952
========= epoch: 43 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.590
[ 2 / 95 ] loss: 0.500
[ 3 / 95 ] loss: 0.530
[ 4 / 95 ] loss: 0.516
[ 5 / 95 ] loss: 0.687
[ 6 / 95 ] loss: 0.698
[ 7 / 95 ] loss: 0.730
[ 8 / 95 ] loss: 0.614
[ 9 / 95 ] loss: 0.600
[ 10 / 95 ] loss: 0.608
[ 11 / 95 ] loss: 0.728
[ 12 / 95 ] loss: 0.449
[ 13 / 95 ] loss: 0.629
[ 14 / 95 ] loss: 0.478
[ 15 / 95 ] loss: 0.618
[ 16 / 95 ] loss: 0.483
[ 17 / 95 ] loss: 0.502
[ 18 / 95 ] loss: 0.518
[ 19 / 95 ] loss: 0.397
[ 20 / 95 ] loss: 0.686
[ 21 / 95 ] loss: 0.752
[ 22 / 95 ] loss: 0.513
[ 23 / 95 ] loss: 0.445
[ 24 / 95 ] loss: 0.551
[ 25 / 95 ] loss: 0.408
[ 26 / 95 ] loss: 0.669
[ 27 / 95 ] loss: 0.468
[ 28 / 95 ] loss: 0.818
[ 29 / 95 ] loss: 0.578
[ 30 / 95 ] loss: 0.830
[ 31 / 95 ] loss: 0.620
[ 32 / 95 ] loss: 0.368
[ 33 / 95 ] loss: 0.713
[ 34 / 95 ] loss: 0.861
[ 35 / 95 ] loss: 0.456
[ 36 / 95 ] loss: 0.366
[ 37 / 95 ] loss: 0.609
[ 38 / 95 ] loss: 0.651
[ 39 / 95 ] loss: 0.648
[ 40 / 95 ] loss: 0.644
[ 41 / 95 ] loss: 0.562
[ 42 / 95 ] loss: 0.610
[ 43 / 95 ] loss: 0.578
[ 44 / 95 ] loss: 0.514
[ 45 / 95 ] loss: 0.551
[ 46 / 95 ] loss: 0.617
[ 47 / 95 ] loss: 0.508
[ 48 / 95 ] loss: 0.568
[ 49 / 95 ] loss: 0.312
[ 50 / 95 ] loss: 0.669
[ 51 / 95 ] loss: 0.561
[ 52 / 95 ] loss: 0.713
[ 53 / 95 ] loss: 0.730
[ 54 / 95 ] loss: 0.482
[ 55 / 95 ] loss: 0.489
[ 56 / 95 ] loss: 0.474
[ 57 / 95 ] loss: 0.536
[ 58 / 95 ] loss: 0.632
[ 59 / 95 ] loss: 0.570
[ 60 / 95 ] loss: 0.601
[ 61 / 95 ] loss: 0.585
[ 62 / 95 ] loss: 0.745
[ 63 / 95 ] loss: 0.532
[ 64 / 95 ] loss: 0.625
[ 65 / 95 ] loss: 0.745
[ 66 / 95 ] loss: 0.667
[ 67 / 95 ] loss: 0.490
[ 68 / 95 ] loss: 0.587
[ 69 / 95 ] loss: 0.429
[ 70 / 95 ] loss: 0.645
[ 71 / 95 ] loss: 0.617
[ 72 / 95 ] loss: 0.789
[ 73 / 95 ] loss: 0.532
[ 74 / 95 ] loss: 0.671
[ 75 / 95 ] loss: 0.671
[ 76 / 95 ] loss: 0.538
[ 77 / 95 ] loss: 0.703
[ 78 / 95 ] loss: 0.539
[ 79 / 95 ] loss: 0.676
[ 80 / 95 ] loss: 0.406
[ 81 / 95 ] loss: 0.638
[ 82 / 95 ] loss: 0.668
[ 83 / 95 ] loss: 0.617
[ 84 / 95 ] loss: 0.548
[ 85 / 95 ] loss: 0.577
[ 86 / 95 ] loss: 0.526
[ 87 / 95 ] loss: 0.499
[ 88 / 95 ] loss: 0.438
[ 89 / 95 ] loss: 0.598
[ 90 / 95 ] loss: 0.461
[ 91 / 95 ] loss: 0.625
[ 92 / 95 ] loss: 0.515
[ 93 / 95 ] loss: 0.637
[ 94 / 95 ] loss: 0.596
[ 95 / 95 ] loss: 0.441
0.5829606718138645
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.652967
========= epoch: 44 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.462
[ 2 / 95 ] loss: 0.599
[ 3 / 95 ] loss: 0.627
[ 4 / 95 ] loss: 0.698
[ 5 / 95 ] loss: 0.567
[ 6 / 95 ] loss: 0.570
[ 7 / 95 ] loss: 0.747
[ 8 / 95 ] loss: 0.590
[ 9 / 95 ] loss: 0.732
[ 10 / 95 ] loss: 0.593
[ 11 / 95 ] loss: 0.623
[ 12 / 95 ] loss: 0.659
[ 13 / 95 ] loss: 0.517
[ 14 / 95 ] loss: 0.537
[ 15 / 95 ] loss: 0.568
[ 16 / 95 ] loss: 0.540
[ 17 / 95 ] loss: 0.521
[ 18 / 95 ] loss: 0.538
[ 19 / 95 ] loss: 0.428
[ 20 / 95 ] loss: 0.559
[ 21 / 95 ] loss: 0.436
[ 22 / 95 ] loss: 0.620
[ 23 / 95 ] loss: 0.408
[ 24 / 95 ] loss: 0.626
[ 25 / 95 ] loss: 0.494
[ 26 / 95 ] loss: 0.607
[ 27 / 95 ] loss: 0.516
[ 28 / 95 ] loss: 0.815
[ 29 / 95 ] loss: 0.670
[ 30 / 95 ] loss: 0.461
[ 31 / 95 ] loss: 0.538
[ 32 / 95 ] loss: 0.672
[ 33 / 95 ] loss: 0.542
[ 34 / 95 ] loss: 0.644
[ 35 / 95 ] loss: 0.550
[ 36 / 95 ] loss: 0.684
[ 37 / 95 ] loss: 0.752
[ 38 / 95 ] loss: 0.642
[ 39 / 95 ] loss: 0.542
[ 40 / 95 ] loss: 0.644
[ 41 / 95 ] loss: 0.598
[ 42 / 95 ] loss: 0.491
[ 43 / 95 ] loss: 0.664
[ 44 / 95 ] loss: 0.614
[ 45 / 95 ] loss: 0.601
[ 46 / 95 ] loss: 0.518
[ 47 / 95 ] loss: 0.533
[ 48 / 95 ] loss: 0.520
[ 49 / 95 ] loss: 0.501
[ 50 / 95 ] loss: 0.676
[ 51 / 95 ] loss: 0.579
[ 52 / 95 ] loss: 0.583
[ 53 / 95 ] loss: 0.524
[ 54 / 95 ] loss: 0.713
[ 55 / 95 ] loss: 0.502
[ 56 / 95 ] loss: 0.713
[ 57 / 95 ] loss: 0.614
[ 58 / 95 ] loss: 0.506
[ 59 / 95 ] loss: 0.556
[ 60 / 95 ] loss: 0.631
[ 61 / 95 ] loss: 0.600
[ 62 / 95 ] loss: 0.663
[ 63 / 95 ] loss: 0.534
[ 64 / 95 ] loss: 0.438
[ 65 / 95 ] loss: 0.531
[ 66 / 95 ] loss: 0.437
[ 67 / 95 ] loss: 0.545
[ 68 / 95 ] loss: 0.638
[ 69 / 95 ] loss: 0.543
[ 70 / 95 ] loss: 0.594
[ 71 / 95 ] loss: 0.611
[ 72 / 95 ] loss: 0.619
[ 73 / 95 ] loss: 0.539
[ 74 / 95 ] loss: 0.473
[ 75 / 95 ] loss: 0.581
[ 76 / 95 ] loss: 0.618
[ 77 / 95 ] loss: 0.824
[ 78 / 95 ] loss: 0.598
[ 79 / 95 ] loss: 0.605
[ 80 / 95 ] loss: 0.617
[ 81 / 95 ] loss: 0.624
[ 82 / 95 ] loss: 0.487
[ 83 / 95 ] loss: 0.445
[ 84 / 95 ] loss: 0.556
[ 85 / 95 ] loss: 0.387
[ 86 / 95 ] loss: 0.433
[ 87 / 95 ] loss: 0.542
[ 88 / 95 ] loss: 0.523
[ 89 / 95 ] loss: 0.422
[ 90 / 95 ] loss: 0.748
[ 91 / 95 ] loss: 0.616
[ 92 / 95 ] loss: 0.598
[ 93 / 95 ] loss: 0.755
[ 94 / 95 ] loss: 0.568
[ 95 / 95 ] loss: 0.584
0.5797094909768356
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.647753
========= epoch: 45 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.557
[ 2 / 95 ] loss: 0.625
[ 3 / 95 ] loss: 0.447
[ 4 / 95 ] loss: 0.673
[ 5 / 95 ] loss: 0.646
[ 6 / 95 ] loss: 0.537
[ 7 / 95 ] loss: 0.876
[ 8 / 95 ] loss: 0.869
[ 9 / 95 ] loss: 0.517
[ 10 / 95 ] loss: 0.562
[ 11 / 95 ] loss: 0.496
[ 12 / 95 ] loss: 0.946
[ 13 / 95 ] loss: 0.656
[ 14 / 95 ] loss: 0.435
[ 15 / 95 ] loss: 0.513
[ 16 / 95 ] loss: 0.749
[ 17 / 95 ] loss: 0.505
[ 18 / 95 ] loss: 0.448
[ 19 / 95 ] loss: 0.507
[ 20 / 95 ] loss: 0.517
[ 21 / 95 ] loss: 0.493
[ 22 / 95 ] loss: 0.543
[ 23 / 95 ] loss: 0.395
[ 24 / 95 ] loss: 0.591
[ 25 / 95 ] loss: 0.460
[ 26 / 95 ] loss: 0.520
[ 27 / 95 ] loss: 0.722
[ 28 / 95 ] loss: 0.636
[ 29 / 95 ] loss: 0.609
[ 30 / 95 ] loss: 0.463
[ 31 / 95 ] loss: 0.716
[ 32 / 95 ] loss: 0.608
[ 33 / 95 ] loss: 0.645
[ 34 / 95 ] loss: 0.527
[ 35 / 95 ] loss: 0.734
[ 36 / 95 ] loss: 0.677
[ 37 / 95 ] loss: 0.426
[ 38 / 95 ] loss: 0.495
[ 39 / 95 ] loss: 0.576
[ 40 / 95 ] loss: 0.555
[ 41 / 95 ] loss: 0.766
[ 42 / 95 ] loss: 0.516
[ 43 / 95 ] loss: 0.534
[ 44 / 95 ] loss: 0.402
[ 45 / 95 ] loss: 0.758
[ 46 / 95 ] loss: 0.639
[ 47 / 95 ] loss: 0.560
[ 48 / 95 ] loss: 0.474
[ 49 / 95 ] loss: 0.642
[ 50 / 95 ] loss: 0.552
[ 51 / 95 ] loss: 0.450
[ 52 / 95 ] loss: 0.535
[ 53 / 95 ] loss: 0.751
[ 54 / 95 ] loss: 0.585
[ 55 / 95 ] loss: 0.593
[ 56 / 95 ] loss: 0.714
[ 57 / 95 ] loss: 0.492
[ 58 / 95 ] loss: 0.730
[ 59 / 95 ] loss: 0.530
[ 60 / 95 ] loss: 0.603
[ 61 / 95 ] loss: 0.490
[ 62 / 95 ] loss: 0.562
[ 63 / 95 ] loss: 0.540
[ 64 / 95 ] loss: 0.593
[ 65 / 95 ] loss: 0.490
[ 66 / 95 ] loss: 0.734
[ 67 / 95 ] loss: 0.592
[ 68 / 95 ] loss: 0.496
[ 69 / 95 ] loss: 0.446
[ 70 / 95 ] loss: 0.618
[ 71 / 95 ] loss: 0.579
[ 72 / 95 ] loss: 0.719
[ 73 / 95 ] loss: 0.780
[ 74 / 95 ] loss: 0.562
[ 75 / 95 ] loss: 0.736
[ 76 / 95 ] loss: 0.474
[ 77 / 95 ] loss: 0.576
[ 78 / 95 ] loss: 0.785
[ 79 / 95 ] loss: 0.539
[ 80 / 95 ] loss: 0.566
[ 81 / 95 ] loss: 0.661
[ 82 / 95 ] loss: 0.515
[ 83 / 95 ] loss: 0.434
[ 84 / 95 ] loss: 0.557
[ 85 / 95 ] loss: 0.416
[ 86 / 95 ] loss: 0.542
[ 87 / 95 ] loss: 0.620
[ 88 / 95 ] loss: 0.514
[ 89 / 95 ] loss: 0.670
[ 90 / 95 ] loss: 0.566
[ 91 / 95 ] loss: 0.537
[ 92 / 95 ] loss: 0.503
[ 93 / 95 ] loss: 0.384
[ 94 / 95 ] loss: 0.697
[ 95 / 95 ] loss: 0.573
0.5827528727682013
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.653039
========= epoch: 46 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.659
[ 2 / 95 ] loss: 0.485
[ 3 / 95 ] loss: 0.427
[ 4 / 95 ] loss: 0.657
[ 5 / 95 ] loss: 0.544
[ 6 / 95 ] loss: 0.481
[ 7 / 95 ] loss: 0.532
[ 8 / 95 ] loss: 0.714
[ 9 / 95 ] loss: 0.514
[ 10 / 95 ] loss: 0.786
[ 11 / 95 ] loss: 0.691
[ 12 / 95 ] loss: 0.583
[ 13 / 95 ] loss: 0.474
[ 14 / 95 ] loss: 0.643
[ 15 / 95 ] loss: 0.492
[ 16 / 95 ] loss: 0.671
[ 17 / 95 ] loss: 0.968
[ 18 / 95 ] loss: 0.546
[ 19 / 95 ] loss: 0.557
[ 20 / 95 ] loss: 0.430
[ 21 / 95 ] loss: 0.563
[ 22 / 95 ] loss: 0.492
[ 23 / 95 ] loss: 0.506
[ 24 / 95 ] loss: 0.472
[ 25 / 95 ] loss: 0.749
[ 26 / 95 ] loss: 0.463
[ 27 / 95 ] loss: 0.487
[ 28 / 95 ] loss: 0.749
[ 29 / 95 ] loss: 0.608
[ 30 / 95 ] loss: 0.466
[ 31 / 95 ] loss: 0.634
[ 32 / 95 ] loss: 0.612
[ 33 / 95 ] loss: 0.596
[ 34 / 95 ] loss: 0.488
[ 35 / 95 ] loss: 0.554
[ 36 / 95 ] loss: 0.519
[ 37 / 95 ] loss: 0.561
[ 38 / 95 ] loss: 0.748
[ 39 / 95 ] loss: 0.664
[ 40 / 95 ] loss: 0.538
[ 41 / 95 ] loss: 0.542
[ 42 / 95 ] loss: 0.642
[ 43 / 95 ] loss: 0.554
[ 44 / 95 ] loss: 0.605
[ 45 / 95 ] loss: 0.590
[ 46 / 95 ] loss: 0.621
[ 47 / 95 ] loss: 0.607
[ 48 / 95 ] loss: 0.767
[ 49 / 95 ] loss: 0.553
[ 50 / 95 ] loss: 0.582
[ 51 / 95 ] loss: 0.654
[ 52 / 95 ] loss: 0.537
[ 53 / 95 ] loss: 0.821
[ 54 / 95 ] loss: 0.518
[ 55 / 95 ] loss: 0.499
[ 56 / 95 ] loss: 0.425
[ 57 / 95 ] loss: 0.742
[ 58 / 95 ] loss: 0.464
[ 59 / 95 ] loss: 0.464
[ 60 / 95 ] loss: 0.490
[ 61 / 95 ] loss: 0.550
[ 62 / 95 ] loss: 0.494
[ 63 / 95 ] loss: 0.557
[ 64 / 95 ] loss: 0.411
[ 65 / 95 ] loss: 0.562
[ 66 / 95 ] loss: 0.289
[ 67 / 95 ] loss: 0.465
[ 68 / 95 ] loss: 0.560
[ 69 / 95 ] loss: 0.515
[ 70 / 95 ] loss: 0.551
[ 71 / 95 ] loss: 0.835
[ 72 / 95 ] loss: 0.782
[ 73 / 95 ] loss: 0.498
[ 74 / 95 ] loss: 0.651
[ 75 / 95 ] loss: 0.484
[ 76 / 95 ] loss: 0.459
[ 77 / 95 ] loss: 0.654
[ 78 / 95 ] loss: 0.699
[ 79 / 95 ] loss: 0.563
[ 80 / 95 ] loss: 0.625
[ 81 / 95 ] loss: 0.584
[ 82 / 95 ] loss: 0.582
[ 83 / 95 ] loss: 0.554
[ 84 / 95 ] loss: 0.541
[ 85 / 95 ] loss: 0.562
[ 86 / 95 ] loss: 0.554
[ 87 / 95 ] loss: 0.688
[ 88 / 95 ] loss: 0.490
[ 89 / 95 ] loss: 0.636
[ 90 / 95 ] loss: 0.572
[ 91 / 95 ] loss: 0.727
[ 92 / 95 ] loss: 0.622
[ 93 / 95 ] loss: 0.761
[ 94 / 95 ] loss: 0.731
[ 95 / 95 ] loss: 0.768
0.5850374799025686
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.652664
========= epoch: 47 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.622
[ 2 / 95 ] loss: 0.586
[ 3 / 95 ] loss: 0.520
[ 4 / 95 ] loss: 0.516
[ 5 / 95 ] loss: 0.625
[ 6 / 95 ] loss: 0.539
[ 7 / 95 ] loss: 0.685
[ 8 / 95 ] loss: 0.610
[ 9 / 95 ] loss: 0.619
[ 10 / 95 ] loss: 0.448
[ 11 / 95 ] loss: 0.582
[ 12 / 95 ] loss: 0.590
[ 13 / 95 ] loss: 0.516
[ 14 / 95 ] loss: 0.646
[ 15 / 95 ] loss: 0.512
[ 16 / 95 ] loss: 0.577
[ 17 / 95 ] loss: 0.540
[ 18 / 95 ] loss: 0.702
[ 19 / 95 ] loss: 0.652
[ 20 / 95 ] loss: 0.667
[ 21 / 95 ] loss: 0.610
[ 22 / 95 ] loss: 0.568
[ 23 / 95 ] loss: 0.569
[ 24 / 95 ] loss: 0.563
[ 25 / 95 ] loss: 0.528
[ 26 / 95 ] loss: 0.589
[ 27 / 95 ] loss: 0.665
[ 28 / 95 ] loss: 0.454
[ 29 / 95 ] loss: 0.453
[ 30 / 95 ] loss: 0.985
[ 31 / 95 ] loss: 0.592
[ 32 / 95 ] loss: 0.582
[ 33 / 95 ] loss: 0.651
[ 34 / 95 ] loss: 0.565
[ 35 / 95 ] loss: 0.644
[ 36 / 95 ] loss: 0.480
[ 37 / 95 ] loss: 0.577
[ 38 / 95 ] loss: 0.541
[ 39 / 95 ] loss: 0.536
[ 40 / 95 ] loss: 0.464
[ 41 / 95 ] loss: 0.659
[ 42 / 95 ] loss: 0.445
[ 43 / 95 ] loss: 0.620
[ 44 / 95 ] loss: 0.404
[ 45 / 95 ] loss: 0.666
[ 46 / 95 ] loss: 0.676
[ 47 / 95 ] loss: 0.421
[ 48 / 95 ] loss: 0.519
[ 49 / 95 ] loss: 0.650
[ 50 / 95 ] loss: 0.530
[ 51 / 95 ] loss: 0.592
[ 52 / 95 ] loss: 0.537
[ 53 / 95 ] loss: 0.641
[ 54 / 95 ] loss: 0.675
[ 55 / 95 ] loss: 0.514
[ 56 / 95 ] loss: 0.555
[ 57 / 95 ] loss: 0.661
[ 58 / 95 ] loss: 0.625
[ 59 / 95 ] loss: 0.491
[ 60 / 95 ] loss: 0.518
[ 61 / 95 ] loss: 0.630
[ 62 / 95 ] loss: 0.626
[ 63 / 95 ] loss: 0.510
[ 64 / 95 ] loss: 0.620
[ 65 / 95 ] loss: 0.532
[ 66 / 95 ] loss: 0.555
[ 67 / 95 ] loss: 0.704
[ 68 / 95 ] loss: 0.563
[ 69 / 95 ] loss: 0.653
[ 70 / 95 ] loss: 0.558
[ 71 / 95 ] loss: 0.650
[ 72 / 95 ] loss: 0.751
[ 73 / 95 ] loss: 0.401
[ 74 / 95 ] loss: 0.580
[ 75 / 95 ] loss: 0.631
[ 76 / 95 ] loss: 0.825
[ 77 / 95 ] loss: 0.571
[ 78 / 95 ] loss: 0.725
[ 79 / 95 ] loss: 0.553
[ 80 / 95 ] loss: 0.520
[ 81 / 95 ] loss: 0.477
[ 82 / 95 ] loss: 0.618
[ 83 / 95 ] loss: 0.536
[ 84 / 95 ] loss: 0.485
[ 85 / 95 ] loss: 0.570
[ 86 / 95 ] loss: 0.580
[ 87 / 95 ] loss: 0.523
[ 88 / 95 ] loss: 0.607
[ 89 / 95 ] loss: 0.660
[ 90 / 95 ] loss: 0.711
[ 91 / 95 ] loss: 0.540
[ 92 / 95 ] loss: 0.593
[ 93 / 95 ] loss: 0.708
[ 94 / 95 ] loss: 0.557
[ 95 / 95 ] loss: 0.439
0.5845295604906584
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.664459
========= epoch: 48 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.543
[ 2 / 95 ] loss: 0.435
[ 3 / 95 ] loss: 0.545
[ 4 / 95 ] loss: 0.448
[ 5 / 95 ] loss: 0.523
[ 6 / 95 ] loss: 0.575
[ 7 / 95 ] loss: 0.591
[ 8 / 95 ] loss: 0.604
[ 9 / 95 ] loss: 0.590
[ 10 / 95 ] loss: 0.530
[ 11 / 95 ] loss: 0.797
[ 12 / 95 ] loss: 0.904
[ 13 / 95 ] loss: 0.772
[ 14 / 95 ] loss: 0.533
[ 15 / 95 ] loss: 0.687
[ 16 / 95 ] loss: 0.392
[ 17 / 95 ] loss: 0.665
[ 18 / 95 ] loss: 0.743
[ 19 / 95 ] loss: 0.518
[ 20 / 95 ] loss: 0.557
[ 21 / 95 ] loss: 0.523
[ 22 / 95 ] loss: 0.412
[ 23 / 95 ] loss: 0.516
[ 24 / 95 ] loss: 0.487
[ 25 / 95 ] loss: 0.641
[ 26 / 95 ] loss: 0.576
[ 27 / 95 ] loss: 0.671
[ 28 / 95 ] loss: 0.560
[ 29 / 95 ] loss: 0.695
[ 30 / 95 ] loss: 0.505
[ 31 / 95 ] loss: 0.555
[ 32 / 95 ] loss: 0.581
[ 33 / 95 ] loss: 0.366
[ 34 / 95 ] loss: 0.490
[ 35 / 95 ] loss: 0.722
[ 36 / 95 ] loss: 0.528
[ 37 / 95 ] loss: 0.563
[ 38 / 95 ] loss: 0.600
[ 39 / 95 ] loss: 0.579
[ 40 / 95 ] loss: 0.593
[ 41 / 95 ] loss: 0.810
[ 42 / 95 ] loss: 0.653
[ 43 / 95 ] loss: 0.622
[ 44 / 95 ] loss: 0.639
[ 45 / 95 ] loss: 0.490
[ 46 / 95 ] loss: 0.513
[ 47 / 95 ] loss: 0.816
[ 48 / 95 ] loss: 0.510
[ 49 / 95 ] loss: 0.497
[ 50 / 95 ] loss: 0.455
[ 51 / 95 ] loss: 0.669
[ 52 / 95 ] loss: 0.528
[ 53 / 95 ] loss: 0.613
[ 54 / 95 ] loss: 0.594
[ 55 / 95 ] loss: 0.683
[ 56 / 95 ] loss: 0.582
[ 57 / 95 ] loss: 0.467
[ 58 / 95 ] loss: 0.449
[ 59 / 95 ] loss: 0.577
[ 60 / 95 ] loss: 0.454
[ 61 / 95 ] loss: 0.595
[ 62 / 95 ] loss: 0.474
[ 63 / 95 ] loss: 0.471
[ 64 / 95 ] loss: 0.642
[ 65 / 95 ] loss: 0.471
[ 66 / 95 ] loss: 0.547
[ 67 / 95 ] loss: 0.525
[ 68 / 95 ] loss: 0.672
[ 69 / 95 ] loss: 0.547
[ 70 / 95 ] loss: 0.523
[ 71 / 95 ] loss: 0.684
[ 72 / 95 ] loss: 0.777
[ 73 / 95 ] loss: 0.660
[ 74 / 95 ] loss: 0.654
[ 75 / 95 ] loss: 0.614
[ 76 / 95 ] loss: 0.510
[ 77 / 95 ] loss: 0.683
[ 78 / 95 ] loss: 0.509
[ 79 / 95 ] loss: 0.561
[ 80 / 95 ] loss: 0.557
[ 81 / 95 ] loss: 0.477
[ 82 / 95 ] loss: 0.576
[ 83 / 95 ] loss: 0.661
[ 84 / 95 ] loss: 0.715
[ 85 / 95 ] loss: 0.678
[ 86 / 95 ] loss: 0.675
[ 87 / 95 ] loss: 0.529
[ 88 / 95 ] loss: 0.585
[ 89 / 95 ] loss: 0.629
[ 90 / 95 ] loss: 0.658
[ 91 / 95 ] loss: 0.454
[ 92 / 95 ] loss: 0.708
[ 93 / 95 ] loss: 0.551
[ 94 / 95 ] loss: 0.521
[ 95 / 95 ] loss: 0.819
0.5857229825697448
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.652967
========= epoch: 49 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.683
[ 2 / 95 ] loss: 0.529
[ 3 / 95 ] loss: 0.585
[ 4 / 95 ] loss: 0.393
[ 5 / 95 ] loss: 0.579
[ 6 / 95 ] loss: 0.532
[ 7 / 95 ] loss: 0.481
[ 8 / 95 ] loss: 0.537
[ 9 / 95 ] loss: 0.519
[ 10 / 95 ] loss: 0.717
[ 11 / 95 ] loss: 0.474
[ 12 / 95 ] loss: 0.634
[ 13 / 95 ] loss: 0.700
[ 14 / 95 ] loss: 0.565
[ 15 / 95 ] loss: 0.475
[ 16 / 95 ] loss: 0.756
[ 17 / 95 ] loss: 0.531
[ 18 / 95 ] loss: 0.607
[ 19 / 95 ] loss: 0.619
[ 20 / 95 ] loss: 0.611
[ 21 / 95 ] loss: 0.536
[ 22 / 95 ] loss: 0.603
[ 23 / 95 ] loss: 0.538
[ 24 / 95 ] loss: 0.528
[ 25 / 95 ] loss: 0.537
[ 26 / 95 ] loss: 0.518
[ 27 / 95 ] loss: 0.642
[ 28 / 95 ] loss: 0.694
[ 29 / 95 ] loss: 0.492
[ 30 / 95 ] loss: 0.534
[ 31 / 95 ] loss: 0.568
[ 32 / 95 ] loss: 0.494
[ 33 / 95 ] loss: 0.581
[ 34 / 95 ] loss: 0.603
[ 35 / 95 ] loss: 0.453
[ 36 / 95 ] loss: 0.630
[ 37 / 95 ] loss: 0.512
[ 38 / 95 ] loss: 0.746
[ 39 / 95 ] loss: 0.506
[ 40 / 95 ] loss: 0.712
[ 41 / 95 ] loss: 0.440
[ 42 / 95 ] loss: 0.583
[ 43 / 95 ] loss: 0.790
[ 44 / 95 ] loss: 0.777
[ 45 / 95 ] loss: 0.585
[ 46 / 95 ] loss: 0.683
[ 47 / 95 ] loss: 0.489
[ 48 / 95 ] loss: 0.451
[ 49 / 95 ] loss: 0.503
[ 50 / 95 ] loss: 0.403
[ 51 / 95 ] loss: 0.624
[ 52 / 95 ] loss: 0.532
[ 53 / 95 ] loss: 0.513
[ 54 / 95 ] loss: 0.601
[ 55 / 95 ] loss: 0.806
[ 56 / 95 ] loss: 0.678
[ 57 / 95 ] loss: 0.497
[ 58 / 95 ] loss: 0.660
[ 59 / 95 ] loss: 0.827
[ 60 / 95 ] loss: 0.442
[ 61 / 95 ] loss: 0.716
[ 62 / 95 ] loss: 0.504
[ 63 / 95 ] loss: 0.554
[ 64 / 95 ] loss: 0.386
[ 65 / 95 ] loss: 0.514
[ 66 / 95 ] loss: 0.650
[ 67 / 95 ] loss: 0.609
[ 68 / 95 ] loss: 0.551
[ 69 / 95 ] loss: 0.812
[ 70 / 95 ] loss: 0.651
[ 71 / 95 ] loss: 0.739
[ 72 / 95 ] loss: 0.584
[ 73 / 95 ] loss: 0.493
[ 74 / 95 ] loss: 0.567
[ 75 / 95 ] loss: 0.638
[ 76 / 95 ] loss: 0.606
[ 77 / 95 ] loss: 0.664
[ 78 / 95 ] loss: 0.838
[ 79 / 95 ] loss: 0.699
[ 80 / 95 ] loss: 0.658
[ 81 / 95 ] loss: 0.487
[ 82 / 95 ] loss: 0.627
[ 83 / 95 ] loss: 0.556
[ 84 / 95 ] loss: 0.613
[ 85 / 95 ] loss: 0.477
[ 86 / 95 ] loss: 0.492
[ 87 / 95 ] loss: 0.548
[ 88 / 95 ] loss: 0.558
[ 89 / 95 ] loss: 0.554
[ 90 / 95 ] loss: 0.593
[ 91 / 95 ] loss: 0.522
[ 92 / 95 ] loss: 0.490
[ 93 / 95 ] loss: 0.529
[ 94 / 95 ] loss: 0.538
[ 95 / 95 ] loss: 0.897
0.586859401276237
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.653053
========= epoch: 50 ==============
/root/anaconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 95 ] loss: 0.413
[ 2 / 95 ] loss: 0.465
[ 3 / 95 ] loss: 0.426
[ 4 / 95 ] loss: 0.580
[ 5 / 95 ] loss: 0.623
[ 6 / 95 ] loss: 0.516
[ 7 / 95 ] loss: 0.629
[ 8 / 95 ] loss: 0.722
[ 9 / 95 ] loss: 0.582
[ 10 / 95 ] loss: 0.586
[ 11 / 95 ] loss: 0.649
[ 12 / 95 ] loss: 0.692
[ 13 / 95 ] loss: 0.598
[ 14 / 95 ] loss: 0.578
[ 15 / 95 ] loss: 0.677
[ 16 / 95 ] loss: 0.374
[ 17 / 95 ] loss: 0.549
[ 18 / 95 ] loss: 0.482
[ 19 / 95 ] loss: 0.658
[ 20 / 95 ] loss: 0.568
[ 21 / 95 ] loss: 0.644
[ 22 / 95 ] loss: 0.510
[ 23 / 95 ] loss: 0.569
[ 24 / 95 ] loss: 0.663
[ 25 / 95 ] loss: 0.517
[ 26 / 95 ] loss: 0.503
[ 27 / 95 ] loss: 0.532
[ 28 / 95 ] loss: 0.571
[ 29 / 95 ] loss: 0.615
[ 30 / 95 ] loss: 0.495
[ 31 / 95 ] loss: 0.703
[ 32 / 95 ] loss: 0.520
[ 33 / 95 ] loss: 0.396
[ 34 / 95 ] loss: 0.568
[ 35 / 95 ] loss: 0.544
[ 36 / 95 ] loss: 0.653
[ 37 / 95 ] loss: 0.620
[ 38 / 95 ] loss: 0.487
[ 39 / 95 ] loss: 0.644
[ 40 / 95 ] loss: 0.708
[ 41 / 95 ] loss: 0.682
[ 42 / 95 ] loss: 0.538
[ 43 / 95 ] loss: 0.618
[ 44 / 95 ] loss: 0.668
[ 45 / 95 ] loss: 0.494
[ 46 / 95 ] loss: 0.763
[ 47 / 95 ] loss: 0.576
[ 48 / 95 ] loss: 0.561
[ 49 / 95 ] loss: 0.519
[ 50 / 95 ] loss: 0.494
[ 51 / 95 ] loss: 0.682
[ 52 / 95 ] loss: 0.618
[ 53 / 95 ] loss: 0.688
[ 54 / 95 ] loss: 0.514
[ 55 / 95 ] loss: 0.599
[ 56 / 95 ] loss: 0.597
[ 57 / 95 ] loss: 0.539
[ 58 / 95 ] loss: 0.745
[ 59 / 95 ] loss: 0.525
[ 60 / 95 ] loss: 0.803
[ 61 / 95 ] loss: 0.744
[ 62 / 95 ] loss: 0.523
[ 63 / 95 ] loss: 0.457
[ 64 / 95 ] loss: 0.473
[ 65 / 95 ] loss: 0.545
[ 66 / 95 ] loss: 0.620
[ 67 / 95 ] loss: 0.700
[ 68 / 95 ] loss: 0.648
[ 69 / 95 ] loss: 0.604
[ 70 / 95 ] loss: 0.502
[ 71 / 95 ] loss: 0.520
[ 72 / 95 ] loss: 0.637
[ 73 / 95 ] loss: 0.445
[ 74 / 95 ] loss: 0.481
[ 75 / 95 ] loss: 0.494
[ 76 / 95 ] loss: 0.652
[ 77 / 95 ] loss: 0.506
[ 78 / 95 ] loss: 0.628
[ 79 / 95 ] loss: 0.458
[ 80 / 95 ] loss: 0.601
[ 81 / 95 ] loss: 0.472
[ 82 / 95 ] loss: 0.489
[ 83 / 95 ] loss: 0.851
[ 84 / 95 ] loss: 0.546
[ 85 / 95 ] loss: 0.576
[ 86 / 95 ] loss: 0.747
[ 87 / 95 ] loss: 0.603
[ 88 / 95 ] loss: 0.510
[ 89 / 95 ] loss: 0.578
[ 90 / 95 ] loss: 0.511
[ 91 / 95 ] loss: 0.513
[ 92 / 95 ] loss: 0.861
[ 93 / 95 ] loss: 0.717
[ 94 / 95 ] loss: 0.403
[ 95 / 95 ] loss: 0.542
0.5811070294756638
Accuracy: 0.702918 -- Precision: 0.661538 -- Recall: 0.990783 -- F1: 0.793358 -- AUC: 0.646400
