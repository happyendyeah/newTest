nohup: ignoring input
训练集: 946
测试集: 237
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 0 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.714
[ 2 / 60 ] loss: 0.685
[ 3 / 60 ] loss: 0.699
[ 4 / 60 ] loss: 0.689
Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors
[ 5 / 60 ] loss: 0.686
[ 6 / 60 ] loss: 0.702
[ 7 / 60 ] loss: 0.686
[ 8 / 60 ] loss: 0.671
[ 9 / 60 ] loss: 0.695
[ 10 / 60 ] loss: 0.653
[ 11 / 60 ] loss: 0.652
[ 12 / 60 ] loss: 0.643
[ 13 / 60 ] loss: 0.671
[ 14 / 60 ] loss: 0.707
[ 15 / 60 ] loss: 0.654
[ 16 / 60 ] loss: 0.657
[ 17 / 60 ] loss: 0.674
[ 18 / 60 ] loss: 0.714
[ 19 / 60 ] loss: 0.665
[ 20 / 60 ] loss: 0.652
[ 21 / 60 ] loss: 0.688
[ 22 / 60 ] loss: 0.663
[ 23 / 60 ] loss: 0.692
[ 24 / 60 ] loss: 0.716
[ 25 / 60 ] loss: 0.687
[ 26 / 60 ] loss: 0.662
[ 27 / 60 ] loss: 0.639
[ 28 / 60 ] loss: 0.648
[ 29 / 60 ] loss: 0.694
[ 30 / 60 ] loss: 0.635
[ 31 / 60 ] loss: 0.648
[ 32 / 60 ] loss: 0.597
[ 33 / 60 ] loss: 0.573
[ 34 / 60 ] loss: 0.566
[ 35 / 60 ] loss: 0.683
[ 36 / 60 ] loss: 0.870
[ 37 / 60 ] loss: 0.684
[ 38 / 60 ] loss: 0.587
[ 39 / 60 ] loss: 0.648
[ 40 / 60 ] loss: 0.674
[ 41 / 60 ] loss: 0.632
[ 42 / 60 ] loss: 0.721
[ 43 / 60 ] loss: 0.629
[ 44 / 60 ] loss: 0.656
[ 45 / 60 ] loss: 0.632
[ 46 / 60 ] loss: 0.663
[ 47 / 60 ] loss: 0.901
[ 48 / 60 ] loss: 0.625
[ 49 / 60 ] loss: 0.701
[ 50 / 60 ] loss: 0.705
[ 51 / 60 ] loss: 0.693
[ 52 / 60 ] loss: 0.609
[ 53 / 60 ] loss: 0.763
[ 54 / 60 ] loss: 0.693
[ 55 / 60 ] loss: 0.665
[ 56 / 60 ] loss: 0.620
[ 57 / 60 ] loss: 0.675
[ 58 / 60 ] loss: 0.825
[ 59 / 60 ] loss: 0.690
[ 60 / 60 ] loss: 0.829
0.6791604260603586
Accuracy: 0.573840 -- Precision: 0.565611 -- Recall: 0.961538 -- F1: 0.712251 -- AUC: 0.724443
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[15, 87, 154, 170, 158, 80, 141, 143, 173, 86, 208, 33, 7, 35, 184, 225, 181, 101, 178, 108, 73, 216, 140, 187, 182, 157, 130, 125, 14, 148, 168, 212, 84, 38, 234, 236, 49, 149, 18, 152, 198, 183, 100, 228, 220, 111, 47, 204, 199, 95, 180, 5, 61, 133, 62, 70, 44, 9, 78, 10, 162, 164, 102, 134, 188, 112, 20, 8, 169, 230, 24, 128, 224, 54, 156, 131, 50, 121, 210, 40, 29, 223, 69, 31, 118, 172, 113, 221, 206, 231, 155, 129, 123, 81, 94, 135, 21, 64, 171, 197, 98, 115, 13, 39, 104, 96, 145, 97, 146, 52, 1, 17, 53, 30, 229, 91, 65, 114, 222, 34, 77, 179, 166, 235, 19, 107, 55, 186, 25, 75, 163, 227, 109, 139, 28, 43, 200, 105, 4, 217, 82, 117, 167, 41, 57, 83, 26, 209, 233, 213, 214, 116, 190, 72, 144, 122, 23, 79, 194, 89, 205, 32, 63, 120, 74, 136, 45, 42, 46, 27, 150, 196, 103, 201, 203, 137, 48, 185, 193, 93, 71, 85, 207, 177, 218, 12, 219, 232, 106, 195, 138, 175, 36, 66, 191, 59, 88, 147, 142, 153, 160, 159, 11, 58, 226, 124, 22, 76, 202, 51, 165, 2, 3, 16, 237, 60, 67, 110, 56, 6, 151, 189, 132, 174, 92, 37, 90, 68, 126, 215, 161, 119, 192, 176, 99, 211, 127]
[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]
[0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 1 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.633
[ 2 / 60 ] loss: 0.647
[ 3 / 60 ] loss: 0.645
[ 4 / 60 ] loss: 0.626
[ 5 / 60 ] loss: 0.724
[ 6 / 60 ] loss: 0.616
[ 7 / 60 ] loss: 0.694
[ 8 / 60 ] loss: 0.636
[ 9 / 60 ] loss: 0.690
[ 10 / 60 ] loss: 0.646
[ 11 / 60 ] loss: 0.610
[ 12 / 60 ] loss: 0.681
[ 13 / 60 ] loss: 0.595
[ 14 / 60 ] loss: 0.619
[ 15 / 60 ] loss: 0.618
[ 16 / 60 ] loss: 0.650
[ 17 / 60 ] loss: 0.602
[ 18 / 60 ] loss: 0.585
[ 19 / 60 ] loss: 0.610
[ 20 / 60 ] loss: 0.557
[ 21 / 60 ] loss: 0.499
[ 22 / 60 ] loss: 0.607
[ 23 / 60 ] loss: 0.595
[ 24 / 60 ] loss: 0.622
[ 25 / 60 ] loss: 0.674
[ 26 / 60 ] loss: 0.660
[ 27 / 60 ] loss: 0.656
[ 28 / 60 ] loss: 0.701
[ 29 / 60 ] loss: 0.733
[ 30 / 60 ] loss: 0.801
[ 31 / 60 ] loss: 0.778
[ 32 / 60 ] loss: 0.712
[ 33 / 60 ] loss: 0.577
[ 34 / 60 ] loss: 0.696
[ 35 / 60 ] loss: 0.749
[ 36 / 60 ] loss: 0.605
[ 37 / 60 ] loss: 0.631
[ 38 / 60 ] loss: 0.669
[ 39 / 60 ] loss: 0.630
[ 40 / 60 ] loss: 0.646
[ 41 / 60 ] loss: 0.581
[ 42 / 60 ] loss: 0.677
[ 43 / 60 ] loss: 0.735
[ 44 / 60 ] loss: 0.509
[ 45 / 60 ] loss: 0.763
[ 46 / 60 ] loss: 0.540
[ 47 / 60 ] loss: 0.544
[ 48 / 60 ] loss: 0.668
[ 49 / 60 ] loss: 0.571
[ 50 / 60 ] loss: 0.712
[ 51 / 60 ] loss: 0.697
[ 52 / 60 ] loss: 0.484
[ 53 / 60 ] loss: 0.691
[ 54 / 60 ] loss: 0.520
[ 55 / 60 ] loss: 0.535
[ 56 / 60 ] loss: 0.701
[ 57 / 60 ] loss: 0.632
[ 58 / 60 ] loss: 0.625
[ 59 / 60 ] loss: 0.606
[ 60 / 60 ] loss: 0.706
0.6403677841027577
Accuracy: 0.611814 -- Precision: 0.679245 -- Recall: 0.553846 -- F1: 0.610169 -- AUC: 0.692523
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[166, 41, 69, 207, 132, 3, 182, 140, 35, 209, 104, 127, 157, 134, 65, 44, 221, 34, 144, 164, 169, 89, 193, 11, 210, 142, 31, 24, 74, 199, 25, 223, 213, 205, 73, 188, 21, 75, 186, 168, 139, 217, 20, 153, 84, 114, 63, 148, 111, 16, 70, 12, 208, 154, 128, 123, 1, 78, 156, 81, 138, 109, 136, 175, 119, 62, 196, 54, 125, 179, 46, 40, 45, 224, 51, 71, 121, 29, 5, 82, 161, 7, 163, 13, 66, 206, 183, 174, 124, 79, 49, 56, 235, 72, 126, 18, 86, 90, 225, 38, 151, 100, 160, 197, 189, 93, 15, 212, 236, 232, 145, 230, 67, 172, 135, 192, 231, 61, 227, 6, 19, 171, 234, 117, 141, 167, 147, 215, 108, 2, 112, 201, 36, 98, 94, 99, 143, 27, 165, 33, 137, 77, 58, 131, 214, 28, 52, 43, 64, 87, 48, 173, 9, 170, 39, 4, 57, 211, 219, 195, 76, 203, 55, 233, 162, 14, 26, 129, 146, 47, 204, 181, 202, 105, 113, 97, 194, 191, 118, 122, 187, 155, 59, 152, 80, 150, 158, 180, 17, 92, 110, 178, 184, 226, 22, 130, 106, 50, 83, 198, 229, 8, 185, 101, 176, 42, 190, 37, 220, 159, 115, 116, 91, 177, 53, 60, 30, 120, 103, 107, 228, 218, 149, 68, 200, 216, 10, 222, 85, 23, 95, 96, 88, 133, 32, 102, 237]
[0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]
[1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 2 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.668
[ 2 / 60 ] loss: 0.579
[ 3 / 60 ] loss: 0.632
[ 4 / 60 ] loss: 0.575
[ 5 / 60 ] loss: 0.595
[ 6 / 60 ] loss: 0.582
[ 7 / 60 ] loss: 0.651
[ 8 / 60 ] loss: 0.485
[ 9 / 60 ] loss: 0.569
[ 10 / 60 ] loss: 0.544
[ 11 / 60 ] loss: 0.604
[ 12 / 60 ] loss: 0.568
[ 13 / 60 ] loss: 0.642
[ 14 / 60 ] loss: 0.640
[ 15 / 60 ] loss: 0.636
[ 16 / 60 ] loss: 0.744
[ 17 / 60 ] loss: 0.587
[ 18 / 60 ] loss: 0.507
[ 19 / 60 ] loss: 0.445
[ 20 / 60 ] loss: 0.487
[ 21 / 60 ] loss: 0.547
[ 22 / 60 ] loss: 0.508
[ 23 / 60 ] loss: 0.371
[ 24 / 60 ] loss: 0.681
[ 25 / 60 ] loss: 0.597
[ 26 / 60 ] loss: 0.718
[ 27 / 60 ] loss: 0.700
[ 28 / 60 ] loss: 0.731
[ 29 / 60 ] loss: 0.409
[ 30 / 60 ] loss: 0.698
[ 31 / 60 ] loss: 0.595
[ 32 / 60 ] loss: 0.492
[ 33 / 60 ] loss: 0.512
[ 34 / 60 ] loss: 0.583
[ 35 / 60 ] loss: 0.520
[ 36 / 60 ] loss: 0.605
[ 37 / 60 ] loss: 0.603
[ 38 / 60 ] loss: 0.523
[ 39 / 60 ] loss: 0.503
[ 40 / 60 ] loss: 0.622
[ 41 / 60 ] loss: 0.538
[ 42 / 60 ] loss: 0.693
[ 43 / 60 ] loss: 0.638
[ 44 / 60 ] loss: 0.429
[ 45 / 60 ] loss: 0.387
[ 46 / 60 ] loss: 0.654
[ 47 / 60 ] loss: 0.855
[ 48 / 60 ] loss: 0.656
[ 49 / 60 ] loss: 0.491
[ 50 / 60 ] loss: 0.415
[ 51 / 60 ] loss: 0.664
[ 52 / 60 ] loss: 0.598
[ 53 / 60 ] loss: 0.393
[ 54 / 60 ] loss: 0.370
[ 55 / 60 ] loss: 0.559
[ 56 / 60 ] loss: 0.628
[ 57 / 60 ] loss: 0.429
[ 58 / 60 ] loss: 0.535
[ 59 / 60 ] loss: 0.397
[ 60 / 60 ] loss: 0.240
0.5638313541809717
Accuracy: 0.683544 -- Precision: 0.687075 -- Recall: 0.776923 -- F1: 0.729242 -- AUC: 0.741265
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[1, 52, 80, 30, 63, 131, 155, 145, 177, 86, 235, 217, 74, 195, 112, 116, 38, 187, 41, 2, 148, 28, 226, 178, 190, 85, 229, 92, 143, 15, 5, 75, 159, 67, 132, 156, 211, 76, 231, 104, 77, 185, 193, 188, 127, 45, 121, 215, 98, 169, 111, 142, 125, 17, 90, 232, 95, 140, 130, 103, 171, 59, 46, 202, 165, 32, 179, 60, 27, 182, 212, 203, 205, 108, 158, 70, 164, 221, 48, 122, 220, 223, 110, 160, 13, 96, 168, 65, 105, 88, 6, 33, 172, 207, 16, 99, 93, 199, 51, 25, 210, 94, 9, 118, 138, 136, 78, 213, 237, 184, 29, 224, 170, 53, 189, 68, 126, 181, 119, 89, 55, 135, 149, 163, 144, 101, 50, 24, 54, 150, 137, 198, 26, 233, 196, 225, 91, 87, 34, 153, 49, 201, 11, 133, 31, 167, 71, 10, 113, 139, 166, 56, 3, 128, 120, 204, 180, 214, 134, 79, 102, 109, 58, 66, 209, 219, 222, 176, 194, 117, 197, 21, 123, 107, 236, 37, 73, 22, 40, 206, 42, 43, 208, 183, 192, 35, 216, 173, 228, 97, 18, 61, 218, 147, 81, 114, 12, 19, 72, 174, 23, 230, 186, 8, 62, 146, 227, 4, 20, 64, 141, 151, 124, 115, 106, 154, 175, 200, 161, 36, 44, 84, 57, 100, 14, 152, 83, 157, 82, 47, 191, 162, 69, 129, 39, 234, 7]
[1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 3 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.541
[ 2 / 60 ] loss: 0.521
[ 3 / 60 ] loss: 0.381
[ 4 / 60 ] loss: 0.299
[ 5 / 60 ] loss: 0.423
[ 6 / 60 ] loss: 0.470
[ 7 / 60 ] loss: 0.229
[ 8 / 60 ] loss: 0.388
[ 9 / 60 ] loss: 0.522
[ 10 / 60 ] loss: 0.435
[ 11 / 60 ] loss: 0.688
[ 12 / 60 ] loss: 0.595
[ 13 / 60 ] loss: 0.597
[ 14 / 60 ] loss: 0.514
[ 15 / 60 ] loss: 0.526
[ 16 / 60 ] loss: 0.243
[ 17 / 60 ] loss: 0.863
[ 18 / 60 ] loss: 0.919
[ 19 / 60 ] loss: 0.702
[ 20 / 60 ] loss: 0.348
[ 21 / 60 ] loss: 0.665
[ 22 / 60 ] loss: 0.429
[ 23 / 60 ] loss: 0.683
[ 24 / 60 ] loss: 0.591
[ 25 / 60 ] loss: 0.526
[ 26 / 60 ] loss: 0.627
[ 27 / 60 ] loss: 0.639
[ 28 / 60 ] loss: 0.511
[ 29 / 60 ] loss: 0.585
[ 30 / 60 ] loss: 0.541
[ 31 / 60 ] loss: 0.502
[ 32 / 60 ] loss: 0.645
[ 33 / 60 ] loss: 0.642
[ 34 / 60 ] loss: 0.480
[ 35 / 60 ] loss: 0.474
[ 36 / 60 ] loss: 0.370
[ 37 / 60 ] loss: 0.542
[ 38 / 60 ] loss: 0.380
[ 39 / 60 ] loss: 0.502
[ 40 / 60 ] loss: 0.561
[ 41 / 60 ] loss: 0.399
[ 42 / 60 ] loss: 0.601
[ 43 / 60 ] loss: 0.461
[ 44 / 60 ] loss: 0.408
[ 45 / 60 ] loss: 0.439
[ 46 / 60 ] loss: 0.838
[ 47 / 60 ] loss: 0.486
[ 48 / 60 ] loss: 0.496
[ 49 / 60 ] loss: 0.370
[ 50 / 60 ] loss: 0.461
[ 51 / 60 ] loss: 0.802
[ 52 / 60 ] loss: 0.581
[ 53 / 60 ] loss: 0.858
[ 54 / 60 ] loss: 0.531
[ 55 / 60 ] loss: 0.371
[ 56 / 60 ] loss: 0.481
[ 57 / 60 ] loss: 0.663
[ 58 / 60 ] loss: 0.632
[ 59 / 60 ] loss: 0.498
[ 60 / 60 ] loss: 0.285
0.529340014855067
Accuracy: 0.679325 -- Precision: 0.684932 -- Recall: 0.769231 -- F1: 0.724638 -- AUC: 0.726168
========= epoch: 4 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.382
[ 2 / 60 ] loss: 0.486
[ 3 / 60 ] loss: 0.719
[ 4 / 60 ] loss: 0.578
[ 5 / 60 ] loss: 0.779
[ 6 / 60 ] loss: 0.539
[ 7 / 60 ] loss: 0.612
[ 8 / 60 ] loss: 0.444
[ 9 / 60 ] loss: 0.555
[ 10 / 60 ] loss: 0.353
[ 11 / 60 ] loss: 0.695
[ 12 / 60 ] loss: 0.505
[ 13 / 60 ] loss: 0.451
[ 14 / 60 ] loss: 0.533
[ 15 / 60 ] loss: 0.323
[ 16 / 60 ] loss: 0.521
[ 17 / 60 ] loss: 0.518
[ 18 / 60 ] loss: 0.476
[ 19 / 60 ] loss: 0.302
[ 20 / 60 ] loss: 0.476
[ 21 / 60 ] loss: 0.340
[ 22 / 60 ] loss: 0.408
[ 23 / 60 ] loss: 0.671
[ 24 / 60 ] loss: 0.612
[ 25 / 60 ] loss: 0.445
[ 26 / 60 ] loss: 0.440
[ 27 / 60 ] loss: 0.445
[ 28 / 60 ] loss: 0.542
[ 29 / 60 ] loss: 0.674
[ 30 / 60 ] loss: 0.307
[ 31 / 60 ] loss: 0.242
[ 32 / 60 ] loss: 0.447
[ 33 / 60 ] loss: 0.434
[ 34 / 60 ] loss: 0.628
[ 35 / 60 ] loss: 0.431
[ 36 / 60 ] loss: 0.578
[ 37 / 60 ] loss: 0.730
[ 38 / 60 ] loss: 0.452
[ 39 / 60 ] loss: 0.486
[ 40 / 60 ] loss: 0.517
[ 41 / 60 ] loss: 0.295
[ 42 / 60 ] loss: 0.406
[ 43 / 60 ] loss: 0.458
[ 44 / 60 ] loss: 0.505
[ 45 / 60 ] loss: 0.541
[ 46 / 60 ] loss: 0.549
[ 47 / 60 ] loss: 0.412
[ 48 / 60 ] loss: 0.489
[ 49 / 60 ] loss: 0.268
[ 50 / 60 ] loss: 0.595
[ 51 / 60 ] loss: 0.666
[ 52 / 60 ] loss: 0.462
[ 53 / 60 ] loss: 0.282
[ 54 / 60 ] loss: 0.452
[ 55 / 60 ] loss: 0.426
[ 56 / 60 ] loss: 0.510
[ 57 / 60 ] loss: 0.353
[ 58 / 60 ] loss: 0.266
[ 59 / 60 ] loss: 0.647
[ 60 / 60 ] loss: 0.173
0.48050130953391396
Accuracy: 0.700422 -- Precision: 0.747899 -- Recall: 0.684615 -- F1: 0.714859 -- AUC: 0.764702
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[118, 140, 1, 184, 137, 207, 169, 83, 40, 197, 159, 236, 3, 70, 16, 219, 26, 164, 151, 149, 182, 47, 103, 28, 125, 122, 27, 24, 68, 13, 67, 185, 43, 217, 128, 66, 20, 216, 33, 11, 9, 181, 42, 59, 74, 102, 86, 221, 191, 38, 205, 91, 195, 155, 123, 192, 101, 95, 157, 37, 223, 88, 61, 126, 176, 186, 206, 200, 234, 92, 201, 183, 19, 30, 167, 48, 120, 6, 202, 41, 8, 107, 25, 152, 90, 96, 87, 64, 12, 94, 62, 45, 114, 160, 194, 32, 141, 113, 188, 153, 214, 31, 14, 54, 163, 218, 75, 237, 224, 55, 108, 227, 109, 138, 57, 76, 161, 150, 129, 17, 51, 148, 142, 180, 10, 143, 52, 220, 199, 104, 235, 73, 134, 98, 154, 203, 82, 60, 110, 106, 193, 39, 49, 135, 69, 170, 212, 89, 36, 35, 53, 232, 133, 80, 44, 58, 225, 215, 71, 209, 81, 231, 21, 175, 34, 136, 228, 85, 156, 72, 127, 132, 210, 7, 139, 179, 147, 158, 230, 145, 178, 46, 198, 187, 165, 119, 56, 121, 146, 213, 117, 93, 5, 171, 189, 84, 29, 204, 115, 173, 168, 65, 177, 131, 15, 174, 233, 196, 162, 100, 144, 18, 208, 211, 79, 124, 4, 2, 116, 222, 190, 78, 50, 63, 97, 112, 166, 77, 23, 226, 105, 172, 22, 99, 111, 229, 130]
[0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1]
[1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 5 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.532
[ 2 / 60 ] loss: 0.356
[ 3 / 60 ] loss: 0.293
[ 4 / 60 ] loss: 0.234
[ 5 / 60 ] loss: 0.548
[ 6 / 60 ] loss: 0.288
[ 7 / 60 ] loss: 0.538
[ 8 / 60 ] loss: 0.251
[ 9 / 60 ] loss: 0.278
[ 10 / 60 ] loss: 0.384
[ 11 / 60 ] loss: 0.256
[ 12 / 60 ] loss: 0.327
[ 13 / 60 ] loss: 0.612
[ 14 / 60 ] loss: 0.445
[ 15 / 60 ] loss: 0.571
[ 16 / 60 ] loss: 0.275
[ 17 / 60 ] loss: 0.407
[ 18 / 60 ] loss: 0.343
[ 19 / 60 ] loss: 0.422
[ 20 / 60 ] loss: 0.264
[ 21 / 60 ] loss: 0.432
[ 22 / 60 ] loss: 0.376
[ 23 / 60 ] loss: 0.532
[ 24 / 60 ] loss: 0.478
[ 25 / 60 ] loss: 0.472
[ 26 / 60 ] loss: 0.545
[ 27 / 60 ] loss: 0.661
[ 28 / 60 ] loss: 0.484
[ 29 / 60 ] loss: 0.414
[ 30 / 60 ] loss: 0.479
[ 31 / 60 ] loss: 0.626
[ 32 / 60 ] loss: 0.166
[ 33 / 60 ] loss: 0.449
[ 34 / 60 ] loss: 0.220
[ 35 / 60 ] loss: 0.344
[ 36 / 60 ] loss: 0.280
[ 37 / 60 ] loss: 0.325
[ 38 / 60 ] loss: 0.269
[ 39 / 60 ] loss: 0.356
[ 40 / 60 ] loss: 0.210
[ 41 / 60 ] loss: 0.229
[ 42 / 60 ] loss: 0.302
[ 43 / 60 ] loss: 0.494
[ 44 / 60 ] loss: 0.317
[ 45 / 60 ] loss: 0.229
[ 46 / 60 ] loss: 0.166
[ 47 / 60 ] loss: 0.278
[ 48 / 60 ] loss: 0.423
[ 49 / 60 ] loss: 1.004
[ 50 / 60 ] loss: 0.320
[ 51 / 60 ] loss: 0.432
[ 52 / 60 ] loss: 0.446
[ 53 / 60 ] loss: 0.730
[ 54 / 60 ] loss: 0.555
[ 55 / 60 ] loss: 0.378
[ 56 / 60 ] loss: 0.388
[ 57 / 60 ] loss: 0.694
[ 58 / 60 ] loss: 0.581
[ 59 / 60 ] loss: 0.515
[ 60 / 60 ] loss: 1.181
0.4233783945441246
Accuracy: 0.637131 -- Precision: 0.614583 -- Recall: 0.907692 -- F1: 0.732919 -- AUC: 0.638605
========= epoch: 6 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.572
[ 2 / 60 ] loss: 0.499
[ 3 / 60 ] loss: 0.564
[ 4 / 60 ] loss: 0.394
[ 5 / 60 ] loss: 0.368
[ 6 / 60 ] loss: 0.344
[ 7 / 60 ] loss: 0.521
[ 8 / 60 ] loss: 0.262
[ 9 / 60 ] loss: 0.574
[ 10 / 60 ] loss: 0.430
[ 11 / 60 ] loss: 0.527
[ 12 / 60 ] loss: 0.375
[ 13 / 60 ] loss: 0.189
[ 14 / 60 ] loss: 0.589
[ 15 / 60 ] loss: 0.479
[ 16 / 60 ] loss: 0.251
[ 17 / 60 ] loss: 0.455
[ 18 / 60 ] loss: 0.529
[ 19 / 60 ] loss: 0.224
[ 20 / 60 ] loss: 0.632
[ 21 / 60 ] loss: 0.335
[ 22 / 60 ] loss: 0.413
[ 23 / 60 ] loss: 0.354
[ 24 / 60 ] loss: 0.914
[ 25 / 60 ] loss: 0.273
[ 26 / 60 ] loss: 0.417
[ 27 / 60 ] loss: 0.510
[ 28 / 60 ] loss: 0.501
[ 29 / 60 ] loss: 0.404
[ 30 / 60 ] loss: 0.352
[ 31 / 60 ] loss: 0.173
[ 32 / 60 ] loss: 0.545
[ 33 / 60 ] loss: 0.488
[ 34 / 60 ] loss: 0.450
[ 35 / 60 ] loss: 0.716
[ 36 / 60 ] loss: 0.637
[ 37 / 60 ] loss: 0.403
[ 38 / 60 ] loss: 0.413
[ 39 / 60 ] loss: 0.524
[ 40 / 60 ] loss: 0.332
[ 41 / 60 ] loss: 0.273
[ 42 / 60 ] loss: 0.432
[ 43 / 60 ] loss: 0.694
[ 44 / 60 ] loss: 0.721
[ 45 / 60 ] loss: 0.539
[ 46 / 60 ] loss: 0.267
[ 47 / 60 ] loss: 0.355
[ 48 / 60 ] loss: 0.521
[ 49 / 60 ] loss: 0.307
[ 50 / 60 ] loss: 0.426
[ 51 / 60 ] loss: 0.256
[ 52 / 60 ] loss: 0.272
[ 53 / 60 ] loss: 0.414
[ 54 / 60 ] loss: 0.331
[ 55 / 60 ] loss: 0.427
[ 56 / 60 ] loss: 0.355
[ 57 / 60 ] loss: 0.567
[ 58 / 60 ] loss: 0.499
[ 59 / 60 ] loss: 0.385
[ 60 / 60 ] loss: 0.264
0.4372611331442992
Accuracy: 0.713080 -- Precision: 0.746032 -- Recall: 0.723077 -- F1: 0.734375 -- AUC: 0.774623
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[64, 30, 211, 136, 183, 207, 196, 73, 146, 44, 176, 16, 72, 153, 106, 115, 35, 4, 224, 76, 13, 101, 161, 37, 58, 230, 202, 92, 156, 234, 216, 109, 124, 93, 67, 49, 7, 205, 63, 24, 70, 139, 171, 125, 236, 117, 172, 173, 201, 22, 167, 33, 212, 27, 56, 51, 26, 85, 47, 38, 231, 217, 74, 128, 32, 71, 88, 82, 138, 20, 90, 113, 195, 226, 194, 182, 60, 134, 11, 111, 199, 152, 237, 87, 39, 103, 157, 197, 225, 148, 61, 235, 79, 209, 99, 184, 221, 83, 114, 102, 121, 132, 59, 210, 193, 28, 119, 127, 187, 200, 155, 222, 112, 55, 62, 175, 158, 190, 116, 192, 75, 42, 137, 126, 23, 3, 2, 31, 110, 50, 174, 108, 69, 142, 21, 97, 78, 41, 218, 40, 94, 203, 179, 53, 18, 14, 131, 220, 166, 89, 144, 154, 178, 57, 204, 96, 77, 66, 123, 95, 65, 169, 180, 1, 52, 219, 164, 81, 143, 145, 5, 133, 129, 46, 45, 233, 98, 80, 227, 206, 8, 6, 228, 9, 43, 151, 149, 223, 48, 177, 160, 68, 162, 54, 232, 15, 130, 84, 91, 10, 29, 159, 100, 36, 163, 215, 86, 189, 107, 188, 168, 135, 165, 185, 181, 17, 208, 34, 170, 141, 191, 214, 104, 120, 140, 118, 198, 19, 147, 150, 12, 105, 186, 122, 213, 25, 229]
[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]
[0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 7 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.553
[ 2 / 60 ] loss: 0.358
[ 3 / 60 ] loss: 0.257
[ 4 / 60 ] loss: 0.306
[ 5 / 60 ] loss: 0.182
[ 6 / 60 ] loss: 0.378
[ 7 / 60 ] loss: 0.328
[ 8 / 60 ] loss: 0.297
[ 9 / 60 ] loss: 0.470
[ 10 / 60 ] loss: 0.231
[ 11 / 60 ] loss: 0.193
[ 12 / 60 ] loss: 0.274
[ 13 / 60 ] loss: 0.285
[ 14 / 60 ] loss: 0.340
[ 15 / 60 ] loss: 0.256
[ 16 / 60 ] loss: 0.367
[ 17 / 60 ] loss: 0.309
[ 18 / 60 ] loss: 0.220
[ 19 / 60 ] loss: 0.662
[ 20 / 60 ] loss: 0.455
[ 21 / 60 ] loss: 0.399
[ 22 / 60 ] loss: 0.402
[ 23 / 60 ] loss: 0.481
[ 24 / 60 ] loss: 0.095
[ 25 / 60 ] loss: 0.661
[ 26 / 60 ] loss: 0.679
[ 27 / 60 ] loss: 0.398
[ 28 / 60 ] loss: 0.430
[ 29 / 60 ] loss: 0.283
[ 30 / 60 ] loss: 0.592
[ 31 / 60 ] loss: 0.298
[ 32 / 60 ] loss: 0.483
[ 33 / 60 ] loss: 0.253
[ 34 / 60 ] loss: 0.255
[ 35 / 60 ] loss: 0.625
[ 36 / 60 ] loss: 0.432
[ 37 / 60 ] loss: 0.343
[ 38 / 60 ] loss: 0.144
[ 39 / 60 ] loss: 0.473
[ 40 / 60 ] loss: 0.320
[ 41 / 60 ] loss: 0.513
[ 42 / 60 ] loss: 0.489
[ 43 / 60 ] loss: 0.539
[ 44 / 60 ] loss: 0.406
[ 45 / 60 ] loss: 0.330
[ 46 / 60 ] loss: 0.611
[ 47 / 60 ] loss: 0.304
[ 48 / 60 ] loss: 0.223
[ 49 / 60 ] loss: 0.441
[ 50 / 60 ] loss: 0.232
[ 51 / 60 ] loss: 0.186
[ 52 / 60 ] loss: 0.365
[ 53 / 60 ] loss: 0.353
[ 54 / 60 ] loss: 0.209
[ 55 / 60 ] loss: 0.237
[ 56 / 60 ] loss: 0.345
[ 57 / 60 ] loss: 0.491
[ 58 / 60 ] loss: 0.287
[ 59 / 60 ] loss: 0.522
[ 60 / 60 ] loss: 0.099
0.3657604123155276
Accuracy: 0.751055 -- Precision: 0.766917 -- Recall: 0.784615 -- F1: 0.775665 -- AUC: 0.798490
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[98, 36, 237, 96, 206, 4, 63, 40, 76, 1, 208, 205, 28, 85, 162, 87, 197, 55, 24, 189, 167, 13, 146, 2, 186, 223, 171, 44, 226, 174, 30, 229, 228, 139, 108, 140, 125, 34, 17, 11, 21, 212, 84, 216, 147, 79, 110, 48, 31, 219, 116, 199, 177, 214, 35, 182, 145, 137, 7, 58, 42, 5, 60, 188, 92, 119, 234, 49, 190, 183, 54, 77, 66, 166, 123, 43, 29, 200, 86, 6, 12, 160, 180, 20, 47, 81, 52, 131, 19, 150, 133, 83, 117, 222, 71, 94, 124, 128, 104, 221, 155, 103, 218, 122, 153, 156, 181, 232, 80, 67, 225, 114, 3, 64, 163, 10, 56, 185, 45, 25, 53, 130, 89, 65, 95, 154, 33, 136, 74, 170, 201, 207, 93, 144, 112, 27, 132, 179, 9, 15, 50, 70, 158, 61, 59, 57, 18, 233, 235, 231, 191, 198, 141, 102, 121, 101, 107, 194, 169, 120, 41, 152, 184, 211, 236, 106, 22, 118, 8, 16, 172, 168, 90, 69, 176, 38, 72, 164, 73, 134, 115, 224, 165, 37, 78, 68, 105, 138, 209, 46, 135, 97, 159, 39, 204, 210, 127, 129, 202, 149, 113, 32, 230, 192, 196, 173, 91, 213, 143, 99, 227, 75, 220, 51, 157, 26, 148, 126, 82, 178, 100, 62, 111, 151, 175, 187, 195, 161, 23, 142, 14, 217, 215, 193, 109, 203, 88]
[0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0]
[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 8 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.409
[ 2 / 60 ] loss: 0.486
[ 3 / 60 ] loss: 0.418
[ 4 / 60 ] loss: 0.434
[ 5 / 60 ] loss: 0.377
[ 6 / 60 ] loss: 0.162
[ 7 / 60 ] loss: 0.361
[ 8 / 60 ] loss: 0.134
[ 9 / 60 ] loss: 0.158
[ 10 / 60 ] loss: 0.322
[ 11 / 60 ] loss: 0.623
[ 12 / 60 ] loss: 0.344
[ 13 / 60 ] loss: 0.302
[ 14 / 60 ] loss: 0.199
[ 15 / 60 ] loss: 0.152
[ 16 / 60 ] loss: 0.188
[ 17 / 60 ] loss: 0.627
[ 18 / 60 ] loss: 0.413
[ 19 / 60 ] loss: 0.196
[ 20 / 60 ] loss: 0.176
[ 21 / 60 ] loss: 0.119
[ 22 / 60 ] loss: 0.421
[ 23 / 60 ] loss: 0.350
[ 24 / 60 ] loss: 0.443
[ 25 / 60 ] loss: 0.206
[ 26 / 60 ] loss: 0.258
[ 27 / 60 ] loss: 0.438
[ 28 / 60 ] loss: 0.415
[ 29 / 60 ] loss: 0.400
[ 30 / 60 ] loss: 0.142
[ 31 / 60 ] loss: 0.281
[ 32 / 60 ] loss: 0.118
[ 33 / 60 ] loss: 0.248
[ 34 / 60 ] loss: 0.409
[ 35 / 60 ] loss: 0.724
[ 36 / 60 ] loss: 0.243
[ 37 / 60 ] loss: 0.305
[ 38 / 60 ] loss: 0.372
[ 39 / 60 ] loss: 0.178
[ 40 / 60 ] loss: 0.209
[ 41 / 60 ] loss: 0.159
[ 42 / 60 ] loss: 0.236
[ 43 / 60 ] loss: 0.234
[ 44 / 60 ] loss: 0.381
[ 45 / 60 ] loss: 0.099
[ 46 / 60 ] loss: 0.549
[ 47 / 60 ] loss: 0.194
[ 48 / 60 ] loss: 0.157
[ 49 / 60 ] loss: 0.535
[ 50 / 60 ] loss: 0.491
[ 51 / 60 ] loss: 0.520
[ 52 / 60 ] loss: 0.270
[ 53 / 60 ] loss: 0.235
[ 54 / 60 ] loss: 0.145
[ 55 / 60 ] loss: 0.459
[ 56 / 60 ] loss: 0.388
[ 57 / 60 ] loss: 0.178
[ 58 / 60 ] loss: 0.440
[ 59 / 60 ] loss: 0.236
[ 60 / 60 ] loss: 0.086
0.3125135932117701
Accuracy: 0.759494 -- Precision: 0.758865 -- Recall: 0.823077 -- F1: 0.789668 -- AUC: 0.794033
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[9, 85, 41, 90, 87, 80, 158, 127, 54, 61, 117, 64, 157, 202, 55, 192, 134, 69, 155, 32, 135, 95, 107, 26, 219, 126, 18, 206, 119, 235, 43, 142, 83, 105, 38, 222, 207, 52, 152, 109, 184, 197, 144, 121, 2, 146, 68, 35, 216, 140, 94, 102, 106, 56, 1, 227, 91, 131, 217, 174, 154, 86, 156, 137, 51, 122, 232, 148, 77, 37, 93, 34, 116, 113, 115, 7, 237, 24, 210, 200, 71, 164, 101, 3, 172, 76, 223, 63, 141, 204, 112, 74, 149, 213, 138, 209, 88, 25, 221, 10, 5, 208, 14, 167, 231, 179, 46, 98, 145, 120, 165, 150, 45, 36, 60, 226, 171, 62, 49, 31, 214, 73, 151, 177, 42, 188, 191, 67, 6, 186, 128, 190, 168, 110, 100, 125, 12, 211, 114, 8, 50, 173, 57, 175, 17, 111, 161, 22, 4, 30, 189, 218, 75, 182, 70, 92, 81, 84, 201, 78, 79, 199, 225, 15, 236, 29, 160, 27, 185, 139, 130, 163, 23, 72, 89, 21, 65, 194, 136, 147, 59, 205, 198, 39, 180, 196, 103, 176, 99, 118, 104, 215, 44, 170, 233, 183, 193, 19, 16, 166, 58, 143, 40, 13, 195, 33, 133, 129, 159, 47, 162, 230, 153, 178, 187, 169, 28, 124, 181, 132, 11, 220, 20, 96, 229, 224, 66, 203, 212, 234, 123, 228, 82, 48, 97, 108, 53]
[1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0]
[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 9 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.364
[ 2 / 60 ] loss: 0.089
[ 3 / 60 ] loss: 0.366
[ 4 / 60 ] loss: 0.388
[ 5 / 60 ] loss: 0.098
[ 6 / 60 ] loss: 0.515
[ 7 / 60 ] loss: 0.125
[ 8 / 60 ] loss: 0.269
[ 9 / 60 ] loss: 0.406
[ 10 / 60 ] loss: 0.558
[ 11 / 60 ] loss: 0.239
[ 12 / 60 ] loss: 0.220
[ 13 / 60 ] loss: 0.279
[ 14 / 60 ] loss: 0.338
[ 15 / 60 ] loss: 0.295
[ 16 / 60 ] loss: 0.670
[ 17 / 60 ] loss: 0.087
[ 18 / 60 ] loss: 0.462
[ 19 / 60 ] loss: 0.250
[ 20 / 60 ] loss: 0.094
[ 21 / 60 ] loss: 0.184
[ 22 / 60 ] loss: 0.266
[ 23 / 60 ] loss: 0.101
[ 24 / 60 ] loss: 0.377
[ 25 / 60 ] loss: 0.451
[ 26 / 60 ] loss: 0.089
[ 27 / 60 ] loss: 0.245
[ 28 / 60 ] loss: 0.394
[ 29 / 60 ] loss: 0.339
[ 30 / 60 ] loss: 0.346
[ 31 / 60 ] loss: 0.120
[ 32 / 60 ] loss: 0.282
[ 33 / 60 ] loss: 0.407
[ 34 / 60 ] loss: 0.249
[ 35 / 60 ] loss: 0.119
[ 36 / 60 ] loss: 0.300
[ 37 / 60 ] loss: 0.276
[ 38 / 60 ] loss: 0.330
[ 39 / 60 ] loss: 0.333
[ 40 / 60 ] loss: 0.422
[ 41 / 60 ] loss: 0.078
[ 42 / 60 ] loss: 0.477
[ 43 / 60 ] loss: 0.376
[ 44 / 60 ] loss: 0.175
[ 45 / 60 ] loss: 0.383
[ 46 / 60 ] loss: 0.450
[ 47 / 60 ] loss: 0.361
[ 48 / 60 ] loss: 0.451
[ 49 / 60 ] loss: 0.197
[ 50 / 60 ] loss: 0.228
[ 51 / 60 ] loss: 0.203
[ 52 / 60 ] loss: 0.215
[ 53 / 60 ] loss: 0.354
[ 54 / 60 ] loss: 0.229
[ 55 / 60 ] loss: 0.312
[ 56 / 60 ] loss: 0.278
[ 57 / 60 ] loss: 0.358
[ 58 / 60 ] loss: 0.141
[ 59 / 60 ] loss: 0.329
[ 60 / 60 ] loss: 0.347
0.29473981087406476
Accuracy: 0.738397 -- Precision: 0.723684 -- Recall: 0.846154 -- F1: 0.780142 -- AUC: 0.811395
========= epoch: 10 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.100
[ 2 / 60 ] loss: 0.222
[ 3 / 60 ] loss: 0.271
[ 4 / 60 ] loss: 0.324
[ 5 / 60 ] loss: 0.223
[ 6 / 60 ] loss: 0.328
[ 7 / 60 ] loss: 0.418
[ 8 / 60 ] loss: 0.183
[ 9 / 60 ] loss: 0.219
[ 10 / 60 ] loss: 0.308
[ 11 / 60 ] loss: 0.340
[ 12 / 60 ] loss: 0.361
[ 13 / 60 ] loss: 0.506
[ 14 / 60 ] loss: 0.124
[ 15 / 60 ] loss: 0.211
[ 16 / 60 ] loss: 0.256
[ 17 / 60 ] loss: 0.228
[ 18 / 60 ] loss: 0.243
[ 19 / 60 ] loss: 0.199
[ 20 / 60 ] loss: 0.389
[ 21 / 60 ] loss: 0.172
[ 22 / 60 ] loss: 0.222
[ 23 / 60 ] loss: 0.097
[ 24 / 60 ] loss: 0.208
[ 25 / 60 ] loss: 0.193
[ 26 / 60 ] loss: 0.280
[ 27 / 60 ] loss: 0.298
[ 28 / 60 ] loss: 0.274
[ 29 / 60 ] loss: 0.272
[ 30 / 60 ] loss: 0.139
[ 31 / 60 ] loss: 0.263
[ 32 / 60 ] loss: 0.272
[ 33 / 60 ] loss: 0.249
[ 34 / 60 ] loss: 0.275
[ 35 / 60 ] loss: 0.590
[ 36 / 60 ] loss: 0.265
[ 37 / 60 ] loss: 0.118
[ 38 / 60 ] loss: 0.517
[ 39 / 60 ] loss: 0.272
[ 40 / 60 ] loss: 0.277
[ 41 / 60 ] loss: 0.399
[ 42 / 60 ] loss: 0.431
[ 43 / 60 ] loss: 0.192
[ 44 / 60 ] loss: 0.206
[ 45 / 60 ] loss: 0.575
[ 46 / 60 ] loss: 0.684
[ 47 / 60 ] loss: 0.210
[ 48 / 60 ] loss: 0.585
[ 49 / 60 ] loss: 0.572
[ 50 / 60 ] loss: 0.551
[ 51 / 60 ] loss: 0.251
[ 52 / 60 ] loss: 0.330
[ 53 / 60 ] loss: 0.302
[ 54 / 60 ] loss: 0.422
[ 55 / 60 ] loss: 0.877
[ 56 / 60 ] loss: 0.265
[ 57 / 60 ] loss: 0.356
[ 58 / 60 ] loss: 0.681
[ 59 / 60 ] loss: 0.470
[ 60 / 60 ] loss: 1.221
0.3381723099698623
Accuracy: 0.755274 -- Precision: 0.833333 -- Recall: 0.692308 -- F1: 0.756303 -- AUC: 0.796477
========= epoch: 11 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.151
[ 2 / 60 ] loss: 0.858
[ 3 / 60 ] loss: 0.294
[ 4 / 60 ] loss: 0.326
[ 5 / 60 ] loss: 0.383
[ 6 / 60 ] loss: 0.363
[ 7 / 60 ] loss: 0.252
[ 8 / 60 ] loss: 0.332
[ 9 / 60 ] loss: 0.301
[ 10 / 60 ] loss: 0.301
[ 11 / 60 ] loss: 0.481
[ 12 / 60 ] loss: 0.328
[ 13 / 60 ] loss: 0.263
[ 14 / 60 ] loss: 0.182
[ 15 / 60 ] loss: 0.418
[ 16 / 60 ] loss: 0.148
[ 17 / 60 ] loss: 0.274
[ 18 / 60 ] loss: 0.181
[ 19 / 60 ] loss: 0.372
[ 20 / 60 ] loss: 0.631
[ 21 / 60 ] loss: 0.271
[ 22 / 60 ] loss: 0.371
[ 23 / 60 ] loss: 0.520
[ 24 / 60 ] loss: 0.180
[ 25 / 60 ] loss: 0.516
[ 26 / 60 ] loss: 0.102
[ 27 / 60 ] loss: 0.250
[ 28 / 60 ] loss: 0.204
[ 29 / 60 ] loss: 0.404
[ 30 / 60 ] loss: 0.191
[ 31 / 60 ] loss: 0.486
[ 32 / 60 ] loss: 0.475
[ 33 / 60 ] loss: 0.244
[ 34 / 60 ] loss: 0.157
[ 35 / 60 ] loss: 0.451
[ 36 / 60 ] loss: 0.097
[ 37 / 60 ] loss: 0.445
[ 38 / 60 ] loss: 0.498
[ 39 / 60 ] loss: 0.286
[ 40 / 60 ] loss: 0.169
[ 41 / 60 ] loss: 0.623
[ 42 / 60 ] loss: 0.399
[ 43 / 60 ] loss: 0.244
[ 44 / 60 ] loss: 0.342
[ 45 / 60 ] loss: 0.127
[ 46 / 60 ] loss: 0.282
[ 47 / 60 ] loss: 0.175
[ 48 / 60 ] loss: 0.102
[ 49 / 60 ] loss: 0.141
[ 50 / 60 ] loss: 0.081
[ 51 / 60 ] loss: 0.439
[ 52 / 60 ] loss: 0.182
[ 53 / 60 ] loss: 0.303
[ 54 / 60 ] loss: 0.285
[ 55 / 60 ] loss: 0.765
[ 56 / 60 ] loss: 0.250
[ 57 / 60 ] loss: 0.712
[ 58 / 60 ] loss: 0.066
[ 59 / 60 ] loss: 0.207
[ 60 / 60 ] loss: 0.059
0.3156638084600369
Accuracy: 0.738397 -- Precision: 0.769841 -- Recall: 0.746154 -- F1: 0.757813 -- AUC: 0.787347
========= epoch: 12 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.437
[ 2 / 60 ] loss: 0.285
[ 3 / 60 ] loss: 0.239
[ 4 / 60 ] loss: 0.549
[ 5 / 60 ] loss: 0.157
[ 6 / 60 ] loss: 0.260
[ 7 / 60 ] loss: 0.239
[ 8 / 60 ] loss: 0.438
[ 9 / 60 ] loss: 0.170
[ 10 / 60 ] loss: 0.137
[ 11 / 60 ] loss: 0.180
[ 12 / 60 ] loss: 0.398
[ 13 / 60 ] loss: 0.156
[ 14 / 60 ] loss: 0.445
[ 15 / 60 ] loss: 0.283
[ 16 / 60 ] loss: 0.086
[ 17 / 60 ] loss: 0.406
[ 18 / 60 ] loss: 0.116
[ 19 / 60 ] loss: 0.064
[ 20 / 60 ] loss: 0.236
[ 21 / 60 ] loss: 0.420
[ 22 / 60 ] loss: 0.403
[ 23 / 60 ] loss: 0.228
[ 24 / 60 ] loss: 0.084
[ 25 / 60 ] loss: 0.345
[ 26 / 60 ] loss: 0.397
[ 27 / 60 ] loss: 0.242
[ 28 / 60 ] loss: 0.273
[ 29 / 60 ] loss: 0.140
[ 30 / 60 ] loss: 0.094
[ 31 / 60 ] loss: 0.259
[ 32 / 60 ] loss: 0.110
[ 33 / 60 ] loss: 0.352
[ 34 / 60 ] loss: 0.107
[ 35 / 60 ] loss: 0.708
[ 36 / 60 ] loss: 0.779
[ 37 / 60 ] loss: 0.230
[ 38 / 60 ] loss: 0.240
[ 39 / 60 ] loss: 0.095
[ 40 / 60 ] loss: 0.400
[ 41 / 60 ] loss: 0.094
[ 42 / 60 ] loss: 0.395
[ 43 / 60 ] loss: 0.131
[ 44 / 60 ] loss: 0.276
[ 45 / 60 ] loss: 0.088
[ 46 / 60 ] loss: 0.267
[ 47 / 60 ] loss: 0.314
[ 48 / 60 ] loss: 0.156
[ 49 / 60 ] loss: 0.392
[ 50 / 60 ] loss: 0.069
[ 51 / 60 ] loss: 0.114
[ 52 / 60 ] loss: 0.371
[ 53 / 60 ] loss: 0.084
[ 54 / 60 ] loss: 0.356
[ 55 / 60 ] loss: 0.182
[ 56 / 60 ] loss: 0.298
[ 57 / 60 ] loss: 0.304
[ 58 / 60 ] loss: 0.372
[ 59 / 60 ] loss: 0.271
[ 60 / 60 ] loss: 0.073
0.2632004981239637
Accuracy: 0.763713 -- Precision: 0.760563 -- Recall: 0.830769 -- F1: 0.794118 -- AUC: 0.801150
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[149, 218, 26, 203, 129, 75, 40, 177, 210, 54, 237, 178, 95, 225, 195, 3, 67, 171, 135, 116, 152, 8, 78, 111, 196, 20, 113, 155, 25, 39, 166, 106, 102, 109, 132, 80, 85, 174, 212, 34, 236, 194, 114, 10, 120, 208, 27, 94, 11, 158, 84, 134, 16, 201, 93, 119, 55, 193, 205, 23, 69, 14, 61, 57, 117, 124, 70, 68, 183, 9, 56, 12, 231, 163, 141, 103, 71, 74, 83, 200, 59, 101, 153, 170, 18, 148, 182, 136, 211, 223, 230, 186, 145, 185, 30, 48, 97, 43, 22, 104, 118, 100, 123, 214, 2, 73, 49, 63, 189, 50, 142, 112, 35, 37, 46, 151, 13, 105, 88, 5, 77, 90, 150, 15, 126, 167, 19, 133, 161, 108, 226, 175, 179, 41, 138, 157, 199, 192, 62, 130, 139, 60, 217, 154, 64, 38, 219, 98, 227, 127, 213, 92, 168, 191, 137, 44, 233, 31, 216, 234, 206, 79, 65, 176, 220, 89, 215, 204, 232, 131, 36, 181, 146, 24, 72, 45, 159, 224, 190, 229, 162, 122, 47, 115, 33, 32, 7, 173, 82, 235, 128, 96, 81, 180, 6, 21, 187, 221, 66, 198, 17, 160, 207, 164, 144, 28, 125, 76, 29, 202, 184, 99, 222, 51, 1, 58, 143, 121, 42, 197, 53, 209, 140, 228, 165, 147, 4, 110, 87, 169, 188, 91, 156, 52, 86, 172, 107]
[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]
[1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 13 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.167
[ 2 / 60 ] loss: 0.259
[ 3 / 60 ] loss: 0.271
[ 4 / 60 ] loss: 0.177
[ 5 / 60 ] loss: 0.410
[ 6 / 60 ] loss: 0.245
[ 7 / 60 ] loss: 0.161
[ 8 / 60 ] loss: 0.255
[ 9 / 60 ] loss: 0.160
[ 10 / 60 ] loss: 0.081
[ 11 / 60 ] loss: 0.090
[ 12 / 60 ] loss: 0.067
[ 13 / 60 ] loss: 0.333
[ 14 / 60 ] loss: 0.236
[ 15 / 60 ] loss: 0.132
[ 16 / 60 ] loss: 0.093
[ 17 / 60 ] loss: 0.287
[ 18 / 60 ] loss: 0.079
[ 19 / 60 ] loss: 0.063
[ 20 / 60 ] loss: 0.297
[ 21 / 60 ] loss: 0.237
[ 22 / 60 ] loss: 0.072
[ 23 / 60 ] loss: 0.095
[ 24 / 60 ] loss: 0.212
[ 25 / 60 ] loss: 0.864
[ 26 / 60 ] loss: 0.342
[ 27 / 60 ] loss: 0.409
[ 28 / 60 ] loss: 0.233
[ 29 / 60 ] loss: 0.089
[ 30 / 60 ] loss: 0.616
[ 31 / 60 ] loss: 0.180
[ 32 / 60 ] loss: 0.233
[ 33 / 60 ] loss: 0.322
[ 34 / 60 ] loss: 0.237
[ 35 / 60 ] loss: 0.537
[ 36 / 60 ] loss: 0.452
[ 37 / 60 ] loss: 0.080
[ 38 / 60 ] loss: 0.092
[ 39 / 60 ] loss: 0.289
[ 40 / 60 ] loss: 0.172
[ 41 / 60 ] loss: 0.054
[ 42 / 60 ] loss: 0.303
[ 43 / 60 ] loss: 0.450
[ 44 / 60 ] loss: 0.371
[ 45 / 60 ] loss: 0.279
[ 46 / 60 ] loss: 0.136
[ 47 / 60 ] loss: 0.344
[ 48 / 60 ] loss: 0.351
[ 49 / 60 ] loss: 0.297
[ 50 / 60 ] loss: 0.151
[ 51 / 60 ] loss: 0.210
[ 52 / 60 ] loss: 0.383
[ 53 / 60 ] loss: 0.453
[ 54 / 60 ] loss: 0.129
[ 55 / 60 ] loss: 0.334
[ 56 / 60 ] loss: 0.592
[ 57 / 60 ] loss: 0.068
[ 58 / 60 ] loss: 0.348
[ 59 / 60 ] loss: 0.246
[ 60 / 60 ] loss: 0.048
0.25288761767248313
Accuracy: 0.713080 -- Precision: 0.696203 -- Recall: 0.846154 -- F1: 0.763889 -- AUC: 0.779511
========= epoch: 14 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.396
[ 2 / 60 ] loss: 0.061
[ 3 / 60 ] loss: 0.146
[ 4 / 60 ] loss: 0.308
[ 5 / 60 ] loss: 0.486
[ 6 / 60 ] loss: 0.221
[ 7 / 60 ] loss: 0.048
[ 8 / 60 ] loss: 0.377
[ 9 / 60 ] loss: 0.102
[ 10 / 60 ] loss: 0.345
[ 11 / 60 ] loss: 0.245
[ 12 / 60 ] loss: 0.291
[ 13 / 60 ] loss: 0.109
[ 14 / 60 ] loss: 0.257
[ 15 / 60 ] loss: 0.107
[ 16 / 60 ] loss: 0.137
[ 17 / 60 ] loss: 0.111
[ 18 / 60 ] loss: 0.050
[ 19 / 60 ] loss: 0.278
[ 20 / 60 ] loss: 0.063
[ 21 / 60 ] loss: 0.415
[ 22 / 60 ] loss: 0.497
[ 23 / 60 ] loss: 0.384
[ 24 / 60 ] loss: 0.222
[ 25 / 60 ] loss: 0.310
[ 26 / 60 ] loss: 0.070
[ 27 / 60 ] loss: 0.492
[ 28 / 60 ] loss: 0.358
[ 29 / 60 ] loss: 0.226
[ 30 / 60 ] loss: 0.082
[ 31 / 60 ] loss: 0.218
[ 32 / 60 ] loss: 0.679
[ 33 / 60 ] loss: 0.439
[ 34 / 60 ] loss: 0.062
[ 35 / 60 ] loss: 0.172
[ 36 / 60 ] loss: 0.117
[ 37 / 60 ] loss: 0.500
[ 38 / 60 ] loss: 0.267
[ 39 / 60 ] loss: 0.272
[ 40 / 60 ] loss: 0.104
[ 41 / 60 ] loss: 0.099
[ 42 / 60 ] loss: 0.084
[ 43 / 60 ] loss: 0.458
[ 44 / 60 ] loss: 0.333
[ 45 / 60 ] loss: 0.074
[ 46 / 60 ] loss: 0.470
[ 47 / 60 ] loss: 0.051
[ 48 / 60 ] loss: 0.133
[ 49 / 60 ] loss: 0.277
[ 50 / 60 ] loss: 0.216
[ 51 / 60 ] loss: 0.171
[ 52 / 60 ] loss: 0.338
[ 53 / 60 ] loss: 0.047
[ 54 / 60 ] loss: 0.057
[ 55 / 60 ] loss: 0.054
[ 56 / 60 ] loss: 0.421
[ 57 / 60 ] loss: 0.211
[ 58 / 60 ] loss: 0.392
[ 59 / 60 ] loss: 0.324
[ 60 / 60 ] loss: 0.040
0.2378813674673438
Accuracy: 0.738397 -- Precision: 0.742857 -- Recall: 0.800000 -- F1: 0.770370 -- AUC: 0.775162
========= epoch: 15 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.221
[ 2 / 60 ] loss: 0.272
[ 3 / 60 ] loss: 0.393
[ 4 / 60 ] loss: 0.195
[ 5 / 60 ] loss: 0.157
[ 6 / 60 ] loss: 0.068
[ 7 / 60 ] loss: 0.060
[ 8 / 60 ] loss: 0.064
[ 9 / 60 ] loss: 0.108
[ 10 / 60 ] loss: 0.116
[ 11 / 60 ] loss: 0.311
[ 12 / 60 ] loss: 0.225
[ 13 / 60 ] loss: 0.097
[ 14 / 60 ] loss: 0.055
[ 15 / 60 ] loss: 0.043
[ 16 / 60 ] loss: 0.113
[ 17 / 60 ] loss: 0.420
[ 18 / 60 ] loss: 0.257
[ 19 / 60 ] loss: 0.218
[ 20 / 60 ] loss: 0.410
[ 21 / 60 ] loss: 0.099
[ 22 / 60 ] loss: 0.056
[ 23 / 60 ] loss: 0.088
[ 24 / 60 ] loss: 0.495
[ 25 / 60 ] loss: 0.649
[ 26 / 60 ] loss: 0.184
[ 27 / 60 ] loss: 0.376
[ 28 / 60 ] loss: 0.374
[ 29 / 60 ] loss: 0.133
[ 30 / 60 ] loss: 0.140
[ 31 / 60 ] loss: 0.141
[ 32 / 60 ] loss: 0.106
[ 33 / 60 ] loss: 0.093
[ 34 / 60 ] loss: 0.359
[ 35 / 60 ] loss: 0.274
[ 36 / 60 ] loss: 0.247
[ 37 / 60 ] loss: 0.102
[ 38 / 60 ] loss: 0.463
[ 39 / 60 ] loss: 0.325
[ 40 / 60 ] loss: 0.148
[ 41 / 60 ] loss: 0.380
[ 42 / 60 ] loss: 0.292
[ 43 / 60 ] loss: 0.195
[ 44 / 60 ] loss: 0.281
[ 45 / 60 ] loss: 0.120
[ 46 / 60 ] loss: 0.417
[ 47 / 60 ] loss: 0.268
[ 48 / 60 ] loss: 0.236
[ 49 / 60 ] loss: 0.135
[ 50 / 60 ] loss: 0.058
[ 51 / 60 ] loss: 0.225
[ 52 / 60 ] loss: 0.214
[ 53 / 60 ] loss: 0.313
[ 54 / 60 ] loss: 0.394
[ 55 / 60 ] loss: 0.533
[ 56 / 60 ] loss: 0.112
[ 57 / 60 ] loss: 0.229
[ 58 / 60 ] loss: 0.507
[ 59 / 60 ] loss: 0.164
[ 60 / 60 ] loss: 0.056
0.22978201148410637
Accuracy: 0.734177 -- Precision: 0.819048 -- Recall: 0.661538 -- F1: 0.731915 -- AUC: 0.805464
========= epoch: 16 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.082
[ 2 / 60 ] loss: 0.160
[ 3 / 60 ] loss: 0.418
[ 4 / 60 ] loss: 0.512
[ 5 / 60 ] loss: 0.621
[ 6 / 60 ] loss: 0.139
[ 7 / 60 ] loss: 0.082
[ 8 / 60 ] loss: 0.073
[ 9 / 60 ] loss: 0.078
[ 10 / 60 ] loss: 0.046
[ 11 / 60 ] loss: 0.193
[ 12 / 60 ] loss: 0.457
[ 13 / 60 ] loss: 0.283
[ 14 / 60 ] loss: 0.103
[ 15 / 60 ] loss: 0.082
[ 16 / 60 ] loss: 0.256
[ 17 / 60 ] loss: 0.220
[ 18 / 60 ] loss: 0.048
[ 19 / 60 ] loss: 0.154
[ 20 / 60 ] loss: 0.124
[ 21 / 60 ] loss: 0.109
[ 22 / 60 ] loss: 0.182
[ 23 / 60 ] loss: 0.104
[ 24 / 60 ] loss: 0.092
[ 25 / 60 ] loss: 0.492
[ 26 / 60 ] loss: 0.409
[ 27 / 60 ] loss: 0.041
[ 28 / 60 ] loss: 0.367
[ 29 / 60 ] loss: 0.210
[ 30 / 60 ] loss: 0.512
[ 31 / 60 ] loss: 0.198
[ 32 / 60 ] loss: 0.310
[ 33 / 60 ] loss: 0.337
[ 34 / 60 ] loss: 0.041
[ 35 / 60 ] loss: 0.063
[ 36 / 60 ] loss: 0.098
[ 37 / 60 ] loss: 0.410
[ 38 / 60 ] loss: 0.047
[ 39 / 60 ] loss: 0.224
[ 40 / 60 ] loss: 0.047
[ 41 / 60 ] loss: 0.239
[ 42 / 60 ] loss: 0.556
[ 43 / 60 ] loss: 0.292
[ 44 / 60 ] loss: 0.318
[ 45 / 60 ] loss: 0.192
[ 46 / 60 ] loss: 0.118
[ 47 / 60 ] loss: 0.045
[ 48 / 60 ] loss: 0.086
[ 49 / 60 ] loss: 0.353
[ 50 / 60 ] loss: 0.332
[ 51 / 60 ] loss: 0.163
[ 52 / 60 ] loss: 0.098
[ 53 / 60 ] loss: 0.367
[ 54 / 60 ] loss: 0.515
[ 55 / 60 ] loss: 0.321
[ 56 / 60 ] loss: 0.162
[ 57 / 60 ] loss: 0.190
[ 58 / 60 ] loss: 0.563
[ 59 / 60 ] loss: 0.299
[ 60 / 60 ] loss: 1.871
0.2584019358580311
Accuracy: 0.725738 -- Precision: 0.744361 -- Recall: 0.761538 -- F1: 0.752852 -- AUC: 0.774982
========= epoch: 17 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.233
[ 2 / 60 ] loss: 0.327
[ 3 / 60 ] loss: 0.070
[ 4 / 60 ] loss: 0.056
[ 5 / 60 ] loss: 0.090
[ 6 / 60 ] loss: 0.357
[ 7 / 60 ] loss: 0.095
[ 8 / 60 ] loss: 0.687
[ 9 / 60 ] loss: 0.244
[ 10 / 60 ] loss: 0.133
[ 11 / 60 ] loss: 0.424
[ 12 / 60 ] loss: 0.488
[ 13 / 60 ] loss: 0.884
[ 14 / 60 ] loss: 0.132
[ 15 / 60 ] loss: 0.766
[ 16 / 60 ] loss: 0.073
[ 17 / 60 ] loss: 0.278
[ 18 / 60 ] loss: 0.123
[ 19 / 60 ] loss: 0.286
[ 20 / 60 ] loss: 0.500
[ 21 / 60 ] loss: 0.438
[ 22 / 60 ] loss: 0.157
[ 23 / 60 ] loss: 0.278
[ 24 / 60 ] loss: 0.090
[ 25 / 60 ] loss: 0.263
[ 26 / 60 ] loss: 0.075
[ 27 / 60 ] loss: 0.319
[ 28 / 60 ] loss: 0.291
[ 29 / 60 ] loss: 0.364
[ 30 / 60 ] loss: 0.407
[ 31 / 60 ] loss: 0.075
[ 32 / 60 ] loss: 0.374
[ 33 / 60 ] loss: 0.505
[ 34 / 60 ] loss: 0.464
[ 35 / 60 ] loss: 0.227
[ 36 / 60 ] loss: 0.245
[ 37 / 60 ] loss: 0.351
[ 38 / 60 ] loss: 0.295
[ 39 / 60 ] loss: 0.232
[ 40 / 60 ] loss: 0.087
[ 41 / 60 ] loss: 0.083
[ 42 / 60 ] loss: 0.114
[ 43 / 60 ] loss: 0.184
[ 44 / 60 ] loss: 0.170
[ 45 / 60 ] loss: 0.181
[ 46 / 60 ] loss: 0.101
[ 47 / 60 ] loss: 0.262
[ 48 / 60 ] loss: 0.341
[ 49 / 60 ] loss: 0.240
[ 50 / 60 ] loss: 0.200
[ 51 / 60 ] loss: 0.159
[ 52 / 60 ] loss: 0.120
[ 53 / 60 ] loss: 0.932
[ 54 / 60 ] loss: 0.084
[ 55 / 60 ] loss: 0.257
[ 56 / 60 ] loss: 0.293
[ 57 / 60 ] loss: 0.198
[ 58 / 60 ] loss: 0.163
[ 59 / 60 ] loss: 0.299
[ 60 / 60 ] loss: 0.036
0.2699592117841045
Accuracy: 0.734177 -- Precision: 0.807339 -- Recall: 0.676923 -- F1: 0.736402 -- AUC: 0.767649
========= epoch: 18 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.666
[ 2 / 60 ] loss: 0.058
[ 3 / 60 ] loss: 0.281
[ 4 / 60 ] loss: 0.516
[ 5 / 60 ] loss: 0.292
[ 6 / 60 ] loss: 0.125
[ 7 / 60 ] loss: 0.222
[ 8 / 60 ] loss: 0.066
[ 9 / 60 ] loss: 0.063
[ 10 / 60 ] loss: 0.256
[ 11 / 60 ] loss: 0.165
[ 12 / 60 ] loss: 0.537
[ 13 / 60 ] loss: 0.189
[ 14 / 60 ] loss: 0.318
[ 15 / 60 ] loss: 0.065
[ 16 / 60 ] loss: 0.051
[ 17 / 60 ] loss: 0.312
[ 18 / 60 ] loss: 0.265
[ 19 / 60 ] loss: 0.043
[ 20 / 60 ] loss: 0.090
[ 21 / 60 ] loss: 0.093
[ 22 / 60 ] loss: 0.053
[ 23 / 60 ] loss: 0.246
[ 24 / 60 ] loss: 0.137
[ 25 / 60 ] loss: 0.245
[ 26 / 60 ] loss: 0.232
[ 27 / 60 ] loss: 0.067
[ 28 / 60 ] loss: 0.287
[ 29 / 60 ] loss: 0.741
[ 30 / 60 ] loss: 0.225
[ 31 / 60 ] loss: 0.207
[ 32 / 60 ] loss: 0.310
[ 33 / 60 ] loss: 0.095
[ 34 / 60 ] loss: 0.325
[ 35 / 60 ] loss: 0.253
[ 36 / 60 ] loss: 0.343
[ 37 / 60 ] loss: 0.091
[ 38 / 60 ] loss: 0.053
[ 39 / 60 ] loss: 0.464
[ 40 / 60 ] loss: 0.061
[ 41 / 60 ] loss: 0.145
[ 42 / 60 ] loss: 0.068
[ 43 / 60 ] loss: 0.298
[ 44 / 60 ] loss: 0.280
[ 45 / 60 ] loss: 0.436
[ 46 / 60 ] loss: 0.143
[ 47 / 60 ] loss: 0.244
[ 48 / 60 ] loss: 0.344
[ 49 / 60 ] loss: 0.285
[ 50 / 60 ] loss: 0.296
[ 51 / 60 ] loss: 0.234
[ 52 / 60 ] loss: 0.055
[ 53 / 60 ] loss: 0.219
[ 54 / 60 ] loss: 0.121
[ 55 / 60 ] loss: 0.065
[ 56 / 60 ] loss: 0.224
[ 57 / 60 ] loss: 0.236
[ 58 / 60 ] loss: 0.061
[ 59 / 60 ] loss: 0.385
[ 60 / 60 ] loss: 0.175
0.22373597268015147
Accuracy: 0.742616 -- Precision: 0.759398 -- Recall: 0.776923 -- F1: 0.768061 -- AUC: 0.803055
========= epoch: 19 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.115
[ 2 / 60 ] loss: 0.048
[ 3 / 60 ] loss: 0.048
[ 4 / 60 ] loss: 0.230
[ 5 / 60 ] loss: 0.230
[ 6 / 60 ] loss: 0.331
[ 7 / 60 ] loss: 0.094
[ 8 / 60 ] loss: 0.257
[ 9 / 60 ] loss: 0.292
[ 10 / 60 ] loss: 0.139
[ 11 / 60 ] loss: 0.342
[ 12 / 60 ] loss: 0.279
[ 13 / 60 ] loss: 0.068
[ 14 / 60 ] loss: 0.188
[ 15 / 60 ] loss: 0.095
[ 16 / 60 ] loss: 0.131
[ 17 / 60 ] loss: 0.239
[ 18 / 60 ] loss: 0.284
[ 19 / 60 ] loss: 0.041
[ 20 / 60 ] loss: 0.282
[ 21 / 60 ] loss: 0.092
[ 22 / 60 ] loss: 0.278
[ 23 / 60 ] loss: 0.077
[ 24 / 60 ] loss: 0.190
[ 25 / 60 ] loss: 0.265
[ 26 / 60 ] loss: 0.236
[ 27 / 60 ] loss: 0.299
[ 28 / 60 ] loss: 0.263
[ 29 / 60 ] loss: 0.068
[ 30 / 60 ] loss: 0.057
[ 31 / 60 ] loss: 0.209
[ 32 / 60 ] loss: 0.061
[ 33 / 60 ] loss: 0.299
[ 34 / 60 ] loss: 0.156
[ 35 / 60 ] loss: 0.110
[ 36 / 60 ] loss: 0.454
[ 37 / 60 ] loss: 0.403
[ 38 / 60 ] loss: 0.489
[ 39 / 60 ] loss: 0.048
[ 40 / 60 ] loss: 0.047
[ 41 / 60 ] loss: 0.396
[ 42 / 60 ] loss: 0.229
[ 43 / 60 ] loss: 0.219
[ 44 / 60 ] loss: 0.042
[ 45 / 60 ] loss: 0.471
[ 46 / 60 ] loss: 0.247
[ 47 / 60 ] loss: 0.117
[ 48 / 60 ] loss: 0.133
[ 49 / 60 ] loss: 0.096
[ 50 / 60 ] loss: 0.475
[ 51 / 60 ] loss: 0.093
[ 52 / 60 ] loss: 0.377
[ 53 / 60 ] loss: 0.096
[ 54 / 60 ] loss: 0.524
[ 55 / 60 ] loss: 0.337
[ 56 / 60 ] loss: 0.184
[ 57 / 60 ] loss: 0.104
[ 58 / 60 ] loss: 0.048
[ 59 / 60 ] loss: 0.128
[ 60 / 60 ] loss: 0.227
0.20631983031829199
Accuracy: 0.759494 -- Precision: 0.782946 -- Recall: 0.776923 -- F1: 0.779923 -- AUC: 0.776636
========= epoch: 20 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.056
[ 2 / 60 ] loss: 0.126
[ 3 / 60 ] loss: 0.199
[ 4 / 60 ] loss: 0.072
[ 5 / 60 ] loss: 0.081
[ 6 / 60 ] loss: 0.272
[ 7 / 60 ] loss: 0.042
[ 8 / 60 ] loss: 0.069
[ 9 / 60 ] loss: 0.269
[ 10 / 60 ] loss: 0.071
[ 11 / 60 ] loss: 0.469
[ 12 / 60 ] loss: 0.268
[ 13 / 60 ] loss: 0.035
[ 14 / 60 ] loss: 0.358
[ 15 / 60 ] loss: 0.239
[ 16 / 60 ] loss: 0.394
[ 17 / 60 ] loss: 0.219
[ 18 / 60 ] loss: 0.035
[ 19 / 60 ] loss: 0.038
[ 20 / 60 ] loss: 0.044
[ 21 / 60 ] loss: 0.047
[ 22 / 60 ] loss: 0.098
[ 23 / 60 ] loss: 0.099
[ 24 / 60 ] loss: 0.898
[ 25 / 60 ] loss: 0.211
[ 26 / 60 ] loss: 0.033
[ 27 / 60 ] loss: 0.060
[ 28 / 60 ] loss: 0.066
[ 29 / 60 ] loss: 0.655
[ 30 / 60 ] loss: 0.077
[ 31 / 60 ] loss: 0.056
[ 32 / 60 ] loss: 0.038
[ 33 / 60 ] loss: 0.048
[ 34 / 60 ] loss: 0.237
[ 35 / 60 ] loss: 0.044
[ 36 / 60 ] loss: 0.080
[ 37 / 60 ] loss: 0.051
[ 38 / 60 ] loss: 0.053
[ 39 / 60 ] loss: 0.177
[ 40 / 60 ] loss: 0.036
[ 41 / 60 ] loss: 0.041
[ 42 / 60 ] loss: 0.267
[ 43 / 60 ] loss: 0.064
[ 44 / 60 ] loss: 0.063
[ 45 / 60 ] loss: 0.042
[ 46 / 60 ] loss: 0.245
[ 47 / 60 ] loss: 0.255
[ 48 / 60 ] loss: 0.057
[ 49 / 60 ] loss: 0.702
[ 50 / 60 ] loss: 0.316
[ 51 / 60 ] loss: 0.320
[ 52 / 60 ] loss: 0.030
[ 53 / 60 ] loss: 0.045
[ 54 / 60 ] loss: 0.028
[ 55 / 60 ] loss: 0.076
[ 56 / 60 ] loss: 0.296
[ 57 / 60 ] loss: 0.340
[ 58 / 60 ] loss: 0.267
[ 59 / 60 ] loss: 0.247
[ 60 / 60 ] loss: 0.931
0.18418546387304863
Accuracy: 0.746835 -- Precision: 0.753623 -- Recall: 0.800000 -- F1: 0.776119 -- AUC: 0.795866
========= epoch: 21 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.260
[ 2 / 60 ] loss: 0.050
[ 3 / 60 ] loss: 0.334
[ 4 / 60 ] loss: 0.222
[ 5 / 60 ] loss: 0.326
[ 6 / 60 ] loss: 0.413
[ 7 / 60 ] loss: 0.045
[ 8 / 60 ] loss: 0.141
[ 9 / 60 ] loss: 0.055
[ 10 / 60 ] loss: 0.576
[ 11 / 60 ] loss: 0.249
[ 12 / 60 ] loss: 0.082
[ 13 / 60 ] loss: 0.141
[ 14 / 60 ] loss: 0.111
[ 15 / 60 ] loss: 0.466
[ 16 / 60 ] loss: 0.093
[ 17 / 60 ] loss: 0.091
[ 18 / 60 ] loss: 0.288
[ 19 / 60 ] loss: 0.257
[ 20 / 60 ] loss: 0.115
[ 21 / 60 ] loss: 0.239
[ 22 / 60 ] loss: 0.035
[ 23 / 60 ] loss: 0.279
[ 24 / 60 ] loss: 0.340
[ 25 / 60 ] loss: 0.216
[ 26 / 60 ] loss: 0.235
[ 27 / 60 ] loss: 0.302
[ 28 / 60 ] loss: 0.254
[ 29 / 60 ] loss: 0.525
[ 30 / 60 ] loss: 0.162
[ 31 / 60 ] loss: 0.100
[ 32 / 60 ] loss: 0.114
[ 33 / 60 ] loss: 0.217
[ 34 / 60 ] loss: 0.236
[ 35 / 60 ] loss: 0.244
[ 36 / 60 ] loss: 0.293
[ 37 / 60 ] loss: 0.133
[ 38 / 60 ] loss: 0.253
[ 39 / 60 ] loss: 0.093
[ 40 / 60 ] loss: 0.078
[ 41 / 60 ] loss: 0.058
[ 42 / 60 ] loss: 0.224
[ 43 / 60 ] loss: 0.153
[ 44 / 60 ] loss: 0.405
[ 45 / 60 ] loss: 0.043
[ 46 / 60 ] loss: 0.479
[ 47 / 60 ] loss: 0.269
[ 48 / 60 ] loss: 0.051
[ 49 / 60 ] loss: 0.136
[ 50 / 60 ] loss: 0.157
[ 51 / 60 ] loss: 0.028
[ 52 / 60 ] loss: 0.049
[ 53 / 60 ] loss: 0.095
[ 54 / 60 ] loss: 0.153
[ 55 / 60 ] loss: 0.292
[ 56 / 60 ] loss: 0.291
[ 57 / 60 ] loss: 0.279
[ 58 / 60 ] loss: 0.287
[ 59 / 60 ] loss: 0.070
[ 60 / 60 ] loss: 0.041
0.20370493847876786
Accuracy: 0.772152 -- Precision: 0.775362 -- Recall: 0.823077 -- F1: 0.798507 -- AUC: 0.783106
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[124, 129, 169, 234, 122, 99, 56, 45, 90, 174, 175, 215, 208, 64, 185, 9, 154, 65, 160, 232, 230, 106, 186, 168, 80, 202, 182, 7, 74, 120, 10, 88, 12, 39, 237, 121, 177, 11, 48, 123, 50, 4, 140, 167, 142, 146, 94, 3, 176, 107, 218, 141, 137, 212, 164, 92, 166, 36, 198, 42, 139, 53, 73, 104, 75, 178, 220, 159, 180, 55, 132, 156, 5, 213, 158, 14, 98, 28, 192, 201, 194, 222, 46, 125, 19, 81, 38, 149, 109, 131, 21, 15, 181, 170, 209, 30, 152, 115, 33, 171, 191, 1, 102, 51, 163, 93, 136, 24, 211, 127, 207, 225, 187, 217, 118, 113, 89, 229, 62, 25, 78, 206, 195, 214, 226, 153, 71, 84, 31, 68, 67, 223, 96, 157, 22, 86, 119, 161, 172, 219, 63, 37, 66, 204, 126, 77, 183, 17, 6, 103, 151, 216, 101, 155, 41, 35, 85, 133, 111, 117, 138, 61, 47, 58, 145, 134, 2, 189, 105, 100, 29, 57, 97, 228, 128, 184, 173, 150, 79, 16, 110, 20, 116, 91, 200, 43, 44, 59, 130, 162, 221, 13, 82, 135, 95, 143, 72, 87, 32, 83, 233, 76, 69, 108, 224, 236, 203, 165, 49, 210, 54, 112, 231, 34, 40, 26, 179, 18, 188, 114, 196, 8, 190, 52, 205, 235, 144, 60, 197, 23, 199, 27, 227, 193, 147, 148, 70]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 22 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.083
[ 2 / 60 ] loss: 0.052
[ 3 / 60 ] loss: 0.257
[ 4 / 60 ] loss: 0.432
[ 5 / 60 ] loss: 0.310
[ 6 / 60 ] loss: 0.062
[ 7 / 60 ] loss: 0.245
[ 8 / 60 ] loss: 0.073
[ 9 / 60 ] loss: 0.105
[ 10 / 60 ] loss: 0.103
[ 11 / 60 ] loss: 0.085
[ 12 / 60 ] loss: 0.099
[ 13 / 60 ] loss: 0.264
[ 14 / 60 ] loss: 0.227
[ 15 / 60 ] loss: 0.286
[ 16 / 60 ] loss: 0.204
[ 17 / 60 ] loss: 0.581
[ 18 / 60 ] loss: 0.052
[ 19 / 60 ] loss: 0.151
[ 20 / 60 ] loss: 0.047
[ 21 / 60 ] loss: 0.085
[ 22 / 60 ] loss: 0.412
[ 23 / 60 ] loss: 0.075
[ 24 / 60 ] loss: 0.088
[ 25 / 60 ] loss: 0.353
[ 26 / 60 ] loss: 0.269
[ 27 / 60 ] loss: 0.203
[ 28 / 60 ] loss: 0.123
[ 29 / 60 ] loss: 0.419
[ 30 / 60 ] loss: 0.102
[ 31 / 60 ] loss: 0.277
[ 32 / 60 ] loss: 0.046
[ 33 / 60 ] loss: 0.082
[ 34 / 60 ] loss: 0.045
[ 35 / 60 ] loss: 0.228
[ 36 / 60 ] loss: 0.081
[ 37 / 60 ] loss: 0.095
[ 38 / 60 ] loss: 0.079
[ 39 / 60 ] loss: 0.044
[ 40 / 60 ] loss: 0.449
[ 41 / 60 ] loss: 0.032
[ 42 / 60 ] loss: 0.337
[ 43 / 60 ] loss: 0.029
[ 44 / 60 ] loss: 0.036
[ 45 / 60 ] loss: 0.037
[ 46 / 60 ] loss: 0.069
[ 47 / 60 ] loss: 0.033
[ 48 / 60 ] loss: 0.232
[ 49 / 60 ] loss: 0.240
[ 50 / 60 ] loss: 0.295
[ 51 / 60 ] loss: 0.221
[ 52 / 60 ] loss: 0.043
[ 53 / 60 ] loss: 0.106
[ 54 / 60 ] loss: 0.031
[ 55 / 60 ] loss: 0.338
[ 56 / 60 ] loss: 0.236
[ 57 / 60 ] loss: 0.452
[ 58 / 60 ] loss: 0.389
[ 59 / 60 ] loss: 0.407
[ 60 / 60 ] loss: 0.030
0.1811014549806714
Accuracy: 0.751055 -- Precision: 0.748252 -- Recall: 0.823077 -- F1: 0.783883 -- AUC: 0.800935
========= epoch: 23 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.056
[ 2 / 60 ] loss: 0.327
[ 3 / 60 ] loss: 0.041
[ 4 / 60 ] loss: 0.249
[ 5 / 60 ] loss: 0.036
[ 6 / 60 ] loss: 0.282
[ 7 / 60 ] loss: 0.127
[ 8 / 60 ] loss: 0.029
[ 9 / 60 ] loss: 0.086
[ 10 / 60 ] loss: 0.050
[ 11 / 60 ] loss: 0.231
[ 12 / 60 ] loss: 0.040
[ 13 / 60 ] loss: 0.420
[ 14 / 60 ] loss: 0.540
[ 15 / 60 ] loss: 0.229
[ 16 / 60 ] loss: 0.228
[ 17 / 60 ] loss: 0.041
[ 18 / 60 ] loss: 0.212
[ 19 / 60 ] loss: 0.057
[ 20 / 60 ] loss: 0.038
[ 21 / 60 ] loss: 0.222
[ 22 / 60 ] loss: 0.072
[ 23 / 60 ] loss: 0.034
[ 24 / 60 ] loss: 0.243
[ 25 / 60 ] loss: 0.047
[ 26 / 60 ] loss: 0.323
[ 27 / 60 ] loss: 0.287
[ 28 / 60 ] loss: 0.070
[ 29 / 60 ] loss: 0.135
[ 30 / 60 ] loss: 0.108
[ 31 / 60 ] loss: 0.590
[ 32 / 60 ] loss: 0.051
[ 33 / 60 ] loss: 0.122
[ 34 / 60 ] loss: 0.110
[ 35 / 60 ] loss: 0.101
[ 36 / 60 ] loss: 0.081
[ 37 / 60 ] loss: 0.114
[ 38 / 60 ] loss: 0.045
[ 39 / 60 ] loss: 0.056
[ 40 / 60 ] loss: 0.224
[ 41 / 60 ] loss: 0.043
[ 42 / 60 ] loss: 0.045
[ 43 / 60 ] loss: 0.176
[ 44 / 60 ] loss: 0.281
[ 45 / 60 ] loss: 0.054
[ 46 / 60 ] loss: 0.371
[ 47 / 60 ] loss: 0.494
[ 48 / 60 ] loss: 0.075
[ 49 / 60 ] loss: 0.047
[ 50 / 60 ] loss: 0.307
[ 51 / 60 ] loss: 0.210
[ 52 / 60 ] loss: 0.131
[ 53 / 60 ] loss: 0.223
[ 54 / 60 ] loss: 0.029
[ 55 / 60 ] loss: 0.332
[ 56 / 60 ] loss: 0.085
[ 57 / 60 ] loss: 0.257
[ 58 / 60 ] loss: 0.058
[ 59 / 60 ] loss: 0.033
[ 60 / 60 ] loss: 0.030
0.1605844981968403
Accuracy: 0.742616 -- Precision: 0.759398 -- Recall: 0.776923 -- F1: 0.768061 -- AUC: 0.783501
========= epoch: 24 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.418
[ 2 / 60 ] loss: 0.031
[ 3 / 60 ] loss: 0.499
[ 4 / 60 ] loss: 0.046
[ 5 / 60 ] loss: 0.236
[ 6 / 60 ] loss: 0.026
[ 7 / 60 ] loss: 0.082
[ 8 / 60 ] loss: 0.264
[ 9 / 60 ] loss: 0.227
[ 10 / 60 ] loss: 0.130
[ 11 / 60 ] loss: 0.027
[ 12 / 60 ] loss: 0.329
[ 13 / 60 ] loss: 0.037
[ 14 / 60 ] loss: 0.051
[ 15 / 60 ] loss: 0.280
[ 16 / 60 ] loss: 0.037
[ 17 / 60 ] loss: 0.049
[ 18 / 60 ] loss: 0.094
[ 19 / 60 ] loss: 0.235
[ 20 / 60 ] loss: 0.441
[ 21 / 60 ] loss: 0.322
[ 22 / 60 ] loss: 0.075
[ 23 / 60 ] loss: 0.036
[ 24 / 60 ] loss: 0.063
[ 25 / 60 ] loss: 0.220
[ 26 / 60 ] loss: 0.035
[ 27 / 60 ] loss: 0.033
[ 28 / 60 ] loss: 0.056
[ 29 / 60 ] loss: 0.285
[ 30 / 60 ] loss: 0.070
[ 31 / 60 ] loss: 0.147
[ 32 / 60 ] loss: 0.095
[ 33 / 60 ] loss: 0.048
[ 34 / 60 ] loss: 0.040
[ 35 / 60 ] loss: 0.041
[ 36 / 60 ] loss: 0.040
[ 37 / 60 ] loss: 0.044
[ 38 / 60 ] loss: 0.184
[ 39 / 60 ] loss: 0.095
[ 40 / 60 ] loss: 0.206
[ 41 / 60 ] loss: 0.449
[ 42 / 60 ] loss: 0.041
[ 43 / 60 ] loss: 0.194
[ 44 / 60 ] loss: 0.247
[ 45 / 60 ] loss: 0.303
[ 46 / 60 ] loss: 0.150
[ 47 / 60 ] loss: 0.451
[ 48 / 60 ] loss: 0.061
[ 49 / 60 ] loss: 0.147
[ 50 / 60 ] loss: 0.059
[ 51 / 60 ] loss: 0.403
[ 52 / 60 ] loss: 0.047
[ 53 / 60 ] loss: 0.156
[ 54 / 60 ] loss: 0.512
[ 55 / 60 ] loss: 0.045
[ 56 / 60 ] loss: 0.029
[ 57 / 60 ] loss: 0.022
[ 58 / 60 ] loss: 0.269
[ 59 / 60 ] loss: 0.038
[ 60 / 60 ] loss: 0.042
0.1556882072550555
Accuracy: 0.729958 -- Precision: 0.739130 -- Recall: 0.784615 -- F1: 0.761194 -- AUC: 0.789360
========= epoch: 25 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.062
[ 2 / 60 ] loss: 0.038
[ 3 / 60 ] loss: 0.079
[ 4 / 60 ] loss: 0.436
[ 5 / 60 ] loss: 0.027
[ 6 / 60 ] loss: 0.029
[ 7 / 60 ] loss: 0.256
[ 8 / 60 ] loss: 0.254
[ 9 / 60 ] loss: 0.202
[ 10 / 60 ] loss: 0.065
[ 11 / 60 ] loss: 0.195
[ 12 / 60 ] loss: 0.175
[ 13 / 60 ] loss: 0.077
[ 14 / 60 ] loss: 0.034
[ 15 / 60 ] loss: 0.050
[ 16 / 60 ] loss: 0.328
[ 17 / 60 ] loss: 0.329
[ 18 / 60 ] loss: 0.283
[ 19 / 60 ] loss: 0.029
[ 20 / 60 ] loss: 0.067
[ 21 / 60 ] loss: 0.255
[ 22 / 60 ] loss: 0.089
[ 23 / 60 ] loss: 0.245
[ 24 / 60 ] loss: 0.080
[ 25 / 60 ] loss: 0.037
[ 26 / 60 ] loss: 0.030
[ 27 / 60 ] loss: 0.029
[ 28 / 60 ] loss: 0.107
[ 29 / 60 ] loss: 0.022
[ 30 / 60 ] loss: 0.079
[ 31 / 60 ] loss: 0.050
[ 32 / 60 ] loss: 0.044
[ 33 / 60 ] loss: 0.269
[ 34 / 60 ] loss: 0.274
[ 35 / 60 ] loss: 0.044
[ 36 / 60 ] loss: 0.219
[ 37 / 60 ] loss: 0.072
[ 38 / 60 ] loss: 0.414
[ 39 / 60 ] loss: 0.094
[ 40 / 60 ] loss: 0.106
[ 41 / 60 ] loss: 0.236
[ 42 / 60 ] loss: 0.082
[ 43 / 60 ] loss: 0.048
[ 44 / 60 ] loss: 0.060
[ 45 / 60 ] loss: 0.077
[ 46 / 60 ] loss: 0.312
[ 47 / 60 ] loss: 0.459
[ 48 / 60 ] loss: 0.248
[ 49 / 60 ] loss: 0.230
[ 50 / 60 ] loss: 0.052
[ 51 / 60 ] loss: 0.123
[ 52 / 60 ] loss: 0.371
[ 53 / 60 ] loss: 0.331
[ 54 / 60 ] loss: 0.053
[ 55 / 60 ] loss: 0.032
[ 56 / 60 ] loss: 0.231
[ 57 / 60 ] loss: 0.175
[ 58 / 60 ] loss: 0.080
[ 59 / 60 ] loss: 0.130
[ 60 / 60 ] loss: 0.023
0.14878182547787824
Accuracy: 0.734177 -- Precision: 0.763780 -- Recall: 0.746154 -- F1: 0.754864 -- AUC: 0.787922
========= epoch: 26 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.038
[ 2 / 60 ] loss: 0.490
[ 3 / 60 ] loss: 0.248
[ 4 / 60 ] loss: 0.816
[ 5 / 60 ] loss: 0.230
[ 6 / 60 ] loss: 0.033
[ 7 / 60 ] loss: 0.086
[ 8 / 60 ] loss: 0.042
[ 9 / 60 ] loss: 0.242
[ 10 / 60 ] loss: 0.044
[ 11 / 60 ] loss: 0.082
[ 12 / 60 ] loss: 0.038
[ 13 / 60 ] loss: 0.030
[ 14 / 60 ] loss: 0.097
[ 15 / 60 ] loss: 0.047
[ 16 / 60 ] loss: 0.036
[ 17 / 60 ] loss: 0.167
[ 18 / 60 ] loss: 0.142
[ 19 / 60 ] loss: 0.049
[ 20 / 60 ] loss: 0.066
[ 21 / 60 ] loss: 0.072
[ 22 / 60 ] loss: 0.227
[ 23 / 60 ] loss: 0.125
[ 24 / 60 ] loss: 0.035
[ 25 / 60 ] loss: 0.088
[ 26 / 60 ] loss: 0.206
[ 27 / 60 ] loss: 0.293
[ 28 / 60 ] loss: 0.028
[ 29 / 60 ] loss: 0.231
[ 30 / 60 ] loss: 0.238
[ 31 / 60 ] loss: 0.307
[ 32 / 60 ] loss: 0.055
[ 33 / 60 ] loss: 0.046
[ 34 / 60 ] loss: 0.249
[ 35 / 60 ] loss: 0.064
[ 36 / 60 ] loss: 0.024
[ 37 / 60 ] loss: 0.063
[ 38 / 60 ] loss: 0.237
[ 39 / 60 ] loss: 0.031
[ 40 / 60 ] loss: 0.031
[ 41 / 60 ] loss: 0.115
[ 42 / 60 ] loss: 0.113
[ 43 / 60 ] loss: 0.240
[ 44 / 60 ] loss: 0.046
[ 45 / 60 ] loss: 0.038
[ 46 / 60 ] loss: 0.038
[ 47 / 60 ] loss: 0.024
[ 48 / 60 ] loss: 0.027
[ 49 / 60 ] loss: 0.046
[ 50 / 60 ] loss: 0.023
[ 51 / 60 ] loss: 0.248
[ 52 / 60 ] loss: 0.360
[ 53 / 60 ] loss: 0.219
[ 54 / 60 ] loss: 0.193
[ 55 / 60 ] loss: 0.363
[ 56 / 60 ] loss: 0.112
[ 57 / 60 ] loss: 0.026
[ 58 / 60 ] loss: 0.037
[ 59 / 60 ] loss: 0.034
[ 60 / 60 ] loss: 0.280
0.1375719531128804
Accuracy: 0.746835 -- Precision: 0.761194 -- Recall: 0.784615 -- F1: 0.772727 -- AUC: 0.792883
========= epoch: 27 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.078
[ 2 / 60 ] loss: 0.445
[ 3 / 60 ] loss: 0.071
[ 4 / 60 ] loss: 0.132
[ 5 / 60 ] loss: 0.444
[ 6 / 60 ] loss: 0.707
[ 7 / 60 ] loss: 0.223
[ 8 / 60 ] loss: 0.239
[ 9 / 60 ] loss: 0.080
[ 10 / 60 ] loss: 0.469
[ 11 / 60 ] loss: 0.332
[ 12 / 60 ] loss: 1.140
[ 13 / 60 ] loss: 0.424
[ 14 / 60 ] loss: 0.665
[ 15 / 60 ] loss: 0.430
[ 16 / 60 ] loss: 0.291
[ 17 / 60 ] loss: 0.218
[ 18 / 60 ] loss: 0.087
[ 19 / 60 ] loss: 0.125
[ 20 / 60 ] loss: 0.408
[ 21 / 60 ] loss: 0.336
[ 22 / 60 ] loss: 0.356
[ 23 / 60 ] loss: 0.074
[ 24 / 60 ] loss: 0.627
[ 25 / 60 ] loss: 0.303
[ 26 / 60 ] loss: 0.103
[ 27 / 60 ] loss: 0.442
[ 28 / 60 ] loss: 0.230
[ 29 / 60 ] loss: 0.387
[ 30 / 60 ] loss: 0.160
[ 31 / 60 ] loss: 0.196
[ 32 / 60 ] loss: 0.468
[ 33 / 60 ] loss: 0.512
[ 34 / 60 ] loss: 0.432
[ 35 / 60 ] loss: 0.082
[ 36 / 60 ] loss: 0.276
[ 37 / 60 ] loss: 0.386
[ 38 / 60 ] loss: 0.117
[ 39 / 60 ] loss: 0.105
[ 40 / 60 ] loss: 0.152
[ 41 / 60 ] loss: 0.161
[ 42 / 60 ] loss: 0.245
[ 43 / 60 ] loss: 0.203
[ 44 / 60 ] loss: 0.177
[ 45 / 60 ] loss: 0.181
[ 46 / 60 ] loss: 0.451
[ 47 / 60 ] loss: 0.202
[ 48 / 60 ] loss: 0.196
[ 49 / 60 ] loss: 0.295
[ 50 / 60 ] loss: 0.077
[ 51 / 60 ] loss: 0.102
[ 52 / 60 ] loss: 0.163
[ 53 / 60 ] loss: 0.402
[ 54 / 60 ] loss: 0.341
[ 55 / 60 ] loss: 0.047
[ 56 / 60 ] loss: 0.217
[ 57 / 60 ] loss: 0.547
[ 58 / 60 ] loss: 0.225
[ 59 / 60 ] loss: 0.178
[ 60 / 60 ] loss: 0.007
0.2861643220918874
Accuracy: 0.746835 -- Precision: 0.786885 -- Recall: 0.738462 -- F1: 0.761905 -- AUC: 0.759669
========= epoch: 28 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.053
[ 2 / 60 ] loss: 0.324
[ 3 / 60 ] loss: 0.069
[ 4 / 60 ] loss: 0.051
[ 5 / 60 ] loss: 0.373
[ 6 / 60 ] loss: 0.500
[ 7 / 60 ] loss: 0.608
[ 8 / 60 ] loss: 0.208
[ 9 / 60 ] loss: 0.044
[ 10 / 60 ] loss: 0.214
[ 11 / 60 ] loss: 0.233
[ 12 / 60 ] loss: 0.298
[ 13 / 60 ] loss: 0.209
[ 14 / 60 ] loss: 0.204
[ 15 / 60 ] loss: 0.246
[ 16 / 60 ] loss: 0.429
[ 17 / 60 ] loss: 0.222
[ 18 / 60 ] loss: 0.130
[ 19 / 60 ] loss: 0.083
[ 20 / 60 ] loss: 0.208
[ 21 / 60 ] loss: 0.045
[ 22 / 60 ] loss: 0.217
[ 23 / 60 ] loss: 0.208
[ 24 / 60 ] loss: 0.289
[ 25 / 60 ] loss: 0.214
[ 26 / 60 ] loss: 0.043
[ 27 / 60 ] loss: 0.265
[ 28 / 60 ] loss: 0.047
[ 29 / 60 ] loss: 0.084
[ 30 / 60 ] loss: 0.267
[ 31 / 60 ] loss: 0.059
[ 32 / 60 ] loss: 0.244
[ 33 / 60 ] loss: 0.094
[ 34 / 60 ] loss: 0.051
[ 35 / 60 ] loss: 0.612
[ 36 / 60 ] loss: 0.228
[ 37 / 60 ] loss: 0.752
[ 38 / 60 ] loss: 0.127
[ 39 / 60 ] loss: 0.282
[ 40 / 60 ] loss: 0.092
[ 41 / 60 ] loss: 0.102
[ 42 / 60 ] loss: 0.045
[ 43 / 60 ] loss: 0.221
[ 44 / 60 ] loss: 0.161
[ 45 / 60 ] loss: 0.195
[ 46 / 60 ] loss: 0.142
[ 47 / 60 ] loss: 0.075
[ 48 / 60 ] loss: 0.162
[ 49 / 60 ] loss: 0.072
[ 50 / 60 ] loss: 0.040
[ 51 / 60 ] loss: 0.378
[ 52 / 60 ] loss: 0.114
[ 53 / 60 ] loss: 0.133
[ 54 / 60 ] loss: 0.055
[ 55 / 60 ] loss: 0.239
[ 56 / 60 ] loss: 0.232
[ 57 / 60 ] loss: 0.261
[ 58 / 60 ] loss: 0.421
[ 59 / 60 ] loss: 0.054
[ 60 / 60 ] loss: 0.040
0.2011827802285552
Accuracy: 0.713080 -- Precision: 0.715278 -- Recall: 0.792308 -- F1: 0.751825 -- AUC: 0.767973
========= epoch: 29 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.052
[ 2 / 60 ] loss: 0.040
[ 3 / 60 ] loss: 0.044
[ 4 / 60 ] loss: 0.087
[ 5 / 60 ] loss: 0.035
[ 6 / 60 ] loss: 0.051
[ 7 / 60 ] loss: 0.039
[ 8 / 60 ] loss: 0.043
[ 9 / 60 ] loss: 0.158
[ 10 / 60 ] loss: 0.146
[ 11 / 60 ] loss: 0.169
[ 12 / 60 ] loss: 0.634
[ 13 / 60 ] loss: 0.044
[ 14 / 60 ] loss: 0.364
[ 15 / 60 ] loss: 0.427
[ 16 / 60 ] loss: 0.049
[ 17 / 60 ] loss: 0.237
[ 18 / 60 ] loss: 0.034
[ 19 / 60 ] loss: 0.049
[ 20 / 60 ] loss: 0.086
[ 21 / 60 ] loss: 0.244
[ 22 / 60 ] loss: 0.583
[ 23 / 60 ] loss: 0.070
[ 24 / 60 ] loss: 0.123
[ 25 / 60 ] loss: 0.032
[ 26 / 60 ] loss: 0.031
[ 27 / 60 ] loss: 0.258
[ 28 / 60 ] loss: 0.164
[ 29 / 60 ] loss: 0.078
[ 30 / 60 ] loss: 0.619
[ 31 / 60 ] loss: 0.228
[ 32 / 60 ] loss: 0.121
[ 33 / 60 ] loss: 0.139
[ 34 / 60 ] loss: 0.207
[ 35 / 60 ] loss: 0.246
[ 36 / 60 ] loss: 0.140
[ 37 / 60 ] loss: 0.121
[ 38 / 60 ] loss: 0.279
[ 39 / 60 ] loss: 0.055
[ 40 / 60 ] loss: 0.195
[ 41 / 60 ] loss: 0.220
[ 42 / 60 ] loss: 0.047
[ 43 / 60 ] loss: 0.238
[ 44 / 60 ] loss: 0.328
[ 45 / 60 ] loss: 0.267
[ 46 / 60 ] loss: 0.036
[ 47 / 60 ] loss: 0.041
[ 48 / 60 ] loss: 0.050
[ 49 / 60 ] loss: 0.065
[ 50 / 60 ] loss: 0.225
[ 51 / 60 ] loss: 0.541
[ 52 / 60 ] loss: 0.144
[ 53 / 60 ] loss: 0.406
[ 54 / 60 ] loss: 0.407
[ 55 / 60 ] loss: 0.119
[ 56 / 60 ] loss: 0.102
[ 57 / 60 ] loss: 0.118
[ 58 / 60 ] loss: 0.151
[ 59 / 60 ] loss: 0.045
[ 60 / 60 ] loss: 0.006
0.1712699473525087
Accuracy: 0.746835 -- Precision: 0.818182 -- Recall: 0.692308 -- F1: 0.750000 -- AUC: 0.794069
========= epoch: 30 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.252
[ 2 / 60 ] loss: 0.113
[ 3 / 60 ] loss: 0.037
[ 4 / 60 ] loss: 0.299
[ 5 / 60 ] loss: 0.083
[ 6 / 60 ] loss: 0.198
[ 7 / 60 ] loss: 0.042
[ 8 / 60 ] loss: 0.049
[ 9 / 60 ] loss: 0.305
[ 10 / 60 ] loss: 0.090
[ 11 / 60 ] loss: 0.055
[ 12 / 60 ] loss: 0.123
[ 13 / 60 ] loss: 0.670
[ 14 / 60 ] loss: 0.036
[ 15 / 60 ] loss: 0.077
[ 16 / 60 ] loss: 0.085
[ 17 / 60 ] loss: 0.042
[ 18 / 60 ] loss: 0.274
[ 19 / 60 ] loss: 0.222
[ 20 / 60 ] loss: 0.259
[ 21 / 60 ] loss: 0.100
[ 22 / 60 ] loss: 0.207
[ 23 / 60 ] loss: 0.038
[ 24 / 60 ] loss: 0.242
[ 25 / 60 ] loss: 0.223
[ 26 / 60 ] loss: 0.074
[ 27 / 60 ] loss: 0.237
[ 28 / 60 ] loss: 0.280
[ 29 / 60 ] loss: 0.219
[ 30 / 60 ] loss: 0.219
[ 31 / 60 ] loss: 0.215
[ 32 / 60 ] loss: 0.040
[ 33 / 60 ] loss: 0.059
[ 34 / 60 ] loss: 0.082
[ 35 / 60 ] loss: 0.089
[ 36 / 60 ] loss: 0.060
[ 37 / 60 ] loss: 0.029
[ 38 / 60 ] loss: 0.400
[ 39 / 60 ] loss: 0.629
[ 40 / 60 ] loss: 0.368
[ 41 / 60 ] loss: 0.443
[ 42 / 60 ] loss: 0.497
[ 43 / 60 ] loss: 0.233
[ 44 / 60 ] loss: 0.057
[ 45 / 60 ] loss: 0.044
[ 46 / 60 ] loss: 0.055
[ 47 / 60 ] loss: 0.034
[ 48 / 60 ] loss: 0.150
[ 49 / 60 ] loss: 0.059
[ 50 / 60 ] loss: 0.518
[ 51 / 60 ] loss: 0.056
[ 52 / 60 ] loss: 0.115
[ 53 / 60 ] loss: 0.034
[ 54 / 60 ] loss: 0.051
[ 55 / 60 ] loss: 0.266
[ 56 / 60 ] loss: 0.059
[ 57 / 60 ] loss: 0.226
[ 58 / 60 ] loss: 0.058
[ 59 / 60 ] loss: 0.376
[ 60 / 60 ] loss: 0.035
0.1747836905531585
Accuracy: 0.742616 -- Precision: 0.767442 -- Recall: 0.761538 -- F1: 0.764479 -- AUC: 0.787922
========= epoch: 31 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.050
[ 2 / 60 ] loss: 0.056
[ 3 / 60 ] loss: 0.035
[ 4 / 60 ] loss: 0.145
[ 5 / 60 ] loss: 0.041
[ 6 / 60 ] loss: 0.414
[ 7 / 60 ] loss: 0.052
[ 8 / 60 ] loss: 0.033
[ 9 / 60 ] loss: 0.421
[ 10 / 60 ] loss: 0.043
[ 11 / 60 ] loss: 0.029
[ 12 / 60 ] loss: 0.037
[ 13 / 60 ] loss: 0.191
[ 14 / 60 ] loss: 0.038
[ 15 / 60 ] loss: 0.510
[ 16 / 60 ] loss: 0.178
[ 17 / 60 ] loss: 0.547
[ 18 / 60 ] loss: 0.493
[ 19 / 60 ] loss: 0.240
[ 20 / 60 ] loss: 0.041
[ 21 / 60 ] loss: 0.241
[ 22 / 60 ] loss: 0.407
[ 23 / 60 ] loss: 0.201
[ 24 / 60 ] loss: 0.400
[ 25 / 60 ] loss: 0.215
[ 26 / 60 ] loss: 0.225
[ 27 / 60 ] loss: 0.084
[ 28 / 60 ] loss: 0.241
[ 29 / 60 ] loss: 0.240
[ 30 / 60 ] loss: 0.168
[ 31 / 60 ] loss: 0.074
[ 32 / 60 ] loss: 0.072
[ 33 / 60 ] loss: 0.277
[ 34 / 60 ] loss: 0.266
[ 35 / 60 ] loss: 0.080
[ 36 / 60 ] loss: 0.070
[ 37 / 60 ] loss: 0.075
[ 38 / 60 ] loss: 0.154
[ 39 / 60 ] loss: 0.377
[ 40 / 60 ] loss: 0.093
[ 41 / 60 ] loss: 0.377
[ 42 / 60 ] loss: 0.062
[ 43 / 60 ] loss: 0.228
[ 44 / 60 ] loss: 0.277
[ 45 / 60 ] loss: 0.034
[ 46 / 60 ] loss: 0.051
[ 47 / 60 ] loss: 0.075
[ 48 / 60 ] loss: 0.308
[ 49 / 60 ] loss: 0.041
[ 50 / 60 ] loss: 0.052
[ 51 / 60 ] loss: 0.208
[ 52 / 60 ] loss: 0.043
[ 53 / 60 ] loss: 0.210
[ 54 / 60 ] loss: 0.047
[ 55 / 60 ] loss: 0.049
[ 56 / 60 ] loss: 0.038
[ 57 / 60 ] loss: 0.041
[ 58 / 60 ] loss: 0.223
[ 59 / 60 ] loss: 0.031
[ 60 / 60 ] loss: 0.043
0.16645828982194263
Accuracy: 0.742616 -- Precision: 0.748201 -- Recall: 0.800000 -- F1: 0.773234 -- AUC: 0.789180
========= epoch: 32 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.203
[ 2 / 60 ] loss: 0.229
[ 3 / 60 ] loss: 0.233
[ 4 / 60 ] loss: 0.045
[ 5 / 60 ] loss: 0.049
[ 6 / 60 ] loss: 0.228
[ 7 / 60 ] loss: 0.042
[ 8 / 60 ] loss: 0.408
[ 9 / 60 ] loss: 0.217
[ 10 / 60 ] loss: 0.028
[ 11 / 60 ] loss: 0.057
[ 12 / 60 ] loss: 0.109
[ 13 / 60 ] loss: 0.211
[ 14 / 60 ] loss: 0.042
[ 15 / 60 ] loss: 0.398
[ 16 / 60 ] loss: 0.044
[ 17 / 60 ] loss: 0.045
[ 18 / 60 ] loss: 0.034
[ 19 / 60 ] loss: 0.206
[ 20 / 60 ] loss: 0.067
[ 21 / 60 ] loss: 0.066
[ 22 / 60 ] loss: 0.191
[ 23 / 60 ] loss: 0.030
[ 24 / 60 ] loss: 0.091
[ 25 / 60 ] loss: 0.218
[ 26 / 60 ] loss: 0.216
[ 27 / 60 ] loss: 0.218
[ 28 / 60 ] loss: 0.039
[ 29 / 60 ] loss: 0.225
[ 30 / 60 ] loss: 0.480
[ 31 / 60 ] loss: 0.037
[ 32 / 60 ] loss: 0.236
[ 33 / 60 ] loss: 0.036
[ 34 / 60 ] loss: 0.402
[ 35 / 60 ] loss: 0.225
[ 36 / 60 ] loss: 0.098
[ 37 / 60 ] loss: 0.222
[ 38 / 60 ] loss: 0.218
[ 39 / 60 ] loss: 0.303
[ 40 / 60 ] loss: 0.219
[ 41 / 60 ] loss: 0.137
[ 42 / 60 ] loss: 0.025
[ 43 / 60 ] loss: 0.092
[ 44 / 60 ] loss: 0.064
[ 45 / 60 ] loss: 0.067
[ 46 / 60 ] loss: 0.047
[ 47 / 60 ] loss: 0.385
[ 48 / 60 ] loss: 0.042
[ 49 / 60 ] loss: 0.034
[ 50 / 60 ] loss: 0.257
[ 51 / 60 ] loss: 0.299
[ 52 / 60 ] loss: 0.321
[ 53 / 60 ] loss: 0.233
[ 54 / 60 ] loss: 0.136
[ 55 / 60 ] loss: 0.049
[ 56 / 60 ] loss: 0.165
[ 57 / 60 ] loss: 0.051
[ 58 / 60 ] loss: 0.095
[ 59 / 60 ] loss: 0.268
[ 60 / 60 ] loss: 1.462
0.18159678122028708
Accuracy: 0.755274 -- Precision: 0.785714 -- Recall: 0.761538 -- F1: 0.773438 -- AUC: 0.790726
========= epoch: 33 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.222
[ 2 / 60 ] loss: 0.293
[ 3 / 60 ] loss: 0.395
[ 4 / 60 ] loss: 0.335
[ 5 / 60 ] loss: 0.218
[ 6 / 60 ] loss: 0.322
[ 7 / 60 ] loss: 0.666
[ 8 / 60 ] loss: 0.044
[ 9 / 60 ] loss: 0.118
[ 10 / 60 ] loss: 0.084
[ 11 / 60 ] loss: 0.064
[ 12 / 60 ] loss: 0.228
[ 13 / 60 ] loss: 0.064
[ 14 / 60 ] loss: 0.242
[ 15 / 60 ] loss: 0.059
[ 16 / 60 ] loss: 0.212
[ 17 / 60 ] loss: 0.087
[ 18 / 60 ] loss: 0.395
[ 19 / 60 ] loss: 0.085
[ 20 / 60 ] loss: 0.086
[ 21 / 60 ] loss: 0.164
[ 22 / 60 ] loss: 0.593
[ 23 / 60 ] loss: 0.239
[ 24 / 60 ] loss: 0.278
[ 25 / 60 ] loss: 0.256
[ 26 / 60 ] loss: 0.058
[ 27 / 60 ] loss: 0.236
[ 28 / 60 ] loss: 0.076
[ 29 / 60 ] loss: 0.201
[ 30 / 60 ] loss: 0.069
[ 31 / 60 ] loss: 0.050
[ 32 / 60 ] loss: 0.073
[ 33 / 60 ] loss: 0.355
[ 34 / 60 ] loss: 0.214
[ 35 / 60 ] loss: 0.186
[ 36 / 60 ] loss: 0.048
[ 37 / 60 ] loss: 0.050
[ 38 / 60 ] loss: 0.209
[ 39 / 60 ] loss: 0.041
[ 40 / 60 ] loss: 0.272
[ 41 / 60 ] loss: 0.041
[ 42 / 60 ] loss: 0.057
[ 43 / 60 ] loss: 0.034
[ 44 / 60 ] loss: 0.269
[ 45 / 60 ] loss: 0.048
[ 46 / 60 ] loss: 0.037
[ 47 / 60 ] loss: 0.487
[ 48 / 60 ] loss: 0.394
[ 49 / 60 ] loss: 0.058
[ 50 / 60 ] loss: 0.031
[ 51 / 60 ] loss: 0.449
[ 52 / 60 ] loss: 0.080
[ 53 / 60 ] loss: 0.052
[ 54 / 60 ] loss: 0.128
[ 55 / 60 ] loss: 0.038
[ 56 / 60 ] loss: 0.046
[ 57 / 60 ] loss: 0.052
[ 58 / 60 ] loss: 0.036
[ 59 / 60 ] loss: 0.259
[ 60 / 60 ] loss: 0.039
0.17538381076107423
Accuracy: 0.746835 -- Precision: 0.750000 -- Recall: 0.807692 -- F1: 0.777778 -- AUC: 0.780805
========= epoch: 34 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.034
[ 2 / 60 ] loss: 0.052
[ 3 / 60 ] loss: 0.224
[ 4 / 60 ] loss: 0.038
[ 5 / 60 ] loss: 0.248
[ 6 / 60 ] loss: 0.046
[ 7 / 60 ] loss: 0.298
[ 8 / 60 ] loss: 0.062
[ 9 / 60 ] loss: 0.030
[ 10 / 60 ] loss: 0.232
[ 11 / 60 ] loss: 0.414
[ 12 / 60 ] loss: 0.074
[ 13 / 60 ] loss: 0.222
[ 14 / 60 ] loss: 0.228
[ 15 / 60 ] loss: 0.036
[ 16 / 60 ] loss: 0.218
[ 17 / 60 ] loss: 0.224
[ 18 / 60 ] loss: 0.214
[ 19 / 60 ] loss: 0.059
[ 20 / 60 ] loss: 0.042
[ 21 / 60 ] loss: 0.041
[ 22 / 60 ] loss: 0.232
[ 23 / 60 ] loss: 0.037
[ 24 / 60 ] loss: 0.228
[ 25 / 60 ] loss: 0.023
[ 26 / 60 ] loss: 0.218
[ 27 / 60 ] loss: 0.382
[ 28 / 60 ] loss: 0.126
[ 29 / 60 ] loss: 0.036
[ 30 / 60 ] loss: 0.050
[ 31 / 60 ] loss: 0.030
[ 32 / 60 ] loss: 0.029
[ 33 / 60 ] loss: 0.046
[ 34 / 60 ] loss: 0.050
[ 35 / 60 ] loss: 0.219
[ 36 / 60 ] loss: 0.040
[ 37 / 60 ] loss: 0.053
[ 38 / 60 ] loss: 0.227
[ 39 / 60 ] loss: 0.055
[ 40 / 60 ] loss: 0.145
[ 41 / 60 ] loss: 0.215
[ 42 / 60 ] loss: 0.154
[ 43 / 60 ] loss: 0.055
[ 44 / 60 ] loss: 0.046
[ 45 / 60 ] loss: 0.212
[ 46 / 60 ] loss: 0.030
[ 47 / 60 ] loss: 0.214
[ 48 / 60 ] loss: 0.056
[ 49 / 60 ] loss: 0.607
[ 50 / 60 ] loss: 0.032
[ 51 / 60 ] loss: 0.026
[ 52 / 60 ] loss: 0.046
[ 53 / 60 ] loss: 0.327
[ 54 / 60 ] loss: 0.241
[ 55 / 60 ] loss: 0.130
[ 56 / 60 ] loss: 0.519
[ 57 / 60 ] loss: 0.035
[ 58 / 60 ] loss: 0.250
[ 59 / 60 ] loss: 0.192
[ 60 / 60 ] loss: 0.023
0.1445546891229848
Accuracy: 0.738397 -- Precision: 0.736111 -- Recall: 0.815385 -- F1: 0.773723 -- AUC: 0.785550
========= epoch: 35 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.062
[ 2 / 60 ] loss: 0.029
[ 3 / 60 ] loss: 0.256
[ 4 / 60 ] loss: 0.035
[ 5 / 60 ] loss: 0.042
[ 6 / 60 ] loss: 0.028
[ 7 / 60 ] loss: 0.130
[ 8 / 60 ] loss: 0.047
[ 9 / 60 ] loss: 0.096
[ 10 / 60 ] loss: 0.032
[ 11 / 60 ] loss: 0.225
[ 12 / 60 ] loss: 0.033
[ 13 / 60 ] loss: 0.219
[ 14 / 60 ] loss: 0.055
[ 15 / 60 ] loss: 0.225
[ 16 / 60 ] loss: 0.031
[ 17 / 60 ] loss: 0.031
[ 18 / 60 ] loss: 0.431
[ 19 / 60 ] loss: 0.428
[ 20 / 60 ] loss: 0.092
[ 21 / 60 ] loss: 0.221
[ 22 / 60 ] loss: 0.035
[ 23 / 60 ] loss: 0.224
[ 24 / 60 ] loss: 0.028
[ 25 / 60 ] loss: 0.022
[ 26 / 60 ] loss: 0.057
[ 27 / 60 ] loss: 0.028
[ 28 / 60 ] loss: 0.047
[ 29 / 60 ] loss: 0.220
[ 30 / 60 ] loss: 0.018
[ 31 / 60 ] loss: 0.039
[ 32 / 60 ] loss: 0.069
[ 33 / 60 ] loss: 0.108
[ 34 / 60 ] loss: 0.044
[ 35 / 60 ] loss: 0.218
[ 36 / 60 ] loss: 0.380
[ 37 / 60 ] loss: 0.071
[ 38 / 60 ] loss: 0.221
[ 39 / 60 ] loss: 0.218
[ 40 / 60 ] loss: 0.027
[ 41 / 60 ] loss: 0.031
[ 42 / 60 ] loss: 0.013
[ 43 / 60 ] loss: 0.153
[ 44 / 60 ] loss: 0.224
[ 45 / 60 ] loss: 0.429
[ 46 / 60 ] loss: 0.047
[ 47 / 60 ] loss: 0.032
[ 48 / 60 ] loss: 0.035
[ 49 / 60 ] loss: 0.210
[ 50 / 60 ] loss: 0.227
[ 51 / 60 ] loss: 0.036
[ 52 / 60 ] loss: 0.429
[ 53 / 60 ] loss: 0.060
[ 54 / 60 ] loss: 0.217
[ 55 / 60 ] loss: 0.210
[ 56 / 60 ] loss: 0.430
[ 57 / 60 ] loss: 0.038
[ 58 / 60 ] loss: 0.211
[ 59 / 60 ] loss: 0.338
[ 60 / 60 ] loss: 0.038
0.1371274313889444
Accuracy: 0.738397 -- Precision: 0.736111 -- Recall: 0.815385 -- F1: 0.773723 -- AUC: 0.790151
========= epoch: 36 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.035
[ 2 / 60 ] loss: 0.035
[ 3 / 60 ] loss: 0.030
[ 4 / 60 ] loss: 0.017
[ 5 / 60 ] loss: 0.279
[ 6 / 60 ] loss: 0.240
[ 7 / 60 ] loss: 0.036
[ 8 / 60 ] loss: 0.077
[ 9 / 60 ] loss: 0.097
[ 10 / 60 ] loss: 0.094
[ 11 / 60 ] loss: 0.213
[ 12 / 60 ] loss: 0.035
[ 13 / 60 ] loss: 0.421
[ 14 / 60 ] loss: 0.214
[ 15 / 60 ] loss: 0.037
[ 16 / 60 ] loss: 0.024
[ 17 / 60 ] loss: 0.316
[ 18 / 60 ] loss: 0.033
[ 19 / 60 ] loss: 0.234
[ 20 / 60 ] loss: 0.034
[ 21 / 60 ] loss: 0.051
[ 22 / 60 ] loss: 0.200
[ 23 / 60 ] loss: 0.033
[ 24 / 60 ] loss: 0.392
[ 25 / 60 ] loss: 0.235
[ 26 / 60 ] loss: 0.028
[ 27 / 60 ] loss: 0.036
[ 28 / 60 ] loss: 0.037
[ 29 / 60 ] loss: 0.048
[ 30 / 60 ] loss: 0.215
[ 31 / 60 ] loss: 0.055
[ 32 / 60 ] loss: 0.390
[ 33 / 60 ] loss: 0.220
[ 34 / 60 ] loss: 0.092
[ 35 / 60 ] loss: 0.243
[ 36 / 60 ] loss: 0.037
[ 37 / 60 ] loss: 0.211
[ 38 / 60 ] loss: 0.217
[ 39 / 60 ] loss: 0.206
[ 40 / 60 ] loss: 0.376
[ 41 / 60 ] loss: 0.037
[ 42 / 60 ] loss: 0.036
[ 43 / 60 ] loss: 0.295
[ 44 / 60 ] loss: 0.244
[ 45 / 60 ] loss: 0.049
[ 46 / 60 ] loss: 0.253
[ 47 / 60 ] loss: 0.051
[ 48 / 60 ] loss: 0.375
[ 49 / 60 ] loss: 0.047
[ 50 / 60 ] loss: 0.256
[ 51 / 60 ] loss: 0.069
[ 52 / 60 ] loss: 0.123
[ 53 / 60 ] loss: 0.203
[ 54 / 60 ] loss: 0.040
[ 55 / 60 ] loss: 0.221
[ 56 / 60 ] loss: 0.043
[ 57 / 60 ] loss: 0.040
[ 58 / 60 ] loss: 0.037
[ 59 / 60 ] loss: 0.065
[ 60 / 60 ] loss: 0.032
0.13897474901750684
Accuracy: 0.751055 -- Precision: 0.738255 -- Recall: 0.846154 -- F1: 0.788530 -- AUC: 0.791589
========= epoch: 37 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.431
[ 2 / 60 ] loss: 0.036
[ 3 / 60 ] loss: 0.024
[ 4 / 60 ] loss: 0.409
[ 5 / 60 ] loss: 0.025
[ 6 / 60 ] loss: 0.228
[ 7 / 60 ] loss: 0.040
[ 8 / 60 ] loss: 0.042
[ 9 / 60 ] loss: 0.046
[ 10 / 60 ] loss: 0.213
[ 11 / 60 ] loss: 0.031
[ 12 / 60 ] loss: 0.865
[ 13 / 60 ] loss: 0.224
[ 14 / 60 ] loss: 0.227
[ 15 / 60 ] loss: 0.040
[ 16 / 60 ] loss: 0.026
[ 17 / 60 ] loss: 0.025
[ 18 / 60 ] loss: 0.033
[ 19 / 60 ] loss: 0.385
[ 20 / 60 ] loss: 0.399
[ 21 / 60 ] loss: 0.059
[ 22 / 60 ] loss: 0.214
[ 23 / 60 ] loss: 0.034
[ 24 / 60 ] loss: 0.063
[ 25 / 60 ] loss: 0.033
[ 26 / 60 ] loss: 0.042
[ 27 / 60 ] loss: 0.399
[ 28 / 60 ] loss: 0.049
[ 29 / 60 ] loss: 0.021
[ 30 / 60 ] loss: 0.229
[ 31 / 60 ] loss: 0.049
[ 32 / 60 ] loss: 0.272
[ 33 / 60 ] loss: 0.030
[ 34 / 60 ] loss: 0.166
[ 35 / 60 ] loss: 0.117
[ 36 / 60 ] loss: 0.057
[ 37 / 60 ] loss: 0.046
[ 38 / 60 ] loss: 0.231
[ 39 / 60 ] loss: 0.213
[ 40 / 60 ] loss: 0.227
[ 41 / 60 ] loss: 0.071
[ 42 / 60 ] loss: 0.267
[ 43 / 60 ] loss: 0.031
[ 44 / 60 ] loss: 0.108
[ 45 / 60 ] loss: 0.026
[ 46 / 60 ] loss: 0.204
[ 47 / 60 ] loss: 0.402
[ 48 / 60 ] loss: 0.223
[ 49 / 60 ] loss: 0.055
[ 50 / 60 ] loss: 0.050
[ 51 / 60 ] loss: 0.031
[ 52 / 60 ] loss: 0.041
[ 53 / 60 ] loss: 0.073
[ 54 / 60 ] loss: 0.292
[ 55 / 60 ] loss: 0.025
[ 56 / 60 ] loss: 0.118
[ 57 / 60 ] loss: 0.025
[ 58 / 60 ] loss: 0.221
[ 59 / 60 ] loss: 0.401
[ 60 / 60 ] loss: 0.002
0.14942132234961417
Accuracy: 0.755274 -- Precision: 0.760870 -- Recall: 0.807692 -- F1: 0.783582 -- AUC: 0.796729
========= epoch: 38 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.412
[ 2 / 60 ] loss: 0.043
[ 3 / 60 ] loss: 0.277
[ 4 / 60 ] loss: 0.044
[ 5 / 60 ] loss: 0.413
[ 6 / 60 ] loss: 0.026
[ 7 / 60 ] loss: 0.026
[ 8 / 60 ] loss: 0.038
[ 9 / 60 ] loss: 0.045
[ 10 / 60 ] loss: 0.034
[ 11 / 60 ] loss: 0.036
[ 12 / 60 ] loss: 0.020
[ 13 / 60 ] loss: 0.036
[ 14 / 60 ] loss: 0.227
[ 15 / 60 ] loss: 0.035
[ 16 / 60 ] loss: 0.036
[ 17 / 60 ] loss: 0.252
[ 18 / 60 ] loss: 0.026
[ 19 / 60 ] loss: 0.238
[ 20 / 60 ] loss: 0.450
[ 21 / 60 ] loss: 0.022
[ 22 / 60 ] loss: 0.032
[ 23 / 60 ] loss: 0.035
[ 24 / 60 ] loss: 0.392
[ 25 / 60 ] loss: 0.054
[ 26 / 60 ] loss: 0.046
[ 27 / 60 ] loss: 0.031
[ 28 / 60 ] loss: 0.223
[ 29 / 60 ] loss: 0.153
[ 30 / 60 ] loss: 0.114
[ 31 / 60 ] loss: 0.234
[ 32 / 60 ] loss: 0.039
[ 33 / 60 ] loss: 0.591
[ 34 / 60 ] loss: 0.032
[ 35 / 60 ] loss: 0.221
[ 36 / 60 ] loss: 0.212
[ 37 / 60 ] loss: 0.099
[ 38 / 60 ] loss: 0.241
[ 39 / 60 ] loss: 0.039
[ 40 / 60 ] loss: 0.031
[ 41 / 60 ] loss: 0.127
[ 42 / 60 ] loss: 0.147
[ 43 / 60 ] loss: 0.913
[ 44 / 60 ] loss: 0.060
[ 45 / 60 ] loss: 0.047
[ 46 / 60 ] loss: 0.211
[ 47 / 60 ] loss: 0.025
[ 48 / 60 ] loss: 0.138
[ 49 / 60 ] loss: 0.037
[ 50 / 60 ] loss: 0.201
[ 51 / 60 ] loss: 0.208
[ 52 / 60 ] loss: 0.206
[ 53 / 60 ] loss: 0.095
[ 54 / 60 ] loss: 0.072
[ 55 / 60 ] loss: 0.210
[ 56 / 60 ] loss: 0.048
[ 57 / 60 ] loss: 0.038
[ 58 / 60 ] loss: 0.075
[ 59 / 60 ] loss: 0.033
[ 60 / 60 ] loss: 1.450
0.16492732943346103
Accuracy: 0.738397 -- Precision: 0.729730 -- Recall: 0.830769 -- F1: 0.776978 -- AUC: 0.789612
========= epoch: 39 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.026
[ 2 / 60 ] loss: 0.331
[ 3 / 60 ] loss: 0.039
[ 4 / 60 ] loss: 0.045
[ 5 / 60 ] loss: 0.202
[ 6 / 60 ] loss: 0.072
[ 7 / 60 ] loss: 0.063
[ 8 / 60 ] loss: 0.387
[ 9 / 60 ] loss: 0.214
[ 10 / 60 ] loss: 0.129
[ 11 / 60 ] loss: 0.060
[ 12 / 60 ] loss: 0.343
[ 13 / 60 ] loss: 0.194
[ 14 / 60 ] loss: 0.378
[ 15 / 60 ] loss: 0.041
[ 16 / 60 ] loss: 0.063
[ 17 / 60 ] loss: 0.266
[ 18 / 60 ] loss: 0.045
[ 19 / 60 ] loss: 0.201
[ 20 / 60 ] loss: 0.041
[ 21 / 60 ] loss: 0.035
[ 22 / 60 ] loss: 0.069
[ 23 / 60 ] loss: 0.216
[ 24 / 60 ] loss: 0.131
[ 25 / 60 ] loss: 0.398
[ 26 / 60 ] loss: 0.201
[ 27 / 60 ] loss: 0.197
[ 28 / 60 ] loss: 0.051
[ 29 / 60 ] loss: 0.057
[ 30 / 60 ] loss: 0.077
[ 31 / 60 ] loss: 0.111
[ 32 / 60 ] loss: 0.035
[ 33 / 60 ] loss: 0.147
[ 34 / 60 ] loss: 0.195
[ 35 / 60 ] loss: 0.040
[ 36 / 60 ] loss: 0.333
[ 37 / 60 ] loss: 0.052
[ 38 / 60 ] loss: 0.423
[ 39 / 60 ] loss: 0.385
[ 40 / 60 ] loss: 0.041
[ 41 / 60 ] loss: 0.382
[ 42 / 60 ] loss: 0.551
[ 43 / 60 ] loss: 0.027
[ 44 / 60 ] loss: 0.215
[ 45 / 60 ] loss: 0.083
[ 46 / 60 ] loss: 0.370
[ 47 / 60 ] loss: 0.042
[ 48 / 60 ] loss: 0.046
[ 49 / 60 ] loss: 0.053
[ 50 / 60 ] loss: 0.044
[ 51 / 60 ] loss: 0.368
[ 52 / 60 ] loss: 0.049
[ 53 / 60 ] loss: 0.040
[ 54 / 60 ] loss: 0.054
[ 55 / 60 ] loss: 0.053
[ 56 / 60 ] loss: 0.052
[ 57 / 60 ] loss: 0.256
[ 58 / 60 ] loss: 0.049
[ 59 / 60 ] loss: 0.208
[ 60 / 60 ] loss: 0.088
0.156086008126537
Accuracy: 0.763713 -- Precision: 0.772059 -- Recall: 0.807692 -- F1: 0.789474 -- AUC: 0.796909
========= epoch: 40 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.095
[ 2 / 60 ] loss: 0.048
[ 3 / 60 ] loss: 0.246
[ 4 / 60 ] loss: 0.097
[ 5 / 60 ] loss: 0.238
[ 6 / 60 ] loss: 0.413
[ 7 / 60 ] loss: 0.032
[ 8 / 60 ] loss: 0.344
[ 9 / 60 ] loss: 0.043
[ 10 / 60 ] loss: 0.205
[ 11 / 60 ] loss: 0.206
[ 12 / 60 ] loss: 0.206
[ 13 / 60 ] loss: 0.031
[ 14 / 60 ] loss: 0.217
[ 15 / 60 ] loss: 0.031
[ 16 / 60 ] loss: 0.040
[ 17 / 60 ] loss: 0.240
[ 18 / 60 ] loss: 0.124
[ 19 / 60 ] loss: 0.029
[ 20 / 60 ] loss: 0.055
[ 21 / 60 ] loss: 0.302
[ 22 / 60 ] loss: 0.215
[ 23 / 60 ] loss: 0.063
[ 24 / 60 ] loss: 0.370
[ 25 / 60 ] loss: 0.243
[ 26 / 60 ] loss: 0.066
[ 27 / 60 ] loss: 0.236
[ 28 / 60 ] loss: 0.300
[ 29 / 60 ] loss: 0.196
[ 30 / 60 ] loss: 0.374
[ 31 / 60 ] loss: 0.045
[ 32 / 60 ] loss: 0.270
[ 33 / 60 ] loss: 0.128
[ 34 / 60 ] loss: 0.042
[ 35 / 60 ] loss: 0.193
[ 36 / 60 ] loss: 0.232
[ 37 / 60 ] loss: 0.147
[ 38 / 60 ] loss: 0.250
[ 39 / 60 ] loss: 0.143
[ 40 / 60 ] loss: 0.258
[ 41 / 60 ] loss: 0.202
[ 42 / 60 ] loss: 0.264
[ 43 / 60 ] loss: 0.452
[ 44 / 60 ] loss: 0.397
[ 45 / 60 ] loss: 0.065
[ 46 / 60 ] loss: 0.171
[ 47 / 60 ] loss: 0.125
[ 48 / 60 ] loss: 0.066
[ 49 / 60 ] loss: 0.161
[ 50 / 60 ] loss: 0.087
[ 51 / 60 ] loss: 0.257
[ 52 / 60 ] loss: 0.449
[ 53 / 60 ] loss: 0.064
[ 54 / 60 ] loss: 0.137
[ 55 / 60 ] loss: 0.308
[ 56 / 60 ] loss: 0.124
[ 57 / 60 ] loss: 0.054
[ 58 / 60 ] loss: 0.121
[ 59 / 60 ] loss: 0.258
[ 60 / 60 ] loss: 0.067
0.1807129715879758
Accuracy: 0.751055 -- Precision: 0.741497 -- Recall: 0.838462 -- F1: 0.787004 -- AUC: 0.797843
========= epoch: 41 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.067
[ 2 / 60 ] loss: 0.069
[ 3 / 60 ] loss: 0.039
[ 4 / 60 ] loss: 0.034
[ 5 / 60 ] loss: 0.024
[ 6 / 60 ] loss: 0.070
[ 7 / 60 ] loss: 0.105
[ 8 / 60 ] loss: 0.088
[ 9 / 60 ] loss: 0.101
[ 10 / 60 ] loss: 0.088
[ 11 / 60 ] loss: 0.393
[ 12 / 60 ] loss: 0.406
[ 13 / 60 ] loss: 0.090
[ 14 / 60 ] loss: 0.028
[ 15 / 60 ] loss: 0.048
[ 16 / 60 ] loss: 0.093
[ 17 / 60 ] loss: 0.260
[ 18 / 60 ] loss: 0.022
[ 19 / 60 ] loss: 0.307
[ 20 / 60 ] loss: 0.029
[ 21 / 60 ] loss: 0.045
[ 22 / 60 ] loss: 0.049
[ 23 / 60 ] loss: 0.026
[ 24 / 60 ] loss: 0.260
[ 25 / 60 ] loss: 0.491
[ 26 / 60 ] loss: 0.015
[ 27 / 60 ] loss: 0.142
[ 28 / 60 ] loss: 0.317
[ 29 / 60 ] loss: 0.022
[ 30 / 60 ] loss: 0.216
[ 31 / 60 ] loss: 0.064
[ 32 / 60 ] loss: 0.557
[ 33 / 60 ] loss: 0.013
[ 34 / 60 ] loss: 0.241
[ 35 / 60 ] loss: 0.282
[ 36 / 60 ] loss: 0.076
[ 37 / 60 ] loss: 0.048
[ 38 / 60 ] loss: 0.014
[ 39 / 60 ] loss: 0.229
[ 40 / 60 ] loss: 0.030
[ 41 / 60 ] loss: 0.577
[ 42 / 60 ] loss: 0.085
[ 43 / 60 ] loss: 0.026
[ 44 / 60 ] loss: 0.332
[ 45 / 60 ] loss: 0.104
[ 46 / 60 ] loss: 0.077
[ 47 / 60 ] loss: 0.452
[ 48 / 60 ] loss: 0.063
[ 49 / 60 ] loss: 0.249
[ 50 / 60 ] loss: 0.291
[ 51 / 60 ] loss: 0.112
[ 52 / 60 ] loss: 0.164
[ 53 / 60 ] loss: 0.024
[ 54 / 60 ] loss: 0.031
[ 55 / 60 ] loss: 0.042
[ 56 / 60 ] loss: 0.428
[ 57 / 60 ] loss: 0.110
[ 58 / 60 ] loss: 0.572
[ 59 / 60 ] loss: 0.395
[ 60 / 60 ] loss: 0.048
0.1612825120954464
Accuracy: 0.751055 -- Precision: 0.751773 -- Recall: 0.815385 -- F1: 0.782288 -- AUC: 0.765780
========= epoch: 42 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.082
[ 2 / 60 ] loss: 0.650
[ 3 / 60 ] loss: 0.028
[ 4 / 60 ] loss: 0.122
[ 5 / 60 ] loss: 0.565
[ 6 / 60 ] loss: 0.182
[ 7 / 60 ] loss: 0.425
[ 8 / 60 ] loss: 0.593
[ 9 / 60 ] loss: 1.073
[ 10 / 60 ] loss: 0.095
[ 11 / 60 ] loss: 1.060
[ 12 / 60 ] loss: 0.450
[ 13 / 60 ] loss: 0.339
[ 14 / 60 ] loss: 0.309
[ 15 / 60 ] loss: 0.156
[ 16 / 60 ] loss: 0.067
[ 17 / 60 ] loss: 0.170
[ 18 / 60 ] loss: 0.249
[ 19 / 60 ] loss: 0.176
[ 20 / 60 ] loss: 0.298
[ 21 / 60 ] loss: 0.309
[ 22 / 60 ] loss: 0.263
[ 23 / 60 ] loss: 0.062
[ 24 / 60 ] loss: 0.096
[ 25 / 60 ] loss: 0.325
[ 26 / 60 ] loss: 0.545
[ 27 / 60 ] loss: 0.125
[ 28 / 60 ] loss: 0.130
[ 29 / 60 ] loss: 0.096
[ 30 / 60 ] loss: 0.079
[ 31 / 60 ] loss: 0.226
[ 32 / 60 ] loss: 0.149
[ 33 / 60 ] loss: 0.069
[ 34 / 60 ] loss: 0.072
[ 35 / 60 ] loss: 0.140
[ 36 / 60 ] loss: 0.207
[ 37 / 60 ] loss: 0.313
[ 38 / 60 ] loss: 0.225
[ 39 / 60 ] loss: 0.229
[ 40 / 60 ] loss: 0.037
[ 41 / 60 ] loss: 0.405
[ 42 / 60 ] loss: 0.138
[ 43 / 60 ] loss: 0.029
[ 44 / 60 ] loss: 0.031
[ 45 / 60 ] loss: 0.222
[ 46 / 60 ] loss: 0.198
[ 47 / 60 ] loss: 0.186
[ 48 / 60 ] loss: 0.052
[ 49 / 60 ] loss: 0.271
[ 50 / 60 ] loss: 0.036
[ 51 / 60 ] loss: 0.054
[ 52 / 60 ] loss: 0.069
[ 53 / 60 ] loss: 0.292
[ 54 / 60 ] loss: 0.114
[ 55 / 60 ] loss: 0.038
[ 56 / 60 ] loss: 0.386
[ 57 / 60 ] loss: 0.039
[ 58 / 60 ] loss: 0.051
[ 59 / 60 ] loss: 0.047
[ 60 / 60 ] loss: 0.029
0.22455692710354924
Accuracy: 0.751055 -- Precision: 0.735099 -- Recall: 0.853846 -- F1: 0.790036 -- AUC: 0.748095
========= epoch: 43 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.092
[ 2 / 60 ] loss: 0.023
[ 3 / 60 ] loss: 0.316
[ 4 / 60 ] loss: 0.039
[ 5 / 60 ] loss: 0.216
[ 6 / 60 ] loss: 0.120
[ 7 / 60 ] loss: 0.025
[ 8 / 60 ] loss: 0.107
[ 9 / 60 ] loss: 0.248
[ 10 / 60 ] loss: 0.227
[ 11 / 60 ] loss: 0.028
[ 12 / 60 ] loss: 0.120
[ 13 / 60 ] loss: 0.031
[ 14 / 60 ] loss: 0.263
[ 15 / 60 ] loss: 0.058
[ 16 / 60 ] loss: 0.112
[ 17 / 60 ] loss: 0.231
[ 18 / 60 ] loss: 0.478
[ 19 / 60 ] loss: 0.045
[ 20 / 60 ] loss: 0.027
[ 21 / 60 ] loss: 0.089
[ 22 / 60 ] loss: 0.257
[ 23 / 60 ] loss: 0.040
[ 24 / 60 ] loss: 0.027
[ 25 / 60 ] loss: 0.311
[ 26 / 60 ] loss: 0.035
[ 27 / 60 ] loss: 0.040
[ 28 / 60 ] loss: 0.142
[ 29 / 60 ] loss: 0.037
[ 30 / 60 ] loss: 0.130
[ 31 / 60 ] loss: 0.042
[ 32 / 60 ] loss: 0.234
[ 33 / 60 ] loss: 0.177
[ 34 / 60 ] loss: 0.030
[ 35 / 60 ] loss: 0.082
[ 36 / 60 ] loss: 0.331
[ 37 / 60 ] loss: 0.411
[ 38 / 60 ] loss: 0.055
[ 39 / 60 ] loss: 0.539
[ 40 / 60 ] loss: 0.236
[ 41 / 60 ] loss: 0.303
[ 42 / 60 ] loss: 0.114
[ 43 / 60 ] loss: 0.436
[ 44 / 60 ] loss: 0.044
[ 45 / 60 ] loss: 0.439
[ 46 / 60 ] loss: 0.093
[ 47 / 60 ] loss: 0.213
[ 48 / 60 ] loss: 0.031
[ 49 / 60 ] loss: 0.023
[ 50 / 60 ] loss: 0.078
[ 51 / 60 ] loss: 0.073
[ 52 / 60 ] loss: 0.253
[ 53 / 60 ] loss: 0.085
[ 54 / 60 ] loss: 0.098
[ 55 / 60 ] loss: 0.107
[ 56 / 60 ] loss: 0.028
[ 57 / 60 ] loss: 0.255
[ 58 / 60 ] loss: 0.302
[ 59 / 60 ] loss: 0.286
[ 60 / 60 ] loss: 0.007
0.15529824035086978
Accuracy: 0.780591 -- Precision: 0.778571 -- Recall: 0.838462 -- F1: 0.807407 -- AUC: 0.773185
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[18, 203, 112, 143, 21, 63, 9, 177, 167, 7, 211, 156, 84, 169, 41, 8, 25, 149, 227, 79, 73, 218, 186, 176, 166, 215, 190, 138, 61, 94, 15, 205, 68, 202, 106, 108, 146, 95, 20, 150, 39, 237, 116, 4, 225, 163, 115, 135, 101, 75, 235, 228, 107, 23, 207, 38, 155, 52, 3, 14, 19, 175, 43, 36, 221, 96, 137, 126, 88, 157, 153, 30, 113, 70, 174, 179, 55, 134, 60, 141, 64, 199, 151, 81, 102, 171, 182, 204, 197, 54, 82, 89, 29, 121, 11, 24, 209, 62, 212, 10, 125, 77, 31, 145, 67, 229, 216, 223, 194, 131, 76, 100, 72, 93, 187, 147, 120, 142, 51, 132, 46, 26, 206, 103, 117, 224, 208, 98, 2, 5, 222, 37, 129, 111, 57, 104, 6, 214, 165, 33, 92, 42, 114, 13, 71, 144, 44, 164, 148, 50, 198, 193, 16, 109, 105, 192, 99, 48, 210, 128, 200, 45, 22, 130, 78, 32, 161, 226, 154, 188, 136, 236, 139, 201, 219, 122, 91, 191, 160, 80, 110, 69, 133, 56, 85, 66, 170, 232, 123, 152, 159, 118, 90, 86, 74, 172, 162, 17, 195, 158, 97, 231, 40, 181, 173, 185, 230, 168, 27, 28, 183, 119, 189, 47, 53, 65, 220, 35, 196, 87, 233, 58, 127, 180, 49, 1, 124, 184, 178, 234, 140, 217, 12, 213, 34, 59, 83]
[0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]
[0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 44 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.119
[ 2 / 60 ] loss: 0.074
[ 3 / 60 ] loss: 0.028
[ 4 / 60 ] loss: 0.127
[ 5 / 60 ] loss: 0.045
[ 6 / 60 ] loss: 0.250
[ 7 / 60 ] loss: 0.274
[ 8 / 60 ] loss: 0.283
[ 9 / 60 ] loss: 0.066
[ 10 / 60 ] loss: 0.031
[ 11 / 60 ] loss: 0.029
[ 12 / 60 ] loss: 0.165
[ 13 / 60 ] loss: 0.029
[ 14 / 60 ] loss: 0.392
[ 15 / 60 ] loss: 0.301
[ 16 / 60 ] loss: 0.208
[ 17 / 60 ] loss: 0.462
[ 18 / 60 ] loss: 0.304
[ 19 / 60 ] loss: 0.552
[ 20 / 60 ] loss: 0.365
[ 21 / 60 ] loss: 0.986
[ 22 / 60 ] loss: 0.438
[ 23 / 60 ] loss: 0.063
[ 24 / 60 ] loss: 0.438
[ 25 / 60 ] loss: 0.452
[ 26 / 60 ] loss: 0.284
[ 27 / 60 ] loss: 0.399
[ 28 / 60 ] loss: 0.344
[ 29 / 60 ] loss: 0.222
[ 30 / 60 ] loss: 0.353
[ 31 / 60 ] loss: 0.327
[ 32 / 60 ] loss: 0.387
[ 33 / 60 ] loss: 0.402
[ 34 / 60 ] loss: 0.069
[ 35 / 60 ] loss: 0.185
[ 36 / 60 ] loss: 0.465
[ 37 / 60 ] loss: 0.390
[ 38 / 60 ] loss: 0.832
[ 39 / 60 ] loss: 0.799
[ 40 / 60 ] loss: 0.791
[ 41 / 60 ] loss: 0.610
[ 42 / 60 ] loss: 0.688
[ 43 / 60 ] loss: 0.383
[ 44 / 60 ] loss: 0.349
[ 45 / 60 ] loss: 0.651
[ 46 / 60 ] loss: 0.282
[ 47 / 60 ] loss: 0.129
[ 48 / 60 ] loss: 0.266
[ 49 / 60 ] loss: 0.082
[ 50 / 60 ] loss: 0.275
[ 51 / 60 ] loss: 0.466
[ 52 / 60 ] loss: 0.227
[ 53 / 60 ] loss: 0.307
[ 54 / 60 ] loss: 0.719
[ 55 / 60 ] loss: 0.369
[ 56 / 60 ] loss: 0.498
[ 57 / 60 ] loss: 0.244
[ 58 / 60 ] loss: 0.066
[ 59 / 60 ] loss: 0.064
[ 60 / 60 ] loss: 1.132
0.34234949555248023
Accuracy: 0.721519 -- Precision: 0.785714 -- Recall: 0.676923 -- F1: 0.727273 -- AUC: 0.791912
========= epoch: 45 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.064
[ 2 / 60 ] loss: 0.259
[ 3 / 60 ] loss: 0.348
[ 4 / 60 ] loss: 0.180
[ 5 / 60 ] loss: 0.374
[ 6 / 60 ] loss: 0.226
[ 7 / 60 ] loss: 0.396
[ 8 / 60 ] loss: 0.256
[ 9 / 60 ] loss: 0.440
[ 10 / 60 ] loss: 0.219
[ 11 / 60 ] loss: 0.263
[ 12 / 60 ] loss: 0.450
[ 13 / 60 ] loss: 0.092
[ 14 / 60 ] loss: 0.130
[ 15 / 60 ] loss: 0.551
[ 16 / 60 ] loss: 0.628
[ 17 / 60 ] loss: 0.139
[ 18 / 60 ] loss: 0.275
[ 19 / 60 ] loss: 0.439
[ 20 / 60 ] loss: 0.632
[ 21 / 60 ] loss: 0.426
[ 22 / 60 ] loss: 0.394
[ 23 / 60 ] loss: 0.472
[ 24 / 60 ] loss: 0.505
[ 25 / 60 ] loss: 0.151
[ 26 / 60 ] loss: 0.311
[ 27 / 60 ] loss: 0.099
[ 28 / 60 ] loss: 0.243
[ 29 / 60 ] loss: 0.225
[ 30 / 60 ] loss: 0.092
[ 31 / 60 ] loss: 0.077
[ 32 / 60 ] loss: 0.099
[ 33 / 60 ] loss: 0.349
[ 34 / 60 ] loss: 0.573
[ 35 / 60 ] loss: 0.077
[ 36 / 60 ] loss: 0.062
[ 37 / 60 ] loss: 0.094
[ 38 / 60 ] loss: 0.180
[ 39 / 60 ] loss: 0.236
[ 40 / 60 ] loss: 0.235
[ 41 / 60 ] loss: 0.261
[ 42 / 60 ] loss: 0.419
[ 43 / 60 ] loss: 0.106
[ 44 / 60 ] loss: 0.463
[ 45 / 60 ] loss: 0.756
[ 46 / 60 ] loss: 0.108
[ 47 / 60 ] loss: 0.393
[ 48 / 60 ] loss: 0.159
[ 49 / 60 ] loss: 0.241
[ 50 / 60 ] loss: 0.274
[ 51 / 60 ] loss: 0.396
[ 52 / 60 ] loss: 0.198
[ 53 / 60 ] loss: 0.091
[ 54 / 60 ] loss: 0.284
[ 55 / 60 ] loss: 0.142
[ 56 / 60 ] loss: 0.280
[ 57 / 60 ] loss: 0.571
[ 58 / 60 ] loss: 0.292
[ 59 / 60 ] loss: 0.285
[ 60 / 60 ] loss: 0.065
0.28408733680844306
Accuracy: 0.713080 -- Precision: 0.698718 -- Recall: 0.838462 -- F1: 0.762238 -- AUC: 0.776456
========= epoch: 46 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.271
[ 2 / 60 ] loss: 0.259
[ 3 / 60 ] loss: 0.479
[ 4 / 60 ] loss: 0.257
[ 5 / 60 ] loss: 0.246
[ 6 / 60 ] loss: 0.347
[ 7 / 60 ] loss: 0.354
[ 8 / 60 ] loss: 0.154
[ 9 / 60 ] loss: 0.234
[ 10 / 60 ] loss: 0.090
[ 11 / 60 ] loss: 0.233
[ 12 / 60 ] loss: 0.433
[ 13 / 60 ] loss: 0.475
[ 14 / 60 ] loss: 0.073
[ 15 / 60 ] loss: 0.185
[ 16 / 60 ] loss: 0.077
[ 17 / 60 ] loss: 0.083
[ 18 / 60 ] loss: 0.389
[ 19 / 60 ] loss: 0.560
[ 20 / 60 ] loss: 0.242
[ 21 / 60 ] loss: 0.188
[ 22 / 60 ] loss: 0.077
[ 23 / 60 ] loss: 0.255
[ 24 / 60 ] loss: 0.064
[ 25 / 60 ] loss: 0.259
[ 26 / 60 ] loss: 0.340
[ 27 / 60 ] loss: 0.544
[ 28 / 60 ] loss: 0.392
[ 29 / 60 ] loss: 0.385
[ 30 / 60 ] loss: 0.215
[ 31 / 60 ] loss: 0.215
[ 32 / 60 ] loss: 0.209
[ 33 / 60 ] loss: 0.225
[ 34 / 60 ] loss: 0.069
[ 35 / 60 ] loss: 0.066
[ 36 / 60 ] loss: 0.071
[ 37 / 60 ] loss: 0.080
[ 38 / 60 ] loss: 0.196
[ 39 / 60 ] loss: 0.260
[ 40 / 60 ] loss: 0.066
[ 41 / 60 ] loss: 0.268
[ 42 / 60 ] loss: 0.058
[ 43 / 60 ] loss: 0.192
[ 44 / 60 ] loss: 0.460
[ 45 / 60 ] loss: 0.543
[ 46 / 60 ] loss: 0.236
[ 47 / 60 ] loss: 0.225
[ 48 / 60 ] loss: 0.383
[ 49 / 60 ] loss: 0.207
[ 50 / 60 ] loss: 0.200
[ 51 / 60 ] loss: 0.323
[ 52 / 60 ] loss: 0.106
[ 53 / 60 ] loss: 0.229
[ 54 / 60 ] loss: 0.065
[ 55 / 60 ] loss: 0.124
[ 56 / 60 ] loss: 0.212
[ 57 / 60 ] loss: 0.287
[ 58 / 60 ] loss: 0.309
[ 59 / 60 ] loss: 0.049
[ 60 / 60 ] loss: 0.069
0.23596505131572484
Accuracy: 0.725738 -- Precision: 0.737226 -- Recall: 0.776923 -- F1: 0.756554 -- AUC: 0.794464
========= epoch: 47 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.051
[ 2 / 60 ] loss: 0.055
[ 3 / 60 ] loss: 0.050
[ 4 / 60 ] loss: 0.285
[ 5 / 60 ] loss: 0.578
[ 6 / 60 ] loss: 0.226
[ 7 / 60 ] loss: 0.052
[ 8 / 60 ] loss: 0.224
[ 9 / 60 ] loss: 0.055
[ 10 / 60 ] loss: 0.115
[ 11 / 60 ] loss: 0.047
[ 12 / 60 ] loss: 0.352
[ 13 / 60 ] loss: 0.948
[ 14 / 60 ] loss: 0.047
[ 15 / 60 ] loss: 0.485
[ 16 / 60 ] loss: 0.575
[ 17 / 60 ] loss: 0.589
[ 18 / 60 ] loss: 0.232
[ 19 / 60 ] loss: 0.440
[ 20 / 60 ] loss: 1.213
[ 21 / 60 ] loss: 0.480
[ 22 / 60 ] loss: 0.373
[ 23 / 60 ] loss: 0.879
[ 24 / 60 ] loss: 0.773
[ 25 / 60 ] loss: 0.656
[ 26 / 60 ] loss: 0.480
[ 27 / 60 ] loss: 0.572
[ 28 / 60 ] loss: 0.631
[ 29 / 60 ] loss: 0.551
[ 30 / 60 ] loss: 0.487
[ 31 / 60 ] loss: 0.363
[ 32 / 60 ] loss: 0.538
[ 33 / 60 ] loss: 0.424
[ 34 / 60 ] loss: 0.260
[ 35 / 60 ] loss: 0.516
[ 36 / 60 ] loss: 0.260
[ 37 / 60 ] loss: 0.283
[ 38 / 60 ] loss: 0.336
[ 39 / 60 ] loss: 0.338
[ 40 / 60 ] loss: 0.234
[ 41 / 60 ] loss: 0.164
[ 42 / 60 ] loss: 0.247
[ 43 / 60 ] loss: 0.300
[ 44 / 60 ] loss: 0.420
[ 45 / 60 ] loss: 0.457
[ 46 / 60 ] loss: 0.218
[ 47 / 60 ] loss: 0.218
[ 48 / 60 ] loss: 0.144
[ 49 / 60 ] loss: 0.387
[ 50 / 60 ] loss: 0.246
[ 51 / 60 ] loss: 0.088
[ 52 / 60 ] loss: 0.314
[ 53 / 60 ] loss: 0.349
[ 54 / 60 ] loss: 0.100
[ 55 / 60 ] loss: 0.102
[ 56 / 60 ] loss: 0.161
[ 57 / 60 ] loss: 0.166
[ 58 / 60 ] loss: 0.070
[ 59 / 60 ] loss: 0.068
[ 60 / 60 ] loss: 0.087
0.3393666756649812
Accuracy: 0.763713 -- Precision: 0.784615 -- Recall: 0.784615 -- F1: 0.784615 -- AUC: 0.741769
========= epoch: 48 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.188
[ 2 / 60 ] loss: 0.107
[ 3 / 60 ] loss: 0.262
[ 4 / 60 ] loss: 0.064
[ 5 / 60 ] loss: 0.194
[ 6 / 60 ] loss: 0.316
[ 7 / 60 ] loss: 0.050
[ 8 / 60 ] loss: 0.247
[ 9 / 60 ] loss: 0.232
[ 10 / 60 ] loss: 0.162
[ 11 / 60 ] loss: 0.058
[ 12 / 60 ] loss: 0.102
[ 13 / 60 ] loss: 0.041
[ 14 / 60 ] loss: 0.141
[ 15 / 60 ] loss: 0.208
[ 16 / 60 ] loss: 0.559
[ 17 / 60 ] loss: 0.224
[ 18 / 60 ] loss: 0.061
[ 19 / 60 ] loss: 0.064
[ 20 / 60 ] loss: 0.069
[ 21 / 60 ] loss: 0.265
[ 22 / 60 ] loss: 0.071
[ 23 / 60 ] loss: 0.042
[ 24 / 60 ] loss: 0.039
[ 25 / 60 ] loss: 0.118
[ 26 / 60 ] loss: 0.394
[ 27 / 60 ] loss: 0.073
[ 28 / 60 ] loss: 0.300
[ 29 / 60 ] loss: 0.062
[ 30 / 60 ] loss: 0.137
[ 31 / 60 ] loss: 0.263
[ 32 / 60 ] loss: 0.428
[ 33 / 60 ] loss: 0.279
[ 34 / 60 ] loss: 0.259
[ 35 / 60 ] loss: 0.331
[ 36 / 60 ] loss: 0.250
[ 37 / 60 ] loss: 0.031
[ 38 / 60 ] loss: 0.174
[ 39 / 60 ] loss: 0.620
[ 40 / 60 ] loss: 0.398
[ 41 / 60 ] loss: 0.401
[ 42 / 60 ] loss: 0.255
[ 43 / 60 ] loss: 0.052
[ 44 / 60 ] loss: 0.045
[ 45 / 60 ] loss: 0.046
[ 46 / 60 ] loss: 0.047
[ 47 / 60 ] loss: 0.221
[ 48 / 60 ] loss: 0.133
[ 49 / 60 ] loss: 0.092
[ 50 / 60 ] loss: 0.232
[ 51 / 60 ] loss: 0.281
[ 52 / 60 ] loss: 0.436
[ 53 / 60 ] loss: 0.118
[ 54 / 60 ] loss: 0.194
[ 55 / 60 ] loss: 0.706
[ 56 / 60 ] loss: 0.229
[ 57 / 60 ] loss: 0.056
[ 58 / 60 ] loss: 0.209
[ 59 / 60 ] loss: 0.349
[ 60 / 60 ] loss: 0.717
0.21172797186300157
Accuracy: 0.742616 -- Precision: 0.789916 -- Recall: 0.723077 -- F1: 0.755020 -- AUC: 0.813659
========= epoch: 49 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 60 ] loss: 0.302
[ 2 / 60 ] loss: 0.219
[ 3 / 60 ] loss: 0.415
[ 4 / 60 ] loss: 0.338
[ 5 / 60 ] loss: 0.043
[ 6 / 60 ] loss: 0.064
[ 7 / 60 ] loss: 0.262
[ 8 / 60 ] loss: 0.047
[ 9 / 60 ] loss: 0.120
[ 10 / 60 ] loss: 0.234
[ 11 / 60 ] loss: 0.239
[ 12 / 60 ] loss: 0.218
[ 13 / 60 ] loss: 0.324
[ 14 / 60 ] loss: 0.053
[ 15 / 60 ] loss: 0.315
[ 16 / 60 ] loss: 0.054
[ 17 / 60 ] loss: 0.060
[ 18 / 60 ] loss: 0.269
[ 19 / 60 ] loss: 0.168
[ 20 / 60 ] loss: 0.170
[ 21 / 60 ] loss: 0.078
[ 22 / 60 ] loss: 0.232
[ 23 / 60 ] loss: 0.271
[ 24 / 60 ] loss: 0.317
[ 25 / 60 ] loss: 0.083
[ 26 / 60 ] loss: 0.370
[ 27 / 60 ] loss: 0.304
[ 28 / 60 ] loss: 0.387
[ 29 / 60 ] loss: 0.041
[ 30 / 60 ] loss: 0.302
[ 31 / 60 ] loss: 0.224
[ 32 / 60 ] loss: 0.043
[ 33 / 60 ] loss: 0.383
[ 34 / 60 ] loss: 0.068
[ 35 / 60 ] loss: 0.243
[ 36 / 60 ] loss: 0.134
[ 37 / 60 ] loss: 0.512
[ 38 / 60 ] loss: 0.057
[ 39 / 60 ] loss: 0.247
[ 40 / 60 ] loss: 0.253
[ 41 / 60 ] loss: 0.582
[ 42 / 60 ] loss: 0.126
[ 43 / 60 ] loss: 0.225
[ 44 / 60 ] loss: 0.496
[ 45 / 60 ] loss: 0.072
[ 46 / 60 ] loss: 0.239
[ 47 / 60 ] loss: 0.392
[ 48 / 60 ] loss: 0.093
[ 49 / 60 ] loss: 0.133
[ 50 / 60 ] loss: 0.057
[ 51 / 60 ] loss: 0.064
[ 52 / 60 ] loss: 0.385
[ 53 / 60 ] loss: 0.278
[ 54 / 60 ] loss: 0.262
[ 55 / 60 ] loss: 0.241
[ 56 / 60 ] loss: 0.087
[ 57 / 60 ] loss: 0.447
[ 58 / 60 ] loss: 0.220
[ 59 / 60 ] loss: 0.423
[ 60 / 60 ] loss: 0.552
0.23060336355119943
Accuracy: 0.713080 -- Precision: 0.701299 -- Recall: 0.830769 -- F1: 0.760563 -- AUC: 0.722717
0.7805907172995781 0.7785714285714286 0.8384615384615385 0.8074074074074075 0.7731847591660675
[18, 203, 112, 143, 21, 63, 9, 177, 167, 7, 211, 156, 84, 169, 41, 8, 25, 149, 227, 79, 73, 218, 186, 176, 166, 215, 190, 138, 61, 94, 15, 205, 68, 202, 106, 108, 146, 95, 20, 150, 39, 237, 116, 4, 225, 163, 115, 135, 101, 75, 235, 228, 107, 23, 207, 38, 155, 52, 3, 14, 19, 175, 43, 36, 221, 96, 137, 126, 88, 157, 153, 30, 113, 70, 174, 179, 55, 134, 60, 141, 64, 199, 151, 81, 102, 171, 182, 204, 197, 54, 82, 89, 29, 121, 11, 24, 209, 62, 212, 10, 125, 77, 31, 145, 67, 229, 216, 223, 194, 131, 76, 100, 72, 93, 187, 147, 120, 142, 51, 132, 46, 26, 206, 103, 117, 224, 208, 98, 2, 5, 222, 37, 129, 111, 57, 104, 6, 214, 165, 33, 92, 42, 114, 13, 71, 144, 44, 164, 148, 50, 198, 193, 16, 109, 105, 192, 99, 48, 210, 128, 200, 45, 22, 130, 78, 32, 161, 226, 154, 188, 136, 236, 139, 201, 219, 122, 91, 191, 160, 80, 110, 69, 133, 56, 85, 66, 170, 232, 123, 152, 159, 118, 90, 86, 74, 172, 162, 17, 195, 158, 97, 231, 40, 181, 173, 185, 230, 168, 27, 28, 183, 119, 189, 47, 53, 65, 220, 35, 196, 87, 233, 58, 127, 180, 49, 1, 124, 184, 178, 234, 140, 217, 12, 213, 34, 59, 83]
[0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]
[0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]
