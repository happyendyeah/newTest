nohup: ignoring input
训练集: 946
测试集: 237
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 0 ==============
Token indices sequence length is longer than the specified maximum sequence length for this model (3839 > 512). Running this sequence through the model will result in indexing errors
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.698
[ 2 / 40 ] loss: 0.694
[ 3 / 40 ] loss: 0.684
[ 4 / 40 ] loss: 0.697
[ 5 / 40 ] loss: 0.698
[ 6 / 40 ] loss: 0.686
[ 7 / 40 ] loss: 0.692
[ 8 / 40 ] loss: 0.690
[ 9 / 40 ] loss: 0.693
[ 10 / 40 ] loss: 0.681
[ 11 / 40 ] loss: 0.679
[ 12 / 40 ] loss: 0.724
[ 13 / 40 ] loss: 0.692
[ 14 / 40 ] loss: 0.687
[ 15 / 40 ] loss: 0.684
[ 16 / 40 ] loss: 0.692
[ 17 / 40 ] loss: 0.679
[ 18 / 40 ] loss: 0.693
[ 19 / 40 ] loss: 0.701
[ 20 / 40 ] loss: 0.647
[ 21 / 40 ] loss: 0.658
[ 22 / 40 ] loss: 0.710
[ 23 / 40 ] loss: 0.639
[ 24 / 40 ] loss: 0.638
[ 25 / 40 ] loss: 0.682
[ 26 / 40 ] loss: 0.624
[ 27 / 40 ] loss: 0.676
[ 28 / 40 ] loss: 0.658
[ 29 / 40 ] loss: 0.664
[ 30 / 40 ] loss: 0.649
[ 31 / 40 ] loss: 0.623
[ 32 / 40 ] loss: 0.642
[ 33 / 40 ] loss: 0.665
[ 34 / 40 ] loss: 0.703
[ 35 / 40 ] loss: 0.765
[ 36 / 40 ] loss: 0.755
[ 37 / 40 ] loss: 0.696
[ 38 / 40 ] loss: 0.688
[ 39 / 40 ] loss: 0.662
[ 40 / 40 ] loss: 0.642
0.6807831645011901
Accuracy: 0.582278 -- Precision: 0.568889 -- Recall: 0.984615 -- F1: 0.721127 -- AUC: 0.633645
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[26, 195, 171, 154, 217, 167, 22, 30, 157, 148, 63, 65, 223, 103, 162, 226, 225, 182, 220, 210, 185, 6, 115, 200, 156, 28, 79, 50, 197, 31, 139, 47, 222, 83, 17, 118, 173, 8, 104, 159, 35, 1, 3, 174, 70, 55, 145, 92, 230, 54, 39, 69, 36, 204, 175, 125, 12, 150, 80, 100, 18, 192, 48, 202, 216, 96, 29, 52, 108, 10, 76, 178, 212, 4, 193, 198, 131, 237, 224, 51, 137, 101, 236, 53, 138, 40, 120, 164, 158, 88, 126, 72, 221, 64, 136, 166, 169, 25, 144, 74, 13, 21, 49, 45, 107, 73, 134, 99, 91, 105, 194, 127, 56, 147, 68, 60, 203, 66, 234, 199, 129, 211, 184, 231, 59, 81, 15, 140, 102, 5, 113, 43, 205, 190, 41, 143, 189, 109, 161, 62, 98, 33, 146, 208, 155, 201, 233, 32, 229, 179, 11, 135, 85, 34, 219, 168, 141, 132, 218, 228, 58, 128, 176, 87, 122, 95, 183, 82, 9, 114, 46, 151, 2, 78, 111, 213, 16, 44, 93, 112, 232, 37, 180, 116, 214, 188, 86, 196, 14, 67, 207, 57, 160, 142, 75, 130, 227, 165, 7, 206, 110, 121, 172, 124, 106, 19, 153, 27, 89, 209, 119, 90, 181, 97, 61, 117, 24, 187, 191, 133, 84, 23, 170, 123, 38, 149, 152, 20, 186, 215, 163, 42, 71, 235, 177, 94, 77]
[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 1 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.663
[ 2 / 40 ] loss: 0.680
[ 3 / 40 ] loss: 0.663
[ 4 / 40 ] loss: 0.672
[ 5 / 40 ] loss: 0.653
[ 6 / 40 ] loss: 0.622
[ 7 / 40 ] loss: 0.648
[ 8 / 40 ] loss: 0.702
[ 9 / 40 ] loss: 0.629
[ 10 / 40 ] loss: 0.626
[ 11 / 40 ] loss: 0.642
[ 12 / 40 ] loss: 0.649
[ 13 / 40 ] loss: 0.672
[ 14 / 40 ] loss: 0.625
[ 15 / 40 ] loss: 0.695
[ 16 / 40 ] loss: 0.668
[ 17 / 40 ] loss: 0.530
[ 18 / 40 ] loss: 0.651
[ 19 / 40 ] loss: 0.586
[ 20 / 40 ] loss: 0.629
[ 21 / 40 ] loss: 0.715
[ 22 / 40 ] loss: 0.538
[ 23 / 40 ] loss: 0.649
[ 24 / 40 ] loss: 0.561
[ 25 / 40 ] loss: 0.617
[ 26 / 40 ] loss: 0.854
[ 27 / 40 ] loss: 0.657
[ 28 / 40 ] loss: 0.605
[ 29 / 40 ] loss: 0.560
[ 30 / 40 ] loss: 0.603
[ 31 / 40 ] loss: 0.562
[ 32 / 40 ] loss: 0.588
[ 33 / 40 ] loss: 0.637
[ 34 / 40 ] loss: 0.538
[ 35 / 40 ] loss: 0.633
[ 36 / 40 ] loss: 0.534
[ 37 / 40 ] loss: 0.732
[ 38 / 40 ] loss: 0.732
[ 39 / 40 ] loss: 0.531
[ 40 / 40 ] loss: 0.625
0.6343134626746177
Accuracy: 0.654008 -- Precision: 0.625000 -- Recall: 0.923077 -- F1: 0.745342 -- AUC: 0.701653
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[61, 18, 78, 115, 121, 226, 148, 127, 125, 35, 139, 137, 70, 2, 140, 13, 87, 222, 54, 10, 144, 4, 100, 204, 92, 36, 6, 113, 216, 37, 42, 184, 41, 165, 79, 80, 119, 164, 221, 198, 163, 27, 205, 81, 59, 210, 207, 49, 223, 117, 118, 154, 17, 191, 44, 190, 130, 23, 99, 213, 108, 34, 60, 67, 196, 234, 211, 46, 32, 168, 56, 48, 101, 170, 30, 135, 76, 159, 71, 66, 195, 178, 7, 63, 83, 114, 124, 176, 136, 91, 217, 197, 220, 219, 14, 57, 15, 155, 39, 112, 110, 9, 237, 77, 175, 104, 22, 129, 28, 166, 3, 133, 84, 189, 51, 146, 86, 202, 143, 203, 16, 141, 94, 232, 102, 162, 19, 171, 206, 1, 215, 173, 122, 160, 120, 47, 97, 64, 153, 65, 236, 185, 152, 40, 24, 169, 214, 147, 11, 212, 43, 128, 96, 98, 93, 142, 74, 109, 151, 45, 224, 106, 231, 25, 21, 188, 123, 111, 55, 138, 89, 8, 33, 150, 179, 5, 201, 199, 50, 52, 181, 53, 230, 88, 157, 132, 167, 192, 227, 62, 90, 26, 116, 29, 183, 200, 69, 20, 58, 105, 161, 233, 31, 208, 229, 73, 82, 12, 134, 194, 209, 126, 85, 228, 95, 131, 72, 225, 158, 149, 193, 107, 172, 182, 180, 218, 177, 174, 68, 186, 156, 38, 103, 75, 145, 235, 187]
[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]
[0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 2 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.618
[ 2 / 40 ] loss: 0.614
[ 3 / 40 ] loss: 0.534
[ 4 / 40 ] loss: 0.482
[ 5 / 40 ] loss: 0.454
[ 6 / 40 ] loss: 0.536
[ 7 / 40 ] loss: 0.479
[ 8 / 40 ] loss: 0.654
[ 9 / 40 ] loss: 0.602
[ 10 / 40 ] loss: 0.593
[ 11 / 40 ] loss: 0.449
[ 12 / 40 ] loss: 0.688
[ 13 / 40 ] loss: 0.749
[ 14 / 40 ] loss: 0.892
[ 15 / 40 ] loss: 0.676
[ 16 / 40 ] loss: 0.918
[ 17 / 40 ] loss: 0.623
[ 18 / 40 ] loss: 0.660
[ 19 / 40 ] loss: 0.539
[ 20 / 40 ] loss: 0.665
[ 21 / 40 ] loss: 0.574
[ 22 / 40 ] loss: 0.489
[ 23 / 40 ] loss: 0.768
[ 24 / 40 ] loss: 0.669
[ 25 / 40 ] loss: 0.475
[ 26 / 40 ] loss: 0.674
[ 27 / 40 ] loss: 0.552
[ 28 / 40 ] loss: 0.526
[ 29 / 40 ] loss: 0.562
[ 30 / 40 ] loss: 0.607
[ 31 / 40 ] loss: 0.572
[ 32 / 40 ] loss: 0.592
[ 33 / 40 ] loss: 0.557
[ 34 / 40 ] loss: 0.541
[ 35 / 40 ] loss: 0.595
[ 36 / 40 ] loss: 0.472
[ 37 / 40 ] loss: 0.547
[ 38 / 40 ] loss: 0.749
[ 39 / 40 ] loss: 0.556
[ 40 / 40 ] loss: 0.562
0.6015928462147713
Accuracy: 0.654008 -- Precision: 0.671429 -- Recall: 0.723077 -- F1: 0.696296 -- AUC: 0.707045
========= epoch: 3 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.561
[ 2 / 40 ] loss: 0.565
[ 3 / 40 ] loss: 0.516
[ 4 / 40 ] loss: 0.575
[ 5 / 40 ] loss: 0.475
[ 6 / 40 ] loss: 0.542
[ 7 / 40 ] loss: 0.405
[ 8 / 40 ] loss: 0.506
[ 9 / 40 ] loss: 0.530
[ 10 / 40 ] loss: 0.476
[ 11 / 40 ] loss: 0.564
[ 12 / 40 ] loss: 0.551
[ 13 / 40 ] loss: 0.588
[ 14 / 40 ] loss: 0.667
[ 15 / 40 ] loss: 0.636
[ 16 / 40 ] loss: 0.483
[ 17 / 40 ] loss: 0.490
[ 18 / 40 ] loss: 0.705
[ 19 / 40 ] loss: 0.430
[ 20 / 40 ] loss: 0.586
[ 21 / 40 ] loss: 0.414
[ 22 / 40 ] loss: 0.474
[ 23 / 40 ] loss: 0.397
[ 24 / 40 ] loss: 0.428
[ 25 / 40 ] loss: 0.447
[ 26 / 40 ] loss: 0.467
[ 27 / 40 ] loss: 0.428
[ 28 / 40 ] loss: 0.381
[ 29 / 40 ] loss: 0.459
[ 30 / 40 ] loss: 0.450
[ 31 / 40 ] loss: 0.398
[ 32 / 40 ] loss: 0.661
[ 33 / 40 ] loss: 0.530
[ 34 / 40 ] loss: 0.481
[ 35 / 40 ] loss: 0.524
[ 36 / 40 ] loss: 0.494
[ 37 / 40 ] loss: 0.521
[ 38 / 40 ] loss: 0.433
[ 39 / 40 ] loss: 0.593
[ 40 / 40 ] loss: 0.308
0.5034508116543293
Accuracy: 0.696203 -- Precision: 0.713235 -- Recall: 0.746154 -- F1: 0.729323 -- AUC: 0.765421
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[50, 86, 53, 89, 216, 159, 71, 65, 66, 166, 52, 172, 51, 197, 32, 110, 236, 225, 60, 124, 200, 93, 23, 100, 156, 103, 131, 162, 185, 82, 29, 202, 140, 123, 177, 13, 39, 141, 128, 24, 113, 233, 134, 83, 176, 207, 88, 4, 109, 95, 184, 139, 230, 136, 179, 94, 147, 49, 102, 126, 223, 54, 98, 130, 133, 92, 15, 45, 38, 9, 213, 161, 160, 157, 12, 21, 190, 31, 87, 183, 210, 235, 76, 5, 108, 198, 79, 106, 14, 25, 168, 192, 97, 217, 137, 138, 84, 1, 107, 58, 11, 46, 158, 10, 189, 114, 232, 104, 237, 173, 62, 34, 180, 167, 116, 218, 208, 56, 125, 203, 227, 195, 61, 81, 222, 146, 142, 182, 122, 8, 231, 85, 77, 72, 120, 204, 73, 35, 220, 196, 149, 211, 78, 163, 63, 175, 151, 165, 111, 135, 37, 119, 105, 174, 74, 19, 219, 7, 117, 129, 144, 155, 127, 115, 206, 199, 148, 214, 171, 191, 44, 194, 215, 221, 150, 47, 33, 143, 69, 188, 96, 187, 67, 43, 2, 186, 70, 57, 201, 6, 181, 22, 16, 145, 205, 169, 101, 112, 153, 27, 234, 17, 152, 28, 229, 90, 68, 42, 64, 121, 41, 20, 30, 164, 178, 118, 3, 224, 170, 48, 228, 59, 55, 212, 75, 99, 36, 193, 209, 18, 226, 132, 154, 26, 40, 91, 80]
[0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0]
[0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 4 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.351
[ 2 / 40 ] loss: 0.612
[ 3 / 40 ] loss: 0.342
[ 4 / 40 ] loss: 0.547
[ 5 / 40 ] loss: 0.288
[ 6 / 40 ] loss: 0.301
[ 7 / 40 ] loss: 0.347
[ 8 / 40 ] loss: 0.421
[ 9 / 40 ] loss: 0.603
[ 10 / 40 ] loss: 0.337
[ 11 / 40 ] loss: 0.173
[ 12 / 40 ] loss: 0.300
[ 13 / 40 ] loss: 0.438
[ 14 / 40 ] loss: 0.447
[ 15 / 40 ] loss: 0.377
[ 16 / 40 ] loss: 0.495
[ 17 / 40 ] loss: 0.579
[ 18 / 40 ] loss: 0.383
[ 19 / 40 ] loss: 0.378
[ 20 / 40 ] loss: 0.481
[ 21 / 40 ] loss: 0.370
[ 22 / 40 ] loss: 0.429
[ 23 / 40 ] loss: 0.334
[ 24 / 40 ] loss: 0.485
[ 25 / 40 ] loss: 0.343
[ 26 / 40 ] loss: 0.310
[ 27 / 40 ] loss: 0.270
[ 28 / 40 ] loss: 0.482
[ 29 / 40 ] loss: 0.383
[ 30 / 40 ] loss: 0.499
[ 31 / 40 ] loss: 0.426
[ 32 / 40 ] loss: 0.434
[ 33 / 40 ] loss: 0.488
[ 34 / 40 ] loss: 0.354
[ 35 / 40 ] loss: 0.421
[ 36 / 40 ] loss: 0.458
[ 37 / 40 ] loss: 0.405
[ 38 / 40 ] loss: 0.672
[ 39 / 40 ] loss: 0.412
[ 40 / 40 ] loss: 0.386
0.4139807842671871
Accuracy: 0.590717 -- Precision: 0.595376 -- Recall: 0.792308 -- F1: 0.679868 -- AUC: 0.706470
========= epoch: 5 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.314
[ 2 / 40 ] loss: 0.287
[ 3 / 40 ] loss: 0.201
[ 4 / 40 ] loss: 0.338
[ 5 / 40 ] loss: 0.394
[ 6 / 40 ] loss: 0.290
[ 7 / 40 ] loss: 0.264
[ 8 / 40 ] loss: 0.505
[ 9 / 40 ] loss: 0.253
[ 10 / 40 ] loss: 0.266
[ 11 / 40 ] loss: 0.198
[ 12 / 40 ] loss: 0.217
[ 13 / 40 ] loss: 0.288
[ 14 / 40 ] loss: 0.567
[ 15 / 40 ] loss: 0.190
[ 16 / 40 ] loss: 0.140
[ 17 / 40 ] loss: 0.391
[ 18 / 40 ] loss: 0.519
[ 19 / 40 ] loss: 0.201
[ 20 / 40 ] loss: 0.540
[ 21 / 40 ] loss: 0.555
[ 22 / 40 ] loss: 0.478
[ 23 / 40 ] loss: 0.387
[ 24 / 40 ] loss: 0.430
[ 25 / 40 ] loss: 0.256
[ 26 / 40 ] loss: 0.289
[ 27 / 40 ] loss: 0.116
[ 28 / 40 ] loss: 0.326
[ 29 / 40 ] loss: 0.502
[ 30 / 40 ] loss: 0.568
[ 31 / 40 ] loss: 0.420
[ 32 / 40 ] loss: 0.523
[ 33 / 40 ] loss: 0.305
[ 34 / 40 ] loss: 0.426
[ 35 / 40 ] loss: 0.350
[ 36 / 40 ] loss: 0.203
[ 37 / 40 ] loss: 0.327
[ 38 / 40 ] loss: 0.274
[ 39 / 40 ] loss: 0.365
[ 40 / 40 ] loss: 0.474
0.3484066095203161
Accuracy: 0.725738 -- Precision: 0.760000 -- Recall: 0.730769 -- F1: 0.745098 -- AUC: 0.802588
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[52, 60, 135, 93, 64, 127, 146, 227, 44, 98, 88, 112, 181, 136, 39, 73, 94, 207, 43, 160, 164, 144, 190, 68, 124, 34, 83, 41, 117, 134, 139, 185, 114, 205, 103, 66, 81, 111, 12, 13, 220, 133, 20, 101, 184, 149, 75, 161, 145, 7, 18, 62, 51, 147, 74, 106, 150, 209, 65, 40, 91, 8, 235, 102, 49, 119, 171, 125, 54, 19, 199, 26, 232, 128, 61, 87, 22, 86, 167, 228, 56, 198, 126, 105, 5, 189, 96, 72, 10, 142, 237, 162, 28, 118, 225, 148, 216, 59, 47, 46, 109, 224, 177, 191, 48, 204, 1, 182, 143, 95, 67, 222, 137, 6, 29, 130, 78, 194, 53, 108, 2, 178, 183, 173, 71, 168, 55, 187, 121, 89, 195, 202, 3, 69, 79, 16, 42, 169, 50, 172, 100, 123, 9, 138, 140, 63, 21, 84, 151, 45, 30, 153, 230, 163, 37, 99, 132, 115, 179, 231, 212, 233, 57, 175, 211, 155, 122, 77, 92, 27, 38, 97, 158, 15, 174, 180, 154, 131, 58, 116, 107, 70, 152, 208, 219, 210, 80, 113, 192, 110, 196, 32, 193, 157, 120, 129, 165, 23, 36, 4, 234, 206, 186, 104, 197, 166, 156, 24, 11, 213, 85, 226, 217, 176, 35, 200, 170, 188, 82, 215, 218, 33, 141, 214, 229, 76, 14, 201, 203, 31, 90, 25, 17, 221, 159, 223, 236]
[1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]
[0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 6 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.352
[ 2 / 40 ] loss: 0.535
[ 3 / 40 ] loss: 0.457
[ 4 / 40 ] loss: 0.267
[ 5 / 40 ] loss: 0.267
[ 6 / 40 ] loss: 0.216
[ 7 / 40 ] loss: 0.260
[ 8 / 40 ] loss: 0.171
[ 9 / 40 ] loss: 0.111
[ 10 / 40 ] loss: 0.203
[ 11 / 40 ] loss: 0.118
[ 12 / 40 ] loss: 0.234
[ 13 / 40 ] loss: 0.197
[ 14 / 40 ] loss: 0.077
[ 15 / 40 ] loss: 0.231
[ 16 / 40 ] loss: 0.204
[ 17 / 40 ] loss: 0.421
[ 18 / 40 ] loss: 0.341
[ 19 / 40 ] loss: 0.444
[ 20 / 40 ] loss: 0.144
[ 21 / 40 ] loss: 0.366
[ 22 / 40 ] loss: 0.189
[ 23 / 40 ] loss: 0.403
[ 24 / 40 ] loss: 0.126
[ 25 / 40 ] loss: 0.345
[ 26 / 40 ] loss: 0.534
[ 27 / 40 ] loss: 0.394
[ 28 / 40 ] loss: 0.172
[ 29 / 40 ] loss: 0.121
[ 30 / 40 ] loss: 0.231
[ 31 / 40 ] loss: 0.100
[ 32 / 40 ] loss: 0.251
[ 33 / 40 ] loss: 0.270
[ 34 / 40 ] loss: 0.291
[ 35 / 40 ] loss: 0.359
[ 36 / 40 ] loss: 0.270
[ 37 / 40 ] loss: 0.363
[ 38 / 40 ] loss: 0.266
[ 39 / 40 ] loss: 0.339
[ 40 / 40 ] loss: 0.421
0.2765521839261055
Accuracy: 0.704641 -- Precision: 0.812500 -- Recall: 0.600000 -- F1: 0.690265 -- AUC: 0.804457
========= epoch: 7 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.098
[ 2 / 40 ] loss: 0.375
[ 3 / 40 ] loss: 0.251
[ 4 / 40 ] loss: 0.249
[ 5 / 40 ] loss: 0.187
[ 6 / 40 ] loss: 0.217
[ 7 / 40 ] loss: 0.221
[ 8 / 40 ] loss: 0.247
[ 9 / 40 ] loss: 0.112
[ 10 / 40 ] loss: 0.417
[ 11 / 40 ] loss: 0.312
[ 12 / 40 ] loss: 0.098
[ 13 / 40 ] loss: 0.158
[ 14 / 40 ] loss: 0.168
[ 15 / 40 ] loss: 0.290
[ 16 / 40 ] loss: 0.061
[ 17 / 40 ] loss: 0.385
[ 18 / 40 ] loss: 0.213
[ 19 / 40 ] loss: 0.241
[ 20 / 40 ] loss: 0.340
[ 21 / 40 ] loss: 0.292
[ 22 / 40 ] loss: 0.154
[ 23 / 40 ] loss: 0.265
[ 24 / 40 ] loss: 0.258
[ 25 / 40 ] loss: 0.076
[ 26 / 40 ] loss: 0.441
[ 27 / 40 ] loss: 0.405
[ 28 / 40 ] loss: 0.166
[ 29 / 40 ] loss: 0.322
[ 30 / 40 ] loss: 0.297
[ 31 / 40 ] loss: 0.321
[ 32 / 40 ] loss: 0.180
[ 33 / 40 ] loss: 0.151
[ 34 / 40 ] loss: 0.314
[ 35 / 40 ] loss: 0.567
[ 36 / 40 ] loss: 0.331
[ 37 / 40 ] loss: 0.244
[ 38 / 40 ] loss: 0.400
[ 39 / 40 ] loss: 0.272
[ 40 / 40 ] loss: 0.216
0.2578563184477389
Accuracy: 0.683544 -- Precision: 0.684564 -- Recall: 0.784615 -- F1: 0.731183 -- AUC: 0.747376
========= epoch: 8 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.101
[ 2 / 40 ] loss: 0.169
[ 3 / 40 ] loss: 0.249
[ 4 / 40 ] loss: 0.259
[ 5 / 40 ] loss: 0.326
[ 6 / 40 ] loss: 0.132
[ 7 / 40 ] loss: 0.257
[ 8 / 40 ] loss: 0.189
[ 9 / 40 ] loss: 0.080
[ 10 / 40 ] loss: 0.052
[ 11 / 40 ] loss: 0.237
[ 12 / 40 ] loss: 0.210
[ 13 / 40 ] loss: 0.129
[ 14 / 40 ] loss: 0.163
[ 15 / 40 ] loss: 0.097
[ 16 / 40 ] loss: 0.255
[ 17 / 40 ] loss: 0.060
[ 18 / 40 ] loss: 0.199
[ 19 / 40 ] loss: 0.056
[ 20 / 40 ] loss: 0.135
[ 21 / 40 ] loss: 0.051
[ 22 / 40 ] loss: 0.279
[ 23 / 40 ] loss: 0.377
[ 24 / 40 ] loss: 0.463
[ 25 / 40 ] loss: 0.315
[ 26 / 40 ] loss: 0.579
[ 27 / 40 ] loss: 0.060
[ 28 / 40 ] loss: 0.280
[ 29 / 40 ] loss: 0.592
[ 30 / 40 ] loss: 0.528
[ 31 / 40 ] loss: 0.211
[ 32 / 40 ] loss: 0.198
[ 33 / 40 ] loss: 0.439
[ 34 / 40 ] loss: 0.279
[ 35 / 40 ] loss: 0.300
[ 36 / 40 ] loss: 0.578
[ 37 / 40 ] loss: 0.517
[ 38 / 40 ] loss: 0.234
[ 39 / 40 ] loss: 0.446
[ 40 / 40 ] loss: 0.280
0.258974190056324
Accuracy: 0.704641 -- Precision: 0.714286 -- Recall: 0.769231 -- F1: 0.740741 -- AUC: 0.771963
========= epoch: 9 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.475
[ 2 / 40 ] loss: 0.202
[ 3 / 40 ] loss: 0.169
[ 4 / 40 ] loss: 0.403
[ 5 / 40 ] loss: 0.134
[ 6 / 40 ] loss: 0.137
[ 7 / 40 ] loss: 0.320
[ 8 / 40 ] loss: 0.459
[ 9 / 40 ] loss: 0.239
[ 10 / 40 ] loss: 0.219
[ 11 / 40 ] loss: 0.318
[ 12 / 40 ] loss: 0.075
[ 13 / 40 ] loss: 0.265
[ 14 / 40 ] loss: 0.445
[ 15 / 40 ] loss: 0.178
[ 16 / 40 ] loss: 0.122
[ 17 / 40 ] loss: 0.136
[ 18 / 40 ] loss: 0.094
[ 19 / 40 ] loss: 0.144
[ 20 / 40 ] loss: 0.213
[ 21 / 40 ] loss: 0.378
[ 22 / 40 ] loss: 0.349
[ 23 / 40 ] loss: 0.182
[ 24 / 40 ] loss: 0.160
[ 25 / 40 ] loss: 0.452
[ 26 / 40 ] loss: 0.179
[ 27 / 40 ] loss: 0.269
[ 28 / 40 ] loss: 0.104
[ 29 / 40 ] loss: 0.141
[ 30 / 40 ] loss: 0.066
[ 31 / 40 ] loss: 0.189
[ 32 / 40 ] loss: 0.073
[ 33 / 40 ] loss: 0.172
[ 34 / 40 ] loss: 0.089
[ 35 / 40 ] loss: 0.111
[ 36 / 40 ] loss: 0.226
[ 37 / 40 ] loss: 0.164
[ 38 / 40 ] loss: 0.149
[ 39 / 40 ] loss: 0.040
[ 40 / 40 ] loss: 0.147
0.20961745819076896
Accuracy: 0.721519 -- Precision: 0.780702 -- Recall: 0.684615 -- F1: 0.729508 -- AUC: 0.770740
========= epoch: 10 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.088
[ 2 / 40 ] loss: 0.058
[ 3 / 40 ] loss: 0.285
[ 4 / 40 ] loss: 0.163
[ 5 / 40 ] loss: 0.194
[ 6 / 40 ] loss: 0.300
[ 7 / 40 ] loss: 0.029
[ 8 / 40 ] loss: 0.081
[ 9 / 40 ] loss: 0.055
[ 10 / 40 ] loss: 0.128
[ 11 / 40 ] loss: 0.239
[ 12 / 40 ] loss: 0.275
[ 13 / 40 ] loss: 0.124
[ 14 / 40 ] loss: 0.073
[ 15 / 40 ] loss: 0.119
[ 16 / 40 ] loss: 0.531
[ 17 / 40 ] loss: 0.135
[ 18 / 40 ] loss: 0.201
[ 19 / 40 ] loss: 0.089
[ 20 / 40 ] loss: 0.146
[ 21 / 40 ] loss: 0.114
[ 22 / 40 ] loss: 0.140
[ 23 / 40 ] loss: 0.045
[ 24 / 40 ] loss: 0.112
[ 25 / 40 ] loss: 0.192
[ 26 / 40 ] loss: 0.292
[ 27 / 40 ] loss: 0.253
[ 28 / 40 ] loss: 0.236
[ 29 / 40 ] loss: 0.408
[ 30 / 40 ] loss: 0.474
[ 31 / 40 ] loss: 0.570
[ 32 / 40 ] loss: 0.075
[ 33 / 40 ] loss: 0.381
[ 34 / 40 ] loss: 0.250
[ 35 / 40 ] loss: 0.232
[ 36 / 40 ] loss: 0.200
[ 37 / 40 ] loss: 0.093
[ 38 / 40 ] loss: 0.063
[ 39 / 40 ] loss: 0.100
[ 40 / 40 ] loss: 0.094
0.19102656035684049
Accuracy: 0.729958 -- Precision: 0.775000 -- Recall: 0.715385 -- F1: 0.744000 -- AUC: 0.788497
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[170, 94, 87, 65, 197, 162, 202, 210, 164, 176, 41, 69, 24, 224, 140, 79, 227, 100, 158, 134, 23, 75, 156, 184, 27, 13, 133, 10, 205, 204, 111, 54, 180, 150, 2, 191, 203, 26, 114, 232, 122, 70, 108, 193, 112, 90, 30, 104, 97, 80, 1, 71, 135, 138, 187, 152, 113, 9, 175, 192, 51, 102, 84, 49, 230, 236, 67, 66, 207, 61, 8, 147, 226, 223, 208, 171, 55, 194, 64, 73, 110, 132, 59, 159, 68, 3, 58, 32, 129, 47, 109, 189, 225, 91, 235, 37, 160, 190, 234, 149, 106, 22, 57, 141, 60, 81, 14, 25, 56, 123, 120, 155, 174, 130, 169, 105, 116, 98, 222, 74, 182, 117, 218, 237, 76, 77, 142, 35, 99, 63, 153, 165, 183, 11, 96, 119, 186, 188, 103, 7, 215, 88, 185, 85, 34, 89, 221, 15, 78, 33, 179, 118, 39, 46, 214, 92, 36, 42, 86, 228, 137, 52, 31, 44, 233, 127, 168, 107, 126, 18, 45, 163, 178, 172, 148, 16, 136, 50, 28, 38, 195, 212, 206, 220, 231, 19, 157, 12, 125, 167, 124, 115, 181, 83, 166, 40, 4, 29, 199, 200, 48, 62, 216, 72, 82, 144, 43, 229, 146, 53, 198, 21, 6, 131, 154, 217, 128, 161, 20, 145, 211, 101, 219, 151, 213, 196, 93, 95, 5, 121, 201, 177, 143, 17, 209, 139, 173]
[0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0]
[1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 11 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.211
[ 2 / 40 ] loss: 0.285
[ 3 / 40 ] loss: 0.083
[ 4 / 40 ] loss: 0.083
[ 5 / 40 ] loss: 0.123
[ 6 / 40 ] loss: 0.106
[ 7 / 40 ] loss: 0.189
[ 8 / 40 ] loss: 0.277
[ 9 / 40 ] loss: 0.029
[ 10 / 40 ] loss: 0.186
[ 11 / 40 ] loss: 0.133
[ 12 / 40 ] loss: 0.174
[ 13 / 40 ] loss: 0.203
[ 14 / 40 ] loss: 0.225
[ 15 / 40 ] loss: 0.065
[ 16 / 40 ] loss: 0.263
[ 17 / 40 ] loss: 0.309
[ 18 / 40 ] loss: 0.209
[ 19 / 40 ] loss: 0.208
[ 20 / 40 ] loss: 0.345
[ 21 / 40 ] loss: 0.163
[ 22 / 40 ] loss: 0.071
[ 23 / 40 ] loss: 0.369
[ 24 / 40 ] loss: 0.075
[ 25 / 40 ] loss: 0.073
[ 26 / 40 ] loss: 0.071
[ 27 / 40 ] loss: 0.299
[ 28 / 40 ] loss: 0.036
[ 29 / 40 ] loss: 0.120
[ 30 / 40 ] loss: 0.125
[ 31 / 40 ] loss: 0.245
[ 32 / 40 ] loss: 0.108
[ 33 / 40 ] loss: 0.196
[ 34 / 40 ] loss: 0.052
[ 35 / 40 ] loss: 0.367
[ 36 / 40 ] loss: 0.151
[ 37 / 40 ] loss: 0.049
[ 38 / 40 ] loss: 0.098
[ 39 / 40 ] loss: 0.348
[ 40 / 40 ] loss: 0.337
0.1765393025241792
Accuracy: 0.734177 -- Precision: 0.744526 -- Recall: 0.784615 -- F1: 0.764045 -- AUC: 0.791661
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[118, 201, 92, 139, 208, 218, 73, 76, 23, 111, 110, 151, 126, 233, 112, 9, 158, 63, 56, 69, 138, 172, 164, 38, 59, 175, 207, 84, 64, 29, 3, 20, 95, 79, 148, 153, 60, 81, 89, 230, 143, 183, 72, 32, 234, 167, 195, 97, 1, 46, 109, 82, 42, 5, 85, 2, 211, 179, 137, 146, 55, 149, 18, 90, 222, 106, 221, 217, 70, 228, 128, 67, 163, 140, 205, 220, 216, 155, 21, 160, 94, 187, 162, 176, 231, 107, 93, 202, 235, 12, 86, 88, 226, 122, 132, 17, 124, 173, 184, 119, 49, 190, 51, 8, 121, 156, 53, 31, 225, 68, 196, 13, 16, 177, 168, 25, 123, 39, 41, 127, 131, 114, 36, 125, 189, 192, 185, 159, 182, 75, 61, 117, 22, 170, 34, 157, 136, 6, 14, 57, 186, 7, 180, 35, 152, 200, 52, 15, 101, 210, 145, 169, 96, 78, 65, 44, 11, 40, 33, 19, 80, 237, 37, 108, 150, 83, 197, 193, 120, 141, 171, 10, 129, 30, 227, 130, 199, 135, 87, 24, 27, 50, 115, 28, 66, 103, 54, 213, 188, 100, 91, 181, 48, 219, 224, 26, 198, 204, 104, 45, 229, 191, 209, 161, 113, 165, 116, 236, 144, 43, 154, 74, 223, 105, 47, 134, 178, 77, 62, 98, 206, 215, 147, 194, 133, 214, 142, 203, 174, 102, 212, 58, 4, 99, 166, 232, 71]
[1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]
[1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 12 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.051
[ 2 / 40 ] loss: 0.235
[ 3 / 40 ] loss: 0.189
[ 4 / 40 ] loss: 0.233
[ 5 / 40 ] loss: 0.374
[ 6 / 40 ] loss: 0.450
[ 7 / 40 ] loss: 0.166
[ 8 / 40 ] loss: 0.089
[ 9 / 40 ] loss: 0.081
[ 10 / 40 ] loss: 0.039
[ 11 / 40 ] loss: 0.071
[ 12 / 40 ] loss: 0.083
[ 13 / 40 ] loss: 0.122
[ 14 / 40 ] loss: 0.052
[ 15 / 40 ] loss: 0.383
[ 16 / 40 ] loss: 0.046
[ 17 / 40 ] loss: 0.037
[ 18 / 40 ] loss: 0.175
[ 19 / 40 ] loss: 0.127
[ 20 / 40 ] loss: 0.318
[ 21 / 40 ] loss: 0.091
[ 22 / 40 ] loss: 0.031
[ 23 / 40 ] loss: 0.184
[ 24 / 40 ] loss: 0.194
[ 25 / 40 ] loss: 0.069
[ 26 / 40 ] loss: 0.130
[ 27 / 40 ] loss: 0.062
[ 28 / 40 ] loss: 0.170
[ 29 / 40 ] loss: 0.214
[ 30 / 40 ] loss: 0.442
[ 31 / 40 ] loss: 0.071
[ 32 / 40 ] loss: 0.145
[ 33 / 40 ] loss: 0.210
[ 34 / 40 ] loss: 0.089
[ 35 / 40 ] loss: 0.098
[ 36 / 40 ] loss: 0.117
[ 37 / 40 ] loss: 0.321
[ 38 / 40 ] loss: 0.135
[ 39 / 40 ] loss: 0.062
[ 40 / 40 ] loss: 0.288
0.16108873933553697
Accuracy: 0.742616 -- Precision: 0.771654 -- Recall: 0.753846 -- F1: 0.762646 -- AUC: 0.792308
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[160, 79, 173, 3, 196, 47, 59, 174, 88, 49, 105, 147, 101, 7, 167, 45, 31, 153, 29, 151, 175, 14, 141, 30, 204, 155, 106, 67, 12, 55, 140, 18, 53, 63, 144, 172, 135, 117, 228, 143, 102, 205, 75, 27, 62, 206, 94, 170, 136, 134, 125, 38, 87, 93, 51, 83, 20, 184, 97, 16, 202, 108, 8, 111, 113, 211, 118, 138, 163, 185, 220, 64, 148, 100, 80, 70, 74, 161, 60, 233, 213, 230, 77, 121, 35, 169, 37, 13, 40, 41, 24, 179, 156, 109, 152, 2, 65, 115, 68, 86, 42, 78, 85, 99, 96, 73, 198, 226, 61, 54, 180, 133, 208, 127, 222, 91, 186, 44, 207, 178, 19, 116, 171, 32, 57, 52, 221, 119, 177, 10, 188, 183, 195, 150, 139, 137, 234, 89, 158, 176, 217, 122, 197, 9, 157, 235, 22, 227, 159, 110, 187, 71, 182, 237, 165, 199, 26, 82, 103, 112, 232, 126, 25, 90, 212, 46, 98, 146, 209, 5, 6, 166, 191, 81, 229, 225, 124, 168, 236, 4, 11, 23, 69, 214, 15, 28, 48, 215, 181, 149, 219, 223, 224, 131, 201, 164, 193, 114, 72, 95, 162, 189, 50, 190, 142, 84, 1, 123, 104, 92, 36, 129, 218, 34, 200, 231, 43, 194, 33, 21, 66, 17, 192, 132, 39, 128, 145, 210, 120, 107, 130, 76, 58, 203, 154, 216, 56]
[1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]
[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 13 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.053
[ 2 / 40 ] loss: 0.040
[ 3 / 40 ] loss: 0.347
[ 4 / 40 ] loss: 0.065
[ 5 / 40 ] loss: 0.037
[ 6 / 40 ] loss: 0.433
[ 7 / 40 ] loss: 0.115
[ 8 / 40 ] loss: 0.054
[ 9 / 40 ] loss: 0.165
[ 10 / 40 ] loss: 0.098
[ 11 / 40 ] loss: 0.058
[ 12 / 40 ] loss: 0.084
[ 13 / 40 ] loss: 0.289
[ 14 / 40 ] loss: 0.058
[ 15 / 40 ] loss: 0.406
[ 16 / 40 ] loss: 0.029
[ 17 / 40 ] loss: 0.028
[ 18 / 40 ] loss: 0.143
[ 19 / 40 ] loss: 0.164
[ 20 / 40 ] loss: 0.208
[ 21 / 40 ] loss: 0.121
[ 22 / 40 ] loss: 0.179
[ 23 / 40 ] loss: 0.121
[ 24 / 40 ] loss: 0.112
[ 25 / 40 ] loss: 0.168
[ 26 / 40 ] loss: 0.094
[ 27 / 40 ] loss: 0.113
[ 28 / 40 ] loss: 0.171
[ 29 / 40 ] loss: 0.195
[ 30 / 40 ] loss: 0.086
[ 31 / 40 ] loss: 0.479
[ 32 / 40 ] loss: 0.064
[ 33 / 40 ] loss: 0.129
[ 34 / 40 ] loss: 0.080
[ 35 / 40 ] loss: 0.240
[ 36 / 40 ] loss: 0.275
[ 37 / 40 ] loss: 0.178
[ 38 / 40 ] loss: 0.047
[ 39 / 40 ] loss: 0.046
[ 40 / 40 ] loss: 0.463
0.15587986689060926
Accuracy: 0.717300 -- Precision: 0.726619 -- Recall: 0.776923 -- F1: 0.750929 -- AUC: 0.777930
========= epoch: 14 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.139
[ 2 / 40 ] loss: 0.068
[ 3 / 40 ] loss: 0.230
[ 4 / 40 ] loss: 0.065
[ 5 / 40 ] loss: 0.282
[ 6 / 40 ] loss: 0.131
[ 7 / 40 ] loss: 0.111
[ 8 / 40 ] loss: 0.047
[ 9 / 40 ] loss: 0.254
[ 10 / 40 ] loss: 0.254
[ 11 / 40 ] loss: 0.065
[ 12 / 40 ] loss: 0.137
[ 13 / 40 ] loss: 0.190
[ 14 / 40 ] loss: 0.091
[ 15 / 40 ] loss: 0.058
[ 16 / 40 ] loss: 0.018
[ 17 / 40 ] loss: 0.022
[ 18 / 40 ] loss: 0.051
[ 19 / 40 ] loss: 0.110
[ 20 / 40 ] loss: 0.122
[ 21 / 40 ] loss: 0.022
[ 22 / 40 ] loss: 0.115
[ 23 / 40 ] loss: 0.162
[ 24 / 40 ] loss: 0.113
[ 25 / 40 ] loss: 0.012
[ 26 / 40 ] loss: 0.035
[ 27 / 40 ] loss: 0.162
[ 28 / 40 ] loss: 0.274
[ 29 / 40 ] loss: 0.015
[ 30 / 40 ] loss: 0.093
[ 31 / 40 ] loss: 0.131
[ 32 / 40 ] loss: 0.317
[ 33 / 40 ] loss: 0.226
[ 34 / 40 ] loss: 0.093
[ 35 / 40 ] loss: 0.245
[ 36 / 40 ] loss: 0.211
[ 37 / 40 ] loss: 0.291
[ 38 / 40 ] loss: 0.079
[ 39 / 40 ] loss: 0.246
[ 40 / 40 ] loss: 0.229
0.13797343550249935
Accuracy: 0.734177 -- Precision: 0.772358 -- Recall: 0.730769 -- F1: 0.750988 -- AUC: 0.771100
========= epoch: 15 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.103
[ 2 / 40 ] loss: 0.068
[ 3 / 40 ] loss: 0.113
[ 4 / 40 ] loss: 0.083
[ 5 / 40 ] loss: 0.065
[ 6 / 40 ] loss: 0.225
[ 7 / 40 ] loss: 0.057
[ 8 / 40 ] loss: 0.040
[ 9 / 40 ] loss: 0.069
[ 10 / 40 ] loss: 0.092
[ 11 / 40 ] loss: 0.048
[ 12 / 40 ] loss: 0.113
[ 13 / 40 ] loss: 0.036
[ 14 / 40 ] loss: 0.270
[ 15 / 40 ] loss: 0.066
[ 16 / 40 ] loss: 0.038
[ 17 / 40 ] loss: 0.052
[ 18 / 40 ] loss: 0.155
[ 19 / 40 ] loss: 0.349
[ 20 / 40 ] loss: 0.070
[ 21 / 40 ] loss: 0.029
[ 22 / 40 ] loss: 0.047
[ 23 / 40 ] loss: 0.048
[ 24 / 40 ] loss: 0.048
[ 25 / 40 ] loss: 0.076
[ 26 / 40 ] loss: 0.054
[ 27 / 40 ] loss: 0.056
[ 28 / 40 ] loss: 0.347
[ 29 / 40 ] loss: 0.109
[ 30 / 40 ] loss: 0.179
[ 31 / 40 ] loss: 0.410
[ 32 / 40 ] loss: 0.016
[ 33 / 40 ] loss: 0.042
[ 34 / 40 ] loss: 0.017
[ 35 / 40 ] loss: 0.025
[ 36 / 40 ] loss: 0.327
[ 37 / 40 ] loss: 0.381
[ 38 / 40 ] loss: 0.025
[ 39 / 40 ] loss: 0.146
[ 40 / 40 ] loss: 0.018
0.11279117930680513
Accuracy: 0.721519 -- Precision: 0.722222 -- Recall: 0.800000 -- F1: 0.759124 -- AUC: 0.752624
========= epoch: 16 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.043
[ 2 / 40 ] loss: 0.167
[ 3 / 40 ] loss: 0.189
[ 4 / 40 ] loss: 0.022
[ 5 / 40 ] loss: 0.059
[ 6 / 40 ] loss: 0.315
[ 7 / 40 ] loss: 0.015
[ 8 / 40 ] loss: 0.037
[ 9 / 40 ] loss: 0.063
[ 10 / 40 ] loss: 0.099
[ 11 / 40 ] loss: 0.213
[ 12 / 40 ] loss: 0.229
[ 13 / 40 ] loss: 0.196
[ 14 / 40 ] loss: 0.113
[ 15 / 40 ] loss: 0.196
[ 16 / 40 ] loss: 0.160
[ 17 / 40 ] loss: 0.120
[ 18 / 40 ] loss: 0.059
[ 19 / 40 ] loss: 0.023
[ 20 / 40 ] loss: 0.076
[ 21 / 40 ] loss: 0.072
[ 22 / 40 ] loss: 0.184
[ 23 / 40 ] loss: 0.047
[ 24 / 40 ] loss: 0.213
[ 25 / 40 ] loss: 0.036
[ 26 / 40 ] loss: 0.111
[ 27 / 40 ] loss: 0.093
[ 28 / 40 ] loss: 0.021
[ 29 / 40 ] loss: 0.142
[ 30 / 40 ] loss: 0.049
[ 31 / 40 ] loss: 0.113
[ 32 / 40 ] loss: 0.227
[ 33 / 40 ] loss: 0.104
[ 34 / 40 ] loss: 0.187
[ 35 / 40 ] loss: 0.040
[ 36 / 40 ] loss: 0.028
[ 37 / 40 ] loss: 0.027
[ 38 / 40 ] loss: 0.017
[ 39 / 40 ] loss: 0.046
[ 40 / 40 ] loss: 0.102
0.10639063597191126
Accuracy: 0.742616 -- Precision: 0.780488 -- Recall: 0.738462 -- F1: 0.758893 -- AUC: 0.758088
========= epoch: 17 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.217
[ 2 / 40 ] loss: 0.116
[ 3 / 40 ] loss: 0.195
[ 4 / 40 ] loss: 0.127
[ 5 / 40 ] loss: 0.032
[ 6 / 40 ] loss: 0.018
[ 7 / 40 ] loss: 0.025
[ 8 / 40 ] loss: 0.042
[ 9 / 40 ] loss: 0.040
[ 10 / 40 ] loss: 0.109
[ 11 / 40 ] loss: 0.019
[ 12 / 40 ] loss: 0.049
[ 13 / 40 ] loss: 0.102
[ 14 / 40 ] loss: 0.057
[ 15 / 40 ] loss: 0.092
[ 16 / 40 ] loss: 0.047
[ 17 / 40 ] loss: 0.013
[ 18 / 40 ] loss: 0.145
[ 19 / 40 ] loss: 0.038
[ 20 / 40 ] loss: 0.018
[ 21 / 40 ] loss: 0.141
[ 22 / 40 ] loss: 0.023
[ 23 / 40 ] loss: 0.020
[ 24 / 40 ] loss: 0.086
[ 25 / 40 ] loss: 0.020
[ 26 / 40 ] loss: 0.088
[ 27 / 40 ] loss: 0.032
[ 28 / 40 ] loss: 0.028
[ 29 / 40 ] loss: 0.077
[ 30 / 40 ] loss: 0.412
[ 31 / 40 ] loss: 0.040
[ 32 / 40 ] loss: 0.054
[ 33 / 40 ] loss: 0.140
[ 34 / 40 ] loss: 0.225
[ 35 / 40 ] loss: 0.052
[ 36 / 40 ] loss: 0.015
[ 37 / 40 ] loss: 0.022
[ 38 / 40 ] loss: 0.068
[ 39 / 40 ] loss: 0.216
[ 40 / 40 ] loss: 0.075
0.08333616729360074
Accuracy: 0.725738 -- Precision: 0.724138 -- Recall: 0.807692 -- F1: 0.763636 -- AUC: 0.739540
========= epoch: 18 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.062
[ 2 / 40 ] loss: 0.016
[ 3 / 40 ] loss: 0.238
[ 4 / 40 ] loss: 0.015
[ 5 / 40 ] loss: 0.014
[ 6 / 40 ] loss: 0.016
[ 7 / 40 ] loss: 0.011
[ 8 / 40 ] loss: 0.048
[ 9 / 40 ] loss: 0.222
[ 10 / 40 ] loss: 0.237
[ 11 / 40 ] loss: 0.059
[ 12 / 40 ] loss: 0.014
[ 13 / 40 ] loss: 0.022
[ 14 / 40 ] loss: 0.045
[ 15 / 40 ] loss: 0.073
[ 16 / 40 ] loss: 0.014
[ 17 / 40 ] loss: 0.030
[ 18 / 40 ] loss: 0.230
[ 19 / 40 ] loss: 0.030
[ 20 / 40 ] loss: 0.032
[ 21 / 40 ] loss: 0.062
[ 22 / 40 ] loss: 0.024
[ 23 / 40 ] loss: 0.015
[ 24 / 40 ] loss: 0.018
[ 25 / 40 ] loss: 0.020
[ 26 / 40 ] loss: 0.105
[ 27 / 40 ] loss: 0.032
[ 28 / 40 ] loss: 0.028
[ 29 / 40 ] loss: 0.011
[ 30 / 40 ] loss: 0.010
[ 31 / 40 ] loss: 0.045
[ 32 / 40 ] loss: 0.098
[ 33 / 40 ] loss: 0.056
[ 34 / 40 ] loss: 0.240
[ 35 / 40 ] loss: 0.047
[ 36 / 40 ] loss: 0.187
[ 37 / 40 ] loss: 0.153
[ 38 / 40 ] loss: 0.141
[ 39 / 40 ] loss: 0.014
[ 40 / 40 ] loss: 0.008
0.06857181338127702
Accuracy: 0.708861 -- Precision: 0.732824 -- Recall: 0.738462 -- F1: 0.735632 -- AUC: 0.749892
========= epoch: 19 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.030
[ 2 / 40 ] loss: 0.108
[ 3 / 40 ] loss: 0.010
[ 4 / 40 ] loss: 0.111
[ 5 / 40 ] loss: 0.009
[ 6 / 40 ] loss: 0.257
[ 7 / 40 ] loss: 0.068
[ 8 / 40 ] loss: 0.008
[ 9 / 40 ] loss: 0.008
[ 10 / 40 ] loss: 0.061
[ 11 / 40 ] loss: 0.009
[ 12 / 40 ] loss: 0.033
[ 13 / 40 ] loss: 0.014
[ 14 / 40 ] loss: 0.011
[ 15 / 40 ] loss: 0.094
[ 16 / 40 ] loss: 0.008
[ 17 / 40 ] loss: 0.040
[ 18 / 40 ] loss: 0.010
[ 19 / 40 ] loss: 0.008
[ 20 / 40 ] loss: 0.013
[ 21 / 40 ] loss: 0.299
[ 22 / 40 ] loss: 0.023
[ 23 / 40 ] loss: 0.221
[ 24 / 40 ] loss: 0.011
[ 25 / 40 ] loss: 0.187
[ 26 / 40 ] loss: 0.013
[ 27 / 40 ] loss: 0.039
[ 28 / 40 ] loss: 0.010
[ 29 / 40 ] loss: 0.360
[ 30 / 40 ] loss: 0.010
[ 31 / 40 ] loss: 0.260
[ 32 / 40 ] loss: 0.313
[ 33 / 40 ] loss: 0.012
[ 34 / 40 ] loss: 0.320
[ 35 / 40 ] loss: 0.061
[ 36 / 40 ] loss: 0.028
[ 37 / 40 ] loss: 0.026
[ 38 / 40 ] loss: 0.013
[ 39 / 40 ] loss: 0.042
[ 40 / 40 ] loss: 0.022
0.07957001535687595
Accuracy: 0.704641 -- Precision: 0.714286 -- Recall: 0.769231 -- F1: 0.740741 -- AUC: 0.757728
========= epoch: 20 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.021
[ 2 / 40 ] loss: 0.034
[ 3 / 40 ] loss: 0.038
[ 4 / 40 ] loss: 0.091
[ 5 / 40 ] loss: 0.206
[ 6 / 40 ] loss: 0.023
[ 7 / 40 ] loss: 0.012
[ 8 / 40 ] loss: 0.037
[ 9 / 40 ] loss: 0.027
[ 10 / 40 ] loss: 0.212
[ 11 / 40 ] loss: 0.033
[ 12 / 40 ] loss: 0.042
[ 13 / 40 ] loss: 0.082
[ 14 / 40 ] loss: 0.010
[ 15 / 40 ] loss: 0.011
[ 16 / 40 ] loss: 0.296
[ 17 / 40 ] loss: 0.013
[ 18 / 40 ] loss: 0.348
[ 19 / 40 ] loss: 0.015
[ 20 / 40 ] loss: 0.244
[ 21 / 40 ] loss: 0.016
[ 22 / 40 ] loss: 0.207
[ 23 / 40 ] loss: 0.157
[ 24 / 40 ] loss: 0.115
[ 25 / 40 ] loss: 0.027
[ 26 / 40 ] loss: 0.056
[ 27 / 40 ] loss: 0.154
[ 28 / 40 ] loss: 0.072
[ 29 / 40 ] loss: 0.056
[ 30 / 40 ] loss: 0.124
[ 31 / 40 ] loss: 0.261
[ 32 / 40 ] loss: 0.105
[ 33 / 40 ] loss: 0.040
[ 34 / 40 ] loss: 0.032
[ 35 / 40 ] loss: 0.172
[ 36 / 40 ] loss: 0.023
[ 37 / 40 ] loss: 0.027
[ 38 / 40 ] loss: 0.050
[ 39 / 40 ] loss: 0.015
[ 40 / 40 ] loss: 0.019
0.08808185157831758
Accuracy: 0.725738 -- Precision: 0.715232 -- Recall: 0.830769 -- F1: 0.768683 -- AUC: 0.735370
========= epoch: 21 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.130
[ 2 / 40 ] loss: 0.044
[ 3 / 40 ] loss: 0.062
[ 4 / 40 ] loss: 0.011
[ 5 / 40 ] loss: 0.018
[ 6 / 40 ] loss: 0.043
[ 7 / 40 ] loss: 0.011
[ 8 / 40 ] loss: 0.026
[ 9 / 40 ] loss: 0.020
[ 10 / 40 ] loss: 0.069
[ 11 / 40 ] loss: 0.206
[ 12 / 40 ] loss: 0.017
[ 13 / 40 ] loss: 0.335
[ 14 / 40 ] loss: 0.057
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.011
[ 17 / 40 ] loss: 0.033
[ 18 / 40 ] loss: 0.274
[ 19 / 40 ] loss: 0.014
[ 20 / 40 ] loss: 0.011
[ 21 / 40 ] loss: 0.022
[ 22 / 40 ] loss: 0.058
[ 23 / 40 ] loss: 0.060
[ 24 / 40 ] loss: 0.017
[ 25 / 40 ] loss: 0.030
[ 26 / 40 ] loss: 0.007
[ 27 / 40 ] loss: 0.341
[ 28 / 40 ] loss: 0.134
[ 29 / 40 ] loss: 0.008
[ 30 / 40 ] loss: 0.225
[ 31 / 40 ] loss: 0.145
[ 32 / 40 ] loss: 0.013
[ 33 / 40 ] loss: 0.007
[ 34 / 40 ] loss: 0.136
[ 35 / 40 ] loss: 0.171
[ 36 / 40 ] loss: 0.231
[ 37 / 40 ] loss: 0.038
[ 38 / 40 ] loss: 0.016
[ 39 / 40 ] loss: 0.080
[ 40 / 40 ] loss: 0.242
0.0846377071342431
Accuracy: 0.704641 -- Precision: 0.767857 -- Recall: 0.661538 -- F1: 0.710744 -- AUC: 0.744213
========= epoch: 22 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.117
[ 2 / 40 ] loss: 0.071
[ 3 / 40 ] loss: 0.059
[ 4 / 40 ] loss: 0.124
[ 5 / 40 ] loss: 0.008
[ 6 / 40 ] loss: 0.012
[ 7 / 40 ] loss: 0.100
[ 8 / 40 ] loss: 0.034
[ 9 / 40 ] loss: 0.026
[ 10 / 40 ] loss: 0.123
[ 11 / 40 ] loss: 0.010
[ 12 / 40 ] loss: 0.086
[ 13 / 40 ] loss: 0.013
[ 14 / 40 ] loss: 0.049
[ 15 / 40 ] loss: 0.176
[ 16 / 40 ] loss: 0.015
[ 17 / 40 ] loss: 0.065
[ 18 / 40 ] loss: 0.201
[ 19 / 40 ] loss: 0.051
[ 20 / 40 ] loss: 0.014
[ 21 / 40 ] loss: 0.274
[ 22 / 40 ] loss: 0.012
[ 23 / 40 ] loss: 0.019
[ 24 / 40 ] loss: 0.068
[ 25 / 40 ] loss: 0.273
[ 26 / 40 ] loss: 0.096
[ 27 / 40 ] loss: 0.083
[ 28 / 40 ] loss: 0.020
[ 29 / 40 ] loss: 0.047
[ 30 / 40 ] loss: 0.093
[ 31 / 40 ] loss: 0.197
[ 32 / 40 ] loss: 0.121
[ 33 / 40 ] loss: 0.030
[ 34 / 40 ] loss: 0.038
[ 35 / 40 ] loss: 0.044
[ 36 / 40 ] loss: 0.077
[ 37 / 40 ] loss: 0.055
[ 38 / 40 ] loss: 0.029
[ 39 / 40 ] loss: 0.096
[ 40 / 40 ] loss: 0.088
0.07780697904527187
Accuracy: 0.746835 -- Precision: 0.757353 -- Recall: 0.792308 -- F1: 0.774436 -- AUC: 0.784903
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[125, 198, 217, 185, 110, 147, 78, 64, 196, 16, 122, 174, 3, 129, 22, 224, 71, 197, 233, 80, 34, 54, 234, 181, 93, 117, 118, 87, 18, 183, 137, 104, 170, 157, 154, 160, 31, 69, 27, 33, 159, 144, 168, 232, 227, 164, 59, 209, 178, 25, 173, 23, 89, 19, 223, 120, 143, 231, 74, 206, 182, 158, 128, 35, 135, 62, 30, 106, 82, 43, 228, 75, 200, 213, 208, 175, 88, 145, 219, 98, 45, 1, 156, 63, 133, 210, 191, 83, 100, 73, 101, 94, 10, 40, 37, 97, 6, 79, 215, 237, 102, 134, 48, 192, 2, 32, 115, 139, 121, 49, 141, 163, 112, 142, 114, 151, 177, 68, 155, 124, 108, 8, 109, 116, 149, 176, 103, 91, 57, 221, 165, 222, 171, 204, 225, 24, 36, 169, 5, 230, 123, 212, 51, 105, 131, 47, 107, 136, 44, 211, 193, 4, 216, 189, 29, 72, 13, 172, 203, 111, 41, 15, 126, 207, 95, 85, 226, 152, 205, 96, 113, 166, 184, 214, 7, 180, 46, 187, 84, 67, 132, 12, 39, 167, 99, 20, 235, 161, 38, 201, 146, 70, 148, 60, 76, 66, 138, 81, 186, 26, 218, 179, 9, 42, 55, 153, 17, 188, 150, 58, 65, 220, 194, 236, 199, 21, 190, 14, 61, 140, 52, 56, 86, 28, 77, 162, 127, 50, 202, 195, 53, 130, 90, 92, 119, 11, 229]
[0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1]
[1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 23 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.218
[ 2 / 40 ] loss: 0.109
[ 3 / 40 ] loss: 0.012
[ 4 / 40 ] loss: 0.012
[ 5 / 40 ] loss: 0.016
[ 6 / 40 ] loss: 0.156
[ 7 / 40 ] loss: 0.022
[ 8 / 40 ] loss: 0.080
[ 9 / 40 ] loss: 0.054
[ 10 / 40 ] loss: 0.082
[ 11 / 40 ] loss: 0.024
[ 12 / 40 ] loss: 0.348
[ 13 / 40 ] loss: 0.010
[ 14 / 40 ] loss: 0.012
[ 15 / 40 ] loss: 0.028
[ 16 / 40 ] loss: 0.014
[ 17 / 40 ] loss: 0.085
[ 18 / 40 ] loss: 0.055
[ 19 / 40 ] loss: 0.022
[ 20 / 40 ] loss: 0.010
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.021
[ 23 / 40 ] loss: 0.008
[ 24 / 40 ] loss: 0.070
[ 25 / 40 ] loss: 0.028
[ 26 / 40 ] loss: 0.015
[ 27 / 40 ] loss: 0.182
[ 28 / 40 ] loss: 0.051
[ 29 / 40 ] loss: 0.192
[ 30 / 40 ] loss: 0.041
[ 31 / 40 ] loss: 0.067
[ 32 / 40 ] loss: 0.204
[ 33 / 40 ] loss: 0.022
[ 34 / 40 ] loss: 0.013
[ 35 / 40 ] loss: 0.013
[ 36 / 40 ] loss: 0.048
[ 37 / 40 ] loss: 0.008
[ 38 / 40 ] loss: 0.130
[ 39 / 40 ] loss: 0.121
[ 40 / 40 ] loss: 0.005
0.06544795844238252
Accuracy: 0.721519 -- Precision: 0.738806 -- Recall: 0.761538 -- F1: 0.750000 -- AUC: 0.762689
========= epoch: 24 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.074
[ 2 / 40 ] loss: 0.031
[ 3 / 40 ] loss: 0.047
[ 4 / 40 ] loss: 0.132
[ 5 / 40 ] loss: 0.036
[ 6 / 40 ] loss: 0.164
[ 7 / 40 ] loss: 0.217
[ 8 / 40 ] loss: 0.106
[ 9 / 40 ] loss: 0.022
[ 10 / 40 ] loss: 0.185
[ 11 / 40 ] loss: 0.071
[ 12 / 40 ] loss: 0.189
[ 13 / 40 ] loss: 0.008
[ 14 / 40 ] loss: 0.009
[ 15 / 40 ] loss: 0.083
[ 16 / 40 ] loss: 0.021
[ 17 / 40 ] loss: 0.010
[ 18 / 40 ] loss: 0.107
[ 19 / 40 ] loss: 0.064
[ 20 / 40 ] loss: 0.160
[ 21 / 40 ] loss: 0.194
[ 22 / 40 ] loss: 0.026
[ 23 / 40 ] loss: 0.022
[ 24 / 40 ] loss: 0.015
[ 25 / 40 ] loss: 0.028
[ 26 / 40 ] loss: 0.020
[ 27 / 40 ] loss: 0.009
[ 28 / 40 ] loss: 0.028
[ 29 / 40 ] loss: 0.122
[ 30 / 40 ] loss: 0.020
[ 31 / 40 ] loss: 0.040
[ 32 / 40 ] loss: 0.026
[ 33 / 40 ] loss: 0.232
[ 34 / 40 ] loss: 0.236
[ 35 / 40 ] loss: 0.054
[ 36 / 40 ] loss: 0.033
[ 37 / 40 ] loss: 0.260
[ 38 / 40 ] loss: 0.230
[ 39 / 40 ] loss: 0.053
[ 40 / 40 ] loss: 0.004
0.0846499421983026
Accuracy: 0.725738 -- Precision: 0.715232 -- Recall: 0.830769 -- F1: 0.768683 -- AUC: 0.760316
========= epoch: 25 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.017
[ 2 / 40 ] loss: 0.059
[ 3 / 40 ] loss: 0.014
[ 4 / 40 ] loss: 0.010
[ 5 / 40 ] loss: 0.077
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.068
[ 8 / 40 ] loss: 0.099
[ 9 / 40 ] loss: 0.036
[ 10 / 40 ] loss: 0.095
[ 11 / 40 ] loss: 0.070
[ 12 / 40 ] loss: 0.168
[ 13 / 40 ] loss: 0.006
[ 14 / 40 ] loss: 0.045
[ 15 / 40 ] loss: 0.087
[ 16 / 40 ] loss: 0.034
[ 17 / 40 ] loss: 0.016
[ 18 / 40 ] loss: 0.009
[ 19 / 40 ] loss: 0.053
[ 20 / 40 ] loss: 0.064
[ 21 / 40 ] loss: 0.012
[ 22 / 40 ] loss: 0.023
[ 23 / 40 ] loss: 0.086
[ 24 / 40 ] loss: 0.028
[ 25 / 40 ] loss: 0.135
[ 26 / 40 ] loss: 0.126
[ 27 / 40 ] loss: 0.096
[ 28 / 40 ] loss: 0.015
[ 29 / 40 ] loss: 0.077
[ 30 / 40 ] loss: 0.027
[ 31 / 40 ] loss: 0.008
[ 32 / 40 ] loss: 0.061
[ 33 / 40 ] loss: 0.017
[ 34 / 40 ] loss: 0.016
[ 35 / 40 ] loss: 0.009
[ 36 / 40 ] loss: 0.139
[ 37 / 40 ] loss: 0.014
[ 38 / 40 ] loss: 0.010
[ 39 / 40 ] loss: 0.092
[ 40 / 40 ] loss: 0.004
0.05080648462753743
Accuracy: 0.725738 -- Precision: 0.744361 -- Recall: 0.761538 -- F1: 0.752852 -- AUC: 0.764342
========= epoch: 26 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.012
[ 2 / 40 ] loss: 0.013
[ 3 / 40 ] loss: 0.178
[ 4 / 40 ] loss: 0.006
[ 5 / 40 ] loss: 0.035
[ 6 / 40 ] loss: 0.015
[ 7 / 40 ] loss: 0.184
[ 8 / 40 ] loss: 0.247
[ 9 / 40 ] loss: 0.064
[ 10 / 40 ] loss: 0.239
[ 11 / 40 ] loss: 0.007
[ 12 / 40 ] loss: 0.146
[ 13 / 40 ] loss: 0.044
[ 14 / 40 ] loss: 0.014
[ 15 / 40 ] loss: 0.069
[ 16 / 40 ] loss: 0.007
[ 17 / 40 ] loss: 0.022
[ 18 / 40 ] loss: 0.009
[ 19 / 40 ] loss: 0.013
[ 20 / 40 ] loss: 0.062
[ 21 / 40 ] loss: 0.015
[ 22 / 40 ] loss: 0.007
[ 23 / 40 ] loss: 0.112
[ 24 / 40 ] loss: 0.009
[ 25 / 40 ] loss: 0.016
[ 26 / 40 ] loss: 0.023
[ 27 / 40 ] loss: 0.041
[ 28 / 40 ] loss: 0.007
[ 29 / 40 ] loss: 0.039
[ 30 / 40 ] loss: 0.013
[ 31 / 40 ] loss: 0.084
[ 32 / 40 ] loss: 0.064
[ 33 / 40 ] loss: 0.212
[ 34 / 40 ] loss: 0.009
[ 35 / 40 ] loss: 0.088
[ 36 / 40 ] loss: 0.005
[ 37 / 40 ] loss: 0.010
[ 38 / 40 ] loss: 0.182
[ 39 / 40 ] loss: 0.045
[ 40 / 40 ] loss: 0.076
0.061137361626606435
Accuracy: 0.738397 -- Precision: 0.769841 -- Recall: 0.746154 -- F1: 0.757813 -- AUC: 0.766211
========= epoch: 27 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.017
[ 2 / 40 ] loss: 0.009
[ 3 / 40 ] loss: 0.015
[ 4 / 40 ] loss: 0.006
[ 5 / 40 ] loss: 0.013
[ 6 / 40 ] loss: 0.022
[ 7 / 40 ] loss: 0.130
[ 8 / 40 ] loss: 0.021
[ 9 / 40 ] loss: 0.073
[ 10 / 40 ] loss: 0.005
[ 11 / 40 ] loss: 0.261
[ 12 / 40 ] loss: 0.010
[ 13 / 40 ] loss: 0.171
[ 14 / 40 ] loss: 0.086
[ 15 / 40 ] loss: 0.013
[ 16 / 40 ] loss: 0.016
[ 17 / 40 ] loss: 0.012
[ 18 / 40 ] loss: 0.068
[ 19 / 40 ] loss: 0.009
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.028
[ 22 / 40 ] loss: 0.024
[ 23 / 40 ] loss: 0.052
[ 24 / 40 ] loss: 0.066
[ 25 / 40 ] loss: 0.015
[ 26 / 40 ] loss: 0.041
[ 27 / 40 ] loss: 0.009
[ 28 / 40 ] loss: 0.075
[ 29 / 40 ] loss: 0.015
[ 30 / 40 ] loss: 0.017
[ 31 / 40 ] loss: 0.009
[ 32 / 40 ] loss: 0.010
[ 33 / 40 ] loss: 0.022
[ 34 / 40 ] loss: 0.012
[ 35 / 40 ] loss: 0.007
[ 36 / 40 ] loss: 0.068
[ 37 / 40 ] loss: 0.006
[ 38 / 40 ] loss: 0.020
[ 39 / 40 ] loss: 0.005
[ 40 / 40 ] loss: 0.004
0.03668197422521189
Accuracy: 0.721519 -- Precision: 0.738806 -- Recall: 0.761538 -- F1: 0.750000 -- AUC: 0.767002
========= epoch: 28 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.034
[ 2 / 40 ] loss: 0.004
[ 3 / 40 ] loss: 0.019
[ 4 / 40 ] loss: 0.004
[ 5 / 40 ] loss: 0.045
[ 6 / 40 ] loss: 0.012
[ 7 / 40 ] loss: 0.017
[ 8 / 40 ] loss: 0.010
[ 9 / 40 ] loss: 0.015
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.043
[ 12 / 40 ] loss: 0.122
[ 13 / 40 ] loss: 0.011
[ 14 / 40 ] loss: 0.009
[ 15 / 40 ] loss: 0.004
[ 16 / 40 ] loss: 0.151
[ 17 / 40 ] loss: 0.004
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.006
[ 20 / 40 ] loss: 0.053
[ 21 / 40 ] loss: 0.005
[ 22 / 40 ] loss: 0.099
[ 23 / 40 ] loss: 0.061
[ 24 / 40 ] loss: 0.004
[ 25 / 40 ] loss: 0.016
[ 26 / 40 ] loss: 0.010
[ 27 / 40 ] loss: 0.166
[ 28 / 40 ] loss: 0.004
[ 29 / 40 ] loss: 0.006
[ 30 / 40 ] loss: 0.131
[ 31 / 40 ] loss: 0.007
[ 32 / 40 ] loss: 0.003
[ 33 / 40 ] loss: 0.004
[ 34 / 40 ] loss: 0.052
[ 35 / 40 ] loss: 0.006
[ 36 / 40 ] loss: 0.008
[ 37 / 40 ] loss: 0.013
[ 38 / 40 ] loss: 0.006
[ 39 / 40 ] loss: 0.022
[ 40 / 40 ] loss: 0.003
0.02997636898071505
Accuracy: 0.708861 -- Precision: 0.729323 -- Recall: 0.746154 -- F1: 0.737643 -- AUC: 0.782674
========= epoch: 29 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.009
[ 2 / 40 ] loss: 0.016
[ 3 / 40 ] loss: 0.024
[ 4 / 40 ] loss: 0.005
[ 5 / 40 ] loss: 0.037
[ 6 / 40 ] loss: 0.004
[ 7 / 40 ] loss: 0.219
[ 8 / 40 ] loss: 0.024
[ 9 / 40 ] loss: 0.014
[ 10 / 40 ] loss: 0.013
[ 11 / 40 ] loss: 0.009
[ 12 / 40 ] loss: 0.004
[ 13 / 40 ] loss: 0.015
[ 14 / 40 ] loss: 0.016
[ 15 / 40 ] loss: 0.008
[ 16 / 40 ] loss: 0.027
[ 17 / 40 ] loss: 0.010
[ 18 / 40 ] loss: 0.009
[ 19 / 40 ] loss: 0.006
[ 20 / 40 ] loss: 0.007
[ 21 / 40 ] loss: 0.036
[ 22 / 40 ] loss: 0.006
[ 23 / 40 ] loss: 0.061
[ 24 / 40 ] loss: 0.063
[ 25 / 40 ] loss: 0.003
[ 26 / 40 ] loss: 0.003
[ 27 / 40 ] loss: 0.090
[ 28 / 40 ] loss: 0.004
[ 29 / 40 ] loss: 0.010
[ 30 / 40 ] loss: 0.029
[ 31 / 40 ] loss: 0.005
[ 32 / 40 ] loss: 0.123
[ 33 / 40 ] loss: 0.036
[ 34 / 40 ] loss: 0.028
[ 35 / 40 ] loss: 0.016
[ 36 / 40 ] loss: 0.028
[ 37 / 40 ] loss: 0.140
[ 38 / 40 ] loss: 0.070
[ 39 / 40 ] loss: 0.013
[ 40 / 40 ] loss: 0.005
0.03114641170250252
Accuracy: 0.767932 -- Precision: 0.820513 -- Recall: 0.738462 -- F1: 0.777328 -- AUC: 0.796693
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[113, 82, 39, 57, 189, 58, 178, 218, 172, 152, 66, 188, 185, 181, 195, 51, 91, 146, 40, 215, 63, 72, 119, 184, 101, 194, 74, 23, 227, 201, 200, 60, 95, 233, 192, 123, 202, 237, 186, 31, 7, 18, 187, 171, 132, 204, 214, 125, 45, 16, 22, 124, 109, 34, 232, 41, 169, 167, 225, 234, 166, 55, 128, 61, 32, 68, 219, 2, 149, 76, 207, 35, 43, 209, 62, 158, 196, 73, 168, 11, 230, 115, 93, 129, 59, 220, 154, 121, 142, 208, 231, 116, 131, 42, 147, 37, 136, 87, 191, 112, 213, 173, 175, 150, 10, 139, 221, 75, 183, 15, 199, 24, 193, 143, 47, 176, 103, 38, 54, 81, 228, 56, 9, 8, 97, 96, 78, 27, 25, 85, 212, 50, 44, 110, 30, 71, 177, 14, 159, 84, 86, 46, 65, 100, 216, 26, 190, 67, 236, 134, 52, 117, 89, 222, 211, 210, 144, 137, 153, 94, 6, 140, 79, 223, 48, 12, 205, 120, 53, 17, 226, 19, 170, 36, 155, 174, 164, 3, 151, 69, 217, 180, 88, 13, 92, 99, 165, 163, 162, 64, 138, 28, 29, 98, 203, 130, 77, 104, 229, 107, 108, 21, 197, 156, 157, 114, 126, 127, 161, 179, 118, 224, 141, 105, 90, 70, 33, 182, 5, 106, 83, 133, 235, 20, 206, 135, 122, 145, 80, 198, 49, 148, 102, 111, 4, 160, 1]
[1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1]
[1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 30 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.160
[ 2 / 40 ] loss: 0.006
[ 3 / 40 ] loss: 0.010
[ 4 / 40 ] loss: 0.029
[ 5 / 40 ] loss: 0.035
[ 6 / 40 ] loss: 0.003
[ 7 / 40 ] loss: 0.034
[ 8 / 40 ] loss: 0.003
[ 9 / 40 ] loss: 0.004
[ 10 / 40 ] loss: 0.016
[ 11 / 40 ] loss: 0.022
[ 12 / 40 ] loss: 0.226
[ 13 / 40 ] loss: 0.003
[ 14 / 40 ] loss: 0.006
[ 15 / 40 ] loss: 0.004
[ 16 / 40 ] loss: 0.009
[ 17 / 40 ] loss: 0.004
[ 18 / 40 ] loss: 0.018
[ 19 / 40 ] loss: 0.009
[ 20 / 40 ] loss: 0.081
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.062
[ 23 / 40 ] loss: 0.009
[ 24 / 40 ] loss: 0.112
[ 25 / 40 ] loss: 0.032
[ 26 / 40 ] loss: 0.003
[ 27 / 40 ] loss: 0.038
[ 28 / 40 ] loss: 0.033
[ 29 / 40 ] loss: 0.054
[ 30 / 40 ] loss: 0.067
[ 31 / 40 ] loss: 0.032
[ 32 / 40 ] loss: 0.025
[ 33 / 40 ] loss: 0.104
[ 34 / 40 ] loss: 0.008
[ 35 / 40 ] loss: 0.100
[ 36 / 40 ] loss: 0.008
[ 37 / 40 ] loss: 0.004
[ 38 / 40 ] loss: 0.008
[ 39 / 40 ] loss: 0.004
[ 40 / 40 ] loss: 0.234
0.040592546382686126
Accuracy: 0.742616 -- Precision: 0.751825 -- Recall: 0.792308 -- F1: 0.771536 -- AUC: 0.777930
========= epoch: 31 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.045
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.004
[ 4 / 40 ] loss: 0.073
[ 5 / 40 ] loss: 0.003
[ 6 / 40 ] loss: 0.006
[ 7 / 40 ] loss: 0.018
[ 8 / 40 ] loss: 0.042
[ 9 / 40 ] loss: 0.007
[ 10 / 40 ] loss: 0.030
[ 11 / 40 ] loss: 0.007
[ 12 / 40 ] loss: 0.003
[ 13 / 40 ] loss: 0.040
[ 14 / 40 ] loss: 0.004
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.010
[ 17 / 40 ] loss: 0.040
[ 18 / 40 ] loss: 0.005
[ 19 / 40 ] loss: 0.037
[ 20 / 40 ] loss: 0.007
[ 21 / 40 ] loss: 0.008
[ 22 / 40 ] loss: 0.005
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.027
[ 25 / 40 ] loss: 0.004
[ 26 / 40 ] loss: 0.049
[ 27 / 40 ] loss: 0.021
[ 28 / 40 ] loss: 0.101
[ 29 / 40 ] loss: 0.002
[ 30 / 40 ] loss: 0.045
[ 31 / 40 ] loss: 0.004
[ 32 / 40 ] loss: 0.103
[ 33 / 40 ] loss: 0.035
[ 34 / 40 ] loss: 0.003
[ 35 / 40 ] loss: 0.097
[ 36 / 40 ] loss: 0.041
[ 37 / 40 ] loss: 0.004
[ 38 / 40 ] loss: 0.380
[ 39 / 40 ] loss: 0.010
[ 40 / 40 ] loss: 0.015
0.033776357740862295
Accuracy: 0.691983 -- Precision: 0.782178 -- Recall: 0.607692 -- F1: 0.683983 -- AUC: 0.774407
========= epoch: 32 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.061
[ 2 / 40 ] loss: 0.072
[ 3 / 40 ] loss: 0.006
[ 4 / 40 ] loss: 0.013
[ 5 / 40 ] loss: 0.016
[ 6 / 40 ] loss: 0.042
[ 7 / 40 ] loss: 0.022
[ 8 / 40 ] loss: 0.181
[ 9 / 40 ] loss: 0.030
[ 10 / 40 ] loss: 0.003
[ 11 / 40 ] loss: 0.014
[ 12 / 40 ] loss: 0.005
[ 13 / 40 ] loss: 0.026
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.222
[ 16 / 40 ] loss: 0.004
[ 17 / 40 ] loss: 0.154
[ 18 / 40 ] loss: 0.025
[ 19 / 40 ] loss: 0.416
[ 20 / 40 ] loss: 0.003
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.033
[ 23 / 40 ] loss: 0.036
[ 24 / 40 ] loss: 0.006
[ 25 / 40 ] loss: 0.027
[ 26 / 40 ] loss: 0.103
[ 27 / 40 ] loss: 0.139
[ 28 / 40 ] loss: 0.033
[ 29 / 40 ] loss: 0.484
[ 30 / 40 ] loss: 0.064
[ 31 / 40 ] loss: 0.126
[ 32 / 40 ] loss: 0.033
[ 33 / 40 ] loss: 0.189
[ 34 / 40 ] loss: 0.119
[ 35 / 40 ] loss: 0.029
[ 36 / 40 ] loss: 0.044
[ 37 / 40 ] loss: 0.170
[ 38 / 40 ] loss: 0.040
[ 39 / 40 ] loss: 0.022
[ 40 / 40 ] loss: 0.533
0.08907114162575454
Accuracy: 0.708861 -- Precision: 0.732824 -- Recall: 0.738462 -- F1: 0.735632 -- AUC: 0.763911
========= epoch: 33 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.023
[ 2 / 40 ] loss: 0.023
[ 3 / 40 ] loss: 0.012
[ 4 / 40 ] loss: 0.089
[ 5 / 40 ] loss: 0.201
[ 6 / 40 ] loss: 0.336
[ 7 / 40 ] loss: 0.146
[ 8 / 40 ] loss: 0.085
[ 9 / 40 ] loss: 0.049
[ 10 / 40 ] loss: 0.076
[ 11 / 40 ] loss: 0.009
[ 12 / 40 ] loss: 0.030
[ 13 / 40 ] loss: 0.209
[ 14 / 40 ] loss: 0.047
[ 15 / 40 ] loss: 0.046
[ 16 / 40 ] loss: 0.251
[ 17 / 40 ] loss: 0.153
[ 18 / 40 ] loss: 0.142
[ 19 / 40 ] loss: 0.071
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.030
[ 22 / 40 ] loss: 0.034
[ 23 / 40 ] loss: 0.044
[ 24 / 40 ] loss: 0.006
[ 25 / 40 ] loss: 0.005
[ 26 / 40 ] loss: 0.004
[ 27 / 40 ] loss: 0.111
[ 28 / 40 ] loss: 0.013
[ 29 / 40 ] loss: 0.006
[ 30 / 40 ] loss: 0.019
[ 31 / 40 ] loss: 0.004
[ 32 / 40 ] loss: 0.254
[ 33 / 40 ] loss: 0.048
[ 34 / 40 ] loss: 0.018
[ 35 / 40 ] loss: 0.124
[ 36 / 40 ] loss: 0.008
[ 37 / 40 ] loss: 0.053
[ 38 / 40 ] loss: 0.006
[ 39 / 40 ] loss: 0.044
[ 40 / 40 ] loss: 0.010
0.07118090597214177
Accuracy: 0.691983 -- Precision: 0.724409 -- Recall: 0.707692 -- F1: 0.715953 -- AUC: 0.761035
========= epoch: 34 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.087
[ 2 / 40 ] loss: 0.010
[ 3 / 40 ] loss: 0.005
[ 4 / 40 ] loss: 0.006
[ 5 / 40 ] loss: 0.032
[ 6 / 40 ] loss: 0.038
[ 7 / 40 ] loss: 0.134
[ 8 / 40 ] loss: 0.014
[ 9 / 40 ] loss: 0.007
[ 10 / 40 ] loss: 0.006
[ 11 / 40 ] loss: 0.054
[ 12 / 40 ] loss: 0.101
[ 13 / 40 ] loss: 0.031
[ 14 / 40 ] loss: 0.041
[ 15 / 40 ] loss: 0.008
[ 16 / 40 ] loss: 0.005
[ 17 / 40 ] loss: 0.007
[ 18 / 40 ] loss: 0.004
[ 19 / 40 ] loss: 0.006
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.012
[ 22 / 40 ] loss: 0.004
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.028
[ 25 / 40 ] loss: 0.003
[ 26 / 40 ] loss: 0.004
[ 27 / 40 ] loss: 0.004
[ 28 / 40 ] loss: 0.034
[ 29 / 40 ] loss: 0.003
[ 30 / 40 ] loss: 0.013
[ 31 / 40 ] loss: 0.092
[ 32 / 40 ] loss: 0.090
[ 33 / 40 ] loss: 0.005
[ 34 / 40 ] loss: 0.075
[ 35 / 40 ] loss: 0.046
[ 36 / 40 ] loss: 0.034
[ 37 / 40 ] loss: 0.005
[ 38 / 40 ] loss: 0.247
[ 39 / 40 ] loss: 0.004
[ 40 / 40 ] loss: 0.060
0.03420458133914508
Accuracy: 0.717300 -- Precision: 0.720280 -- Recall: 0.792308 -- F1: 0.754579 -- AUC: 0.758411
========= epoch: 35 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.022
[ 2 / 40 ] loss: 0.009
[ 3 / 40 ] loss: 0.003
[ 4 / 40 ] loss: 0.032
[ 5 / 40 ] loss: 0.005
[ 6 / 40 ] loss: 0.017
[ 7 / 40 ] loss: 0.008
[ 8 / 40 ] loss: 0.004
[ 9 / 40 ] loss: 0.073
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.273
[ 12 / 40 ] loss: 0.003
[ 13 / 40 ] loss: 0.006
[ 14 / 40 ] loss: 0.033
[ 15 / 40 ] loss: 0.019
[ 16 / 40 ] loss: 0.083
[ 17 / 40 ] loss: 0.016
[ 18 / 40 ] loss: 0.004
[ 19 / 40 ] loss: 0.003
[ 20 / 40 ] loss: 0.004
[ 21 / 40 ] loss: 0.068
[ 22 / 40 ] loss: 0.100
[ 23 / 40 ] loss: 0.208
[ 24 / 40 ] loss: 0.118
[ 25 / 40 ] loss: 0.003
[ 26 / 40 ] loss: 0.003
[ 27 / 40 ] loss: 0.037
[ 28 / 40 ] loss: 0.051
[ 29 / 40 ] loss: 0.004
[ 30 / 40 ] loss: 0.024
[ 31 / 40 ] loss: 0.083
[ 32 / 40 ] loss: 0.102
[ 33 / 40 ] loss: 0.194
[ 34 / 40 ] loss: 0.086
[ 35 / 40 ] loss: 0.023
[ 36 / 40 ] loss: 0.114
[ 37 / 40 ] loss: 0.023
[ 38 / 40 ] loss: 0.004
[ 39 / 40 ] loss: 0.038
[ 40 / 40 ] loss: 0.002
0.04774413814884611
Accuracy: 0.708861 -- Precision: 0.716312 -- Recall: 0.776923 -- F1: 0.745387 -- AUC: 0.757153
========= epoch: 36 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.033
[ 2 / 40 ] loss: 0.004
[ 3 / 40 ] loss: 0.300
[ 4 / 40 ] loss: 0.006
[ 5 / 40 ] loss: 0.045
[ 6 / 40 ] loss: 0.021
[ 7 / 40 ] loss: 0.043
[ 8 / 40 ] loss: 0.013
[ 9 / 40 ] loss: 0.004
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.021
[ 12 / 40 ] loss: 0.004
[ 13 / 40 ] loss: 0.288
[ 14 / 40 ] loss: 0.054
[ 15 / 40 ] loss: 0.015
[ 16 / 40 ] loss: 0.289
[ 17 / 40 ] loss: 0.027
[ 18 / 40 ] loss: 0.005
[ 19 / 40 ] loss: 0.012
[ 20 / 40 ] loss: 0.084
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.005
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.003
[ 25 / 40 ] loss: 0.005
[ 26 / 40 ] loss: 0.015
[ 27 / 40 ] loss: 0.015
[ 28 / 40 ] loss: 0.017
[ 29 / 40 ] loss: 0.015
[ 30 / 40 ] loss: 0.031
[ 31 / 40 ] loss: 0.017
[ 32 / 40 ] loss: 0.032
[ 33 / 40 ] loss: 0.141
[ 34 / 40 ] loss: 0.091
[ 35 / 40 ] loss: 0.026
[ 36 / 40 ] loss: 0.053
[ 37 / 40 ] loss: 0.072
[ 38 / 40 ] loss: 0.085
[ 39 / 40 ] loss: 0.091
[ 40 / 40 ] loss: 0.003
0.04998988900333643
Accuracy: 0.713080 -- Precision: 0.742188 -- Recall: 0.730769 -- F1: 0.736434 -- AUC: 0.745507
========= epoch: 37 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.125
[ 2 / 40 ] loss: 0.025
[ 3 / 40 ] loss: 0.021
[ 4 / 40 ] loss: 0.004
[ 5 / 40 ] loss: 0.107
[ 6 / 40 ] loss: 0.199
[ 7 / 40 ] loss: 0.065
[ 8 / 40 ] loss: 0.003
[ 9 / 40 ] loss: 0.023
[ 10 / 40 ] loss: 0.293
[ 11 / 40 ] loss: 0.104
[ 12 / 40 ] loss: 0.032
[ 13 / 40 ] loss: 0.009
[ 14 / 40 ] loss: 0.003
[ 15 / 40 ] loss: 0.035
[ 16 / 40 ] loss: 0.017
[ 17 / 40 ] loss: 0.061
[ 18 / 40 ] loss: 0.006
[ 19 / 40 ] loss: 0.023
[ 20 / 40 ] loss: 0.004
[ 21 / 40 ] loss: 0.018
[ 22 / 40 ] loss: 0.012
[ 23 / 40 ] loss: 0.026
[ 24 / 40 ] loss: 0.009
[ 25 / 40 ] loss: 0.005
[ 26 / 40 ] loss: 0.070
[ 27 / 40 ] loss: 0.128
[ 28 / 40 ] loss: 0.012
[ 29 / 40 ] loss: 0.011
[ 30 / 40 ] loss: 0.004
[ 31 / 40 ] loss: 0.090
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.007
[ 34 / 40 ] loss: 0.016
[ 35 / 40 ] loss: 0.004
[ 36 / 40 ] loss: 0.082
[ 37 / 40 ] loss: 0.024
[ 38 / 40 ] loss: 0.028
[ 39 / 40 ] loss: 0.007
[ 40 / 40 ] loss: 0.039
0.0439139491179958
Accuracy: 0.713080 -- Precision: 0.734848 -- Recall: 0.746154 -- F1: 0.740458 -- AUC: 0.761323
========= epoch: 38 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.005
[ 2 / 40 ] loss: 0.005
[ 3 / 40 ] loss: 0.007
[ 4 / 40 ] loss: 0.005
[ 5 / 40 ] loss: 0.023
[ 6 / 40 ] loss: 0.017
[ 7 / 40 ] loss: 0.005
[ 8 / 40 ] loss: 0.027
[ 9 / 40 ] loss: 0.009
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.058
[ 12 / 40 ] loss: 0.041
[ 13 / 40 ] loss: 0.006
[ 14 / 40 ] loss: 0.003
[ 15 / 40 ] loss: 0.015
[ 16 / 40 ] loss: 0.007
[ 17 / 40 ] loss: 0.010
[ 18 / 40 ] loss: 0.024
[ 19 / 40 ] loss: 0.003
[ 20 / 40 ] loss: 0.002
[ 21 / 40 ] loss: 0.002
[ 22 / 40 ] loss: 0.007
[ 23 / 40 ] loss: 0.008
[ 24 / 40 ] loss: 0.004
[ 25 / 40 ] loss: 0.043
[ 26 / 40 ] loss: 0.006
[ 27 / 40 ] loss: 0.016
[ 28 / 40 ] loss: 0.055
[ 29 / 40 ] loss: 0.002
[ 30 / 40 ] loss: 0.003
[ 31 / 40 ] loss: 0.003
[ 32 / 40 ] loss: 0.002
[ 33 / 40 ] loss: 0.005
[ 34 / 40 ] loss: 0.050
[ 35 / 40 ] loss: 0.002
[ 36 / 40 ] loss: 0.262
[ 37 / 40 ] loss: 0.027
[ 38 / 40 ] loss: 0.003
[ 39 / 40 ] loss: 0.002
[ 40 / 40 ] loss: 0.002
0.019544518066686577
Accuracy: 0.717300 -- Precision: 0.729927 -- Recall: 0.769231 -- F1: 0.749064 -- AUC: 0.762832
========= epoch: 39 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.002
[ 2 / 40 ] loss: 0.002
[ 3 / 40 ] loss: 0.131
[ 4 / 40 ] loss: 0.002
[ 5 / 40 ] loss: 0.002
[ 6 / 40 ] loss: 0.002
[ 7 / 40 ] loss: 0.002
[ 8 / 40 ] loss: 0.021
[ 9 / 40 ] loss: 0.004
[ 10 / 40 ] loss: 0.002
[ 11 / 40 ] loss: 0.003
[ 12 / 40 ] loss: 0.007
[ 13 / 40 ] loss: 0.080
[ 14 / 40 ] loss: 0.003
[ 15 / 40 ] loss: 0.018
[ 16 / 40 ] loss: 0.003
[ 17 / 40 ] loss: 0.019
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.024
[ 21 / 40 ] loss: 0.003
[ 22 / 40 ] loss: 0.002
[ 23 / 40 ] loss: 0.247
[ 24 / 40 ] loss: 0.003
[ 25 / 40 ] loss: 0.022
[ 26 / 40 ] loss: 0.200
[ 27 / 40 ] loss: 0.014
[ 28 / 40 ] loss: 0.002
[ 29 / 40 ] loss: 0.131
[ 30 / 40 ] loss: 0.004
[ 31 / 40 ] loss: 0.005
[ 32 / 40 ] loss: 0.002
[ 33 / 40 ] loss: 0.010
[ 34 / 40 ] loss: 0.042
[ 35 / 40 ] loss: 0.013
[ 36 / 40 ] loss: 0.002
[ 37 / 40 ] loss: 0.019
[ 38 / 40 ] loss: 0.003
[ 39 / 40 ] loss: 0.003
[ 40 / 40 ] loss: 0.005
0.026766783342463896
Accuracy: 0.721519 -- Precision: 0.713333 -- Recall: 0.823077 -- F1: 0.764286 -- AUC: 0.757369
========= epoch: 40 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.002
[ 2 / 40 ] loss: 0.031
[ 3 / 40 ] loss: 0.013
[ 4 / 40 ] loss: 0.034
[ 5 / 40 ] loss: 0.005
[ 6 / 40 ] loss: 0.197
[ 7 / 40 ] loss: 0.015
[ 8 / 40 ] loss: 0.290
[ 9 / 40 ] loss: 0.005
[ 10 / 40 ] loss: 0.015
[ 11 / 40 ] loss: 0.003
[ 12 / 40 ] loss: 0.027
[ 13 / 40 ] loss: 0.002
[ 14 / 40 ] loss: 0.021
[ 15 / 40 ] loss: 0.003
[ 16 / 40 ] loss: 0.054
[ 17 / 40 ] loss: 0.002
[ 18 / 40 ] loss: 0.141
[ 19 / 40 ] loss: 0.002
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.005
[ 22 / 40 ] loss: 0.056
[ 23 / 40 ] loss: 0.088
[ 24 / 40 ] loss: 0.010
[ 25 / 40 ] loss: 0.105
[ 26 / 40 ] loss: 0.017
[ 27 / 40 ] loss: 0.039
[ 28 / 40 ] loss: 0.016
[ 29 / 40 ] loss: 0.044
[ 30 / 40 ] loss: 0.063
[ 31 / 40 ] loss: 0.315
[ 32 / 40 ] loss: 0.003
[ 33 / 40 ] loss: 0.007
[ 34 / 40 ] loss: 0.025
[ 35 / 40 ] loss: 0.005
[ 36 / 40 ] loss: 0.043
[ 37 / 40 ] loss: 0.019
[ 38 / 40 ] loss: 0.004
[ 39 / 40 ] loss: 0.004
[ 40 / 40 ] loss: 0.041
0.04445634970907122
Accuracy: 0.713080 -- Precision: 0.709459 -- Recall: 0.807692 -- F1: 0.755396 -- AUC: 0.753774
========= epoch: 41 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.003
[ 2 / 40 ] loss: 0.010
[ 3 / 40 ] loss: 0.002
[ 4 / 40 ] loss: 0.008
[ 5 / 40 ] loss: 0.242
[ 6 / 40 ] loss: 0.007
[ 7 / 40 ] loss: 0.003
[ 8 / 40 ] loss: 0.024
[ 9 / 40 ] loss: 0.017
[ 10 / 40 ] loss: 0.003
[ 11 / 40 ] loss: 0.400
[ 12 / 40 ] loss: 0.003
[ 13 / 40 ] loss: 0.003
[ 14 / 40 ] loss: 0.070
[ 15 / 40 ] loss: 0.003
[ 16 / 40 ] loss: 0.277
[ 17 / 40 ] loss: 0.004
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.006
[ 20 / 40 ] loss: 0.010
[ 21 / 40 ] loss: 0.238
[ 22 / 40 ] loss: 0.016
[ 23 / 40 ] loss: 0.007
[ 24 / 40 ] loss: 0.008
[ 25 / 40 ] loss: 0.020
[ 26 / 40 ] loss: 0.114
[ 27 / 40 ] loss: 0.052
[ 28 / 40 ] loss: 0.033
[ 29 / 40 ] loss: 0.011
[ 30 / 40 ] loss: 0.003
[ 31 / 40 ] loss: 0.021
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.173
[ 34 / 40 ] loss: 0.027
[ 35 / 40 ] loss: 0.010
[ 36 / 40 ] loss: 0.007
[ 37 / 40 ] loss: 0.058
[ 38 / 40 ] loss: 0.018
[ 39 / 40 ] loss: 0.240
[ 40 / 40 ] loss: 0.006
0.05411304857116193
Accuracy: 0.704641 -- Precision: 0.702703 -- Recall: 0.800000 -- F1: 0.748201 -- AUC: 0.763839
========= epoch: 42 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.023
[ 2 / 40 ] loss: 0.005
[ 3 / 40 ] loss: 0.005
[ 4 / 40 ] loss: 0.005
[ 5 / 40 ] loss: 0.025
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.008
[ 8 / 40 ] loss: 0.023
[ 9 / 40 ] loss: 0.104
[ 10 / 40 ] loss: 0.006
[ 11 / 40 ] loss: 0.003
[ 12 / 40 ] loss: 0.035
[ 13 / 40 ] loss: 0.007
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.008
[ 16 / 40 ] loss: 0.004
[ 17 / 40 ] loss: 0.253
[ 18 / 40 ] loss: 0.014
[ 19 / 40 ] loss: 0.004
[ 20 / 40 ] loss: 0.010
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.014
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.012
[ 25 / 40 ] loss: 0.016
[ 26 / 40 ] loss: 0.295
[ 27 / 40 ] loss: 0.459
[ 28 / 40 ] loss: 0.276
[ 29 / 40 ] loss: 0.005
[ 30 / 40 ] loss: 0.007
[ 31 / 40 ] loss: 0.039
[ 32 / 40 ] loss: 0.031
[ 33 / 40 ] loss: 0.079
[ 34 / 40 ] loss: 0.285
[ 35 / 40 ] loss: 0.007
[ 36 / 40 ] loss: 0.009
[ 37 / 40 ] loss: 0.093
[ 38 / 40 ] loss: 0.007
[ 39 / 40 ] loss: 0.037
[ 40 / 40 ] loss: 0.004
0.056146983563667166
Accuracy: 0.729958 -- Precision: 0.753846 -- Recall: 0.753846 -- F1: 0.753846 -- AUC: 0.777714
========= epoch: 43 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.005
[ 2 / 40 ] loss: 0.007
[ 3 / 40 ] loss: 0.016
[ 4 / 40 ] loss: 0.007
[ 5 / 40 ] loss: 0.004
[ 6 / 40 ] loss: 0.010
[ 7 / 40 ] loss: 0.004
[ 8 / 40 ] loss: 0.020
[ 9 / 40 ] loss: 0.209
[ 10 / 40 ] loss: 0.008
[ 11 / 40 ] loss: 0.013
[ 12 / 40 ] loss: 0.017
[ 13 / 40 ] loss: 0.042
[ 14 / 40 ] loss: 0.004
[ 15 / 40 ] loss: 0.023
[ 16 / 40 ] loss: 0.010
[ 17 / 40 ] loss: 0.020
[ 18 / 40 ] loss: 0.092
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.015
[ 21 / 40 ] loss: 0.005
[ 22 / 40 ] loss: 0.093
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.005
[ 25 / 40 ] loss: 0.004
[ 26 / 40 ] loss: 0.015
[ 27 / 40 ] loss: 0.016
[ 28 / 40 ] loss: 0.012
[ 29 / 40 ] loss: 0.004
[ 30 / 40 ] loss: 0.008
[ 31 / 40 ] loss: 0.056
[ 32 / 40 ] loss: 0.006
[ 33 / 40 ] loss: 0.004
[ 34 / 40 ] loss: 0.004
[ 35 / 40 ] loss: 0.003
[ 36 / 40 ] loss: 0.004
[ 37 / 40 ] loss: 0.003
[ 38 / 40 ] loss: 0.043
[ 39 / 40 ] loss: 0.004
[ 40 / 40 ] loss: 0.004
0.020628568349638953
Accuracy: 0.729958 -- Precision: 0.770492 -- Recall: 0.723077 -- F1: 0.746032 -- AUC: 0.753846
========= epoch: 44 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.003
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.005
[ 4 / 40 ] loss: 0.003
[ 5 / 40 ] loss: 0.006
[ 6 / 40 ] loss: 0.136
[ 7 / 40 ] loss: 0.004
[ 8 / 40 ] loss: 0.004
[ 9 / 40 ] loss: 0.024
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.004
[ 12 / 40 ] loss: 0.264
[ 13 / 40 ] loss: 0.003
[ 14 / 40 ] loss: 0.009
[ 15 / 40 ] loss: 0.006
[ 16 / 40 ] loss: 0.003
[ 17 / 40 ] loss: 0.018
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.069
[ 20 / 40 ] loss: 0.220
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.003
[ 23 / 40 ] loss: 0.002
[ 24 / 40 ] loss: 0.002
[ 25 / 40 ] loss: 0.003
[ 26 / 40 ] loss: 0.003
[ 27 / 40 ] loss: 0.011
[ 28 / 40 ] loss: 0.025
[ 29 / 40 ] loss: 0.064
[ 30 / 40 ] loss: 0.003
[ 31 / 40 ] loss: 0.008
[ 32 / 40 ] loss: 0.023
[ 33 / 40 ] loss: 0.008
[ 34 / 40 ] loss: 0.002
[ 35 / 40 ] loss: 0.002
[ 36 / 40 ] loss: 0.003
[ 37 / 40 ] loss: 0.013
[ 38 / 40 ] loss: 0.005
[ 39 / 40 ] loss: 0.087
[ 40 / 40 ] loss: 0.004
0.026612121844664216
Accuracy: 0.721519 -- Precision: 0.766667 -- Recall: 0.707692 -- F1: 0.736000 -- AUC: 0.769842
========= epoch: 45 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.031
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.003
[ 4 / 40 ] loss: 0.013
[ 5 / 40 ] loss: 0.004
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.003
[ 8 / 40 ] loss: 0.005
[ 9 / 40 ] loss: 0.005
[ 10 / 40 ] loss: 0.003
[ 11 / 40 ] loss: 0.049
[ 12 / 40 ] loss: 0.019
[ 13 / 40 ] loss: 0.003
[ 14 / 40 ] loss: 0.002
[ 15 / 40 ] loss: 0.005
[ 16 / 40 ] loss: 0.161
[ 17 / 40 ] loss: 0.014
[ 18 / 40 ] loss: 0.021
[ 19 / 40 ] loss: 0.010
[ 20 / 40 ] loss: 0.003
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.003
[ 23 / 40 ] loss: 0.065
[ 24 / 40 ] loss: 0.004
[ 25 / 40 ] loss: 0.009
[ 26 / 40 ] loss: 0.005
[ 27 / 40 ] loss: 0.084
[ 28 / 40 ] loss: 0.005
[ 29 / 40 ] loss: 0.003
[ 30 / 40 ] loss: 0.004
[ 31 / 40 ] loss: 0.022
[ 32 / 40 ] loss: 0.002
[ 33 / 40 ] loss: 0.003
[ 34 / 40 ] loss: 0.005
[ 35 / 40 ] loss: 0.007
[ 36 / 40 ] loss: 0.002
[ 37 / 40 ] loss: 0.489
[ 38 / 40 ] loss: 0.002
[ 39 / 40 ] loss: 0.004
[ 40 / 40 ] loss: 0.002
0.027289656610810197
Accuracy: 0.742616 -- Precision: 0.771654 -- Recall: 0.753846 -- F1: 0.762646 -- AUC: 0.783285
========= epoch: 46 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.005
[ 2 / 40 ] loss: 0.002
[ 3 / 40 ] loss: 0.059
[ 4 / 40 ] loss: 0.003
[ 5 / 40 ] loss: 0.004
[ 6 / 40 ] loss: 0.003
[ 7 / 40 ] loss: 0.003
[ 8 / 40 ] loss: 0.209
[ 9 / 40 ] loss: 0.035
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.003
[ 12 / 40 ] loss: 0.015
[ 13 / 40 ] loss: 0.005
[ 14 / 40 ] loss: 0.020
[ 15 / 40 ] loss: 0.017
[ 16 / 40 ] loss: 0.030
[ 17 / 40 ] loss: 0.149
[ 18 / 40 ] loss: 0.055
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.032
[ 21 / 40 ] loss: 0.006
[ 22 / 40 ] loss: 0.010
[ 23 / 40 ] loss: 0.007
[ 24 / 40 ] loss: 0.009
[ 25 / 40 ] loss: 0.016
[ 26 / 40 ] loss: 0.015
[ 27 / 40 ] loss: 0.005
[ 28 / 40 ] loss: 0.005
[ 29 / 40 ] loss: 0.004
[ 30 / 40 ] loss: 0.003
[ 31 / 40 ] loss: 0.005
[ 32 / 40 ] loss: 0.003
[ 33 / 40 ] loss: 0.002
[ 34 / 40 ] loss: 0.006
[ 35 / 40 ] loss: 0.003
[ 36 / 40 ] loss: 0.041
[ 37 / 40 ] loss: 0.002
[ 38 / 40 ] loss: 0.004
[ 39 / 40 ] loss: 0.005
[ 40 / 40 ] loss: 0.003
0.020336584461620077
Accuracy: 0.734177 -- Precision: 0.755725 -- Recall: 0.761538 -- F1: 0.758621 -- AUC: 0.788641
========= epoch: 47 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.002
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.005
[ 4 / 40 ] loss: 0.117
[ 5 / 40 ] loss: 0.003
[ 6 / 40 ] loss: 0.002
[ 7 / 40 ] loss: 0.039
[ 8 / 40 ] loss: 0.002
[ 9 / 40 ] loss: 0.010
[ 10 / 40 ] loss: 0.003
[ 11 / 40 ] loss: 0.010
[ 12 / 40 ] loss: 0.020
[ 13 / 40 ] loss: 0.002
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.045
[ 16 / 40 ] loss: 0.013
[ 17 / 40 ] loss: 0.003
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.003
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.011
[ 23 / 40 ] loss: 0.002
[ 24 / 40 ] loss: 0.078
[ 25 / 40 ] loss: 0.009
[ 26 / 40 ] loss: 0.002
[ 27 / 40 ] loss: 0.002
[ 28 / 40 ] loss: 0.003
[ 29 / 40 ] loss: 0.024
[ 30 / 40 ] loss: 0.158
[ 31 / 40 ] loss: 0.020
[ 32 / 40 ] loss: 0.122
[ 33 / 40 ] loss: 0.002
[ 34 / 40 ] loss: 0.002
[ 35 / 40 ] loss: 0.002
[ 36 / 40 ] loss: 0.002
[ 37 / 40 ] loss: 0.084
[ 38 / 40 ] loss: 0.005
[ 39 / 40 ] loss: 0.003
[ 40 / 40 ] loss: 0.002
0.020967251574620604
Accuracy: 0.704641 -- Precision: 0.692308 -- Recall: 0.830769 -- F1: 0.755245 -- AUC: 0.772322
========= epoch: 48 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.138
[ 2 / 40 ] loss: 0.020
[ 3 / 40 ] loss: 0.002
[ 4 / 40 ] loss: 0.002
[ 5 / 40 ] loss: 0.014
[ 6 / 40 ] loss: 0.002
[ 7 / 40 ] loss: 0.003
[ 8 / 40 ] loss: 0.013
[ 9 / 40 ] loss: 0.002
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.011
[ 12 / 40 ] loss: 0.064
[ 13 / 40 ] loss: 0.008
[ 14 / 40 ] loss: 0.072
[ 15 / 40 ] loss: 0.027
[ 16 / 40 ] loss: 0.011
[ 17 / 40 ] loss: 0.002
[ 18 / 40 ] loss: 0.002
[ 19 / 40 ] loss: 0.002
[ 20 / 40 ] loss: 0.003
[ 21 / 40 ] loss: 0.012
[ 22 / 40 ] loss: 0.003
[ 23 / 40 ] loss: 0.003
[ 24 / 40 ] loss: 0.015
[ 25 / 40 ] loss: 0.096
[ 26 / 40 ] loss: 0.003
[ 27 / 40 ] loss: 0.002
[ 28 / 40 ] loss: 0.007
[ 29 / 40 ] loss: 0.236
[ 30 / 40 ] loss: 0.083
[ 31 / 40 ] loss: 0.193
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.099
[ 34 / 40 ] loss: 0.120
[ 35 / 40 ] loss: 0.014
[ 36 / 40 ] loss: 0.012
[ 37 / 40 ] loss: 0.026
[ 38 / 40 ] loss: 0.020
[ 39 / 40 ] loss: 0.014
[ 40 / 40 ] loss: 0.296
0.041536112627363765
Accuracy: 0.717300 -- Precision: 0.733333 -- Recall: 0.761538 -- F1: 0.747170 -- AUC: 0.771136
========= epoch: 49 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.256
[ 2 / 40 ] loss: 0.031
[ 3 / 40 ] loss: 0.020
[ 4 / 40 ] loss: 0.021
[ 5 / 40 ] loss: 0.293
[ 6 / 40 ] loss: 0.019
[ 7 / 40 ] loss: 0.051
[ 8 / 40 ] loss: 0.038
[ 9 / 40 ] loss: 0.009
[ 10 / 40 ] loss: 0.008
[ 11 / 40 ] loss: 0.025
[ 12 / 40 ] loss: 0.022
[ 13 / 40 ] loss: 0.016
[ 14 / 40 ] loss: 0.016
[ 15 / 40 ] loss: 0.007
[ 16 / 40 ] loss: 0.007
[ 17 / 40 ] loss: 0.010
[ 18 / 40 ] loss: 0.005
[ 19 / 40 ] loss: 0.004
[ 20 / 40 ] loss: 0.088
[ 21 / 40 ] loss: 0.005
[ 22 / 40 ] loss: 0.123
[ 23 / 40 ] loss: 0.013
[ 24 / 40 ] loss: 0.003
[ 25 / 40 ] loss: 0.002
[ 26 / 40 ] loss: 0.008
[ 27 / 40 ] loss: 0.112
[ 28 / 40 ] loss: 0.011
[ 29 / 40 ] loss: 0.003
[ 30 / 40 ] loss: 0.008
[ 31 / 40 ] loss: 0.054
[ 32 / 40 ] loss: 0.011
[ 33 / 40 ] loss: 0.023
[ 34 / 40 ] loss: 0.013
[ 35 / 40 ] loss: 0.003
[ 36 / 40 ] loss: 0.018
[ 37 / 40 ] loss: 0.049
[ 38 / 40 ] loss: 0.010
[ 39 / 40 ] loss: 0.010
[ 40 / 40 ] loss: 0.018
0.03608928300673142
Accuracy: 0.729958 -- Precision: 0.750000 -- Recall: 0.761538 -- F1: 0.755725 -- AUC: 0.792272
0.15358649789029535 0.1641025641025641 0.1476923076923077 0.15546558704453442 0.15933860531991373
[113, 82, 39, 57, 189, 58, 178, 218, 172, 152, 66, 188, 185, 181, 195, 51, 91, 146, 40, 215, 63, 72, 119, 184, 101, 194, 74, 23, 227, 201, 200, 60, 95, 233, 192, 123, 202, 237, 186, 31, 7, 18, 187, 171, 132, 204, 214, 125, 45, 16, 22, 124, 109, 34, 232, 41, 169, 167, 225, 234, 166, 55, 128, 61, 32, 68, 219, 2, 149, 76, 207, 35, 43, 209, 62, 158, 196, 73, 168, 11, 230, 115, 93, 129, 59, 220, 154, 121, 142, 208, 231, 116, 131, 42, 147, 37, 136, 87, 191, 112, 213, 173, 175, 150, 10, 139, 221, 75, 183, 15, 199, 24, 193, 143, 47, 176, 103, 38, 54, 81, 228, 56, 9, 8, 97, 96, 78, 27, 25, 85, 212, 50, 44, 110, 30, 71, 177, 14, 159, 84, 86, 46, 65, 100, 216, 26, 190, 67, 236, 134, 52, 117, 89, 222, 211, 210, 144, 137, 153, 94, 6, 140, 79, 223, 48, 12, 205, 120, 53, 17, 226, 19, 170, 36, 155, 174, 164, 3, 151, 69, 217, 180, 88, 13, 92, 99, 165, 163, 162, 64, 138, 28, 29, 98, 203, 130, 77, 104, 229, 107, 108, 21, 197, 156, 157, 114, 126, 127, 161, 179, 118, 224, 141, 105, 90, 70, 33, 182, 5, 106, 83, 133, 235, 20, 206, 135, 122, 145, 80, 198, 49, 148, 102, 111, 4, 160, 1]
[1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1]
训练集: 946
测试集: 237
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 0 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.697
[ 2 / 40 ] loss: 0.696
[ 3 / 40 ] loss: 0.689
[ 4 / 40 ] loss: 0.658
[ 5 / 40 ] loss: 0.681
[ 6 / 40 ] loss: 0.672
[ 7 / 40 ] loss: 0.719
[ 8 / 40 ] loss: 0.663
[ 9 / 40 ] loss: 0.750
[ 10 / 40 ] loss: 0.692
[ 11 / 40 ] loss: 0.680
[ 12 / 40 ] loss: 0.709
[ 13 / 40 ] loss: 0.697
[ 14 / 40 ] loss: 0.714
[ 15 / 40 ] loss: 0.698
[ 16 / 40 ] loss: 0.694
[ 17 / 40 ] loss: 0.695
[ 18 / 40 ] loss: 0.679
[ 19 / 40 ] loss: 0.681
[ 20 / 40 ] loss: 0.711
[ 21 / 40 ] loss: 0.718
[ 22 / 40 ] loss: 0.693
[ 23 / 40 ] loss: 0.685
[ 24 / 40 ] loss: 0.686
[ 25 / 40 ] loss: 0.679
[ 26 / 40 ] loss: 0.691
[ 27 / 40 ] loss: 0.666
[ 28 / 40 ] loss: 0.684
[ 29 / 40 ] loss: 0.671
[ 30 / 40 ] loss: 0.707
[ 31 / 40 ] loss: 0.703
[ 32 / 40 ] loss: 0.629
[ 33 / 40 ] loss: 0.703
[ 34 / 40 ] loss: 0.736
[ 35 / 40 ] loss: 0.709
[ 36 / 40 ] loss: 0.690
[ 37 / 40 ] loss: 0.700
[ 38 / 40 ] loss: 0.703
[ 39 / 40 ] loss: 0.722
[ 40 / 40 ] loss: 0.727
0.6943794578313828
Accuracy: 0.548523 -- Precision: 0.548523 -- Recall: 1.000000 -- F1: 0.708447 -- AUC: 0.616607
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[143, 205, 46, 13, 81, 74, 181, 112, 157, 3, 118, 69, 92, 169, 164, 83, 50, 98, 213, 94, 210, 42, 73, 150, 99, 8, 76, 77, 24, 7, 173, 156, 64, 102, 206, 200, 132, 217, 52, 47, 160, 176, 199, 54, 20, 93, 82, 207, 137, 70, 159, 161, 183, 214, 37, 36, 134, 80, 168, 178, 149, 230, 106, 61, 222, 185, 5, 182, 95, 126, 224, 26, 116, 111, 127, 203, 130, 59, 133, 139, 146, 232, 51, 204, 171, 18, 225, 194, 17, 62, 175, 158, 78, 152, 110, 104, 221, 215, 32, 226, 166, 16, 40, 103, 86, 4, 154, 122, 38, 165, 192, 87, 90, 113, 128, 145, 117, 231, 29, 66, 63, 179, 155, 115, 28, 60, 125, 107, 236, 211, 235, 33, 142, 170, 212, 43, 101, 109, 140, 75, 180, 121, 27, 23, 120, 22, 97, 57, 177, 227, 119, 219, 19, 208, 108, 84, 25, 233, 58, 151, 15, 31, 79, 167, 148, 68, 88, 228, 186, 34, 153, 11, 14, 190, 196, 96, 67, 174, 162, 44, 39, 188, 56, 129, 89, 21, 191, 41, 229, 124, 131, 72, 71, 163, 114, 141, 136, 55, 105, 123, 198, 45, 85, 197, 216, 172, 201, 202, 100, 187, 223, 195, 147, 135, 193, 184, 91, 237, 53, 9, 35, 2, 1, 144, 10, 218, 30, 234, 48, 6, 49, 138, 189, 65, 220, 12, 209]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 1 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.674
[ 2 / 40 ] loss: 0.679
[ 3 / 40 ] loss: 0.690
[ 4 / 40 ] loss: 0.676
[ 5 / 40 ] loss: 0.688
[ 6 / 40 ] loss: 0.695
[ 7 / 40 ] loss: 0.699
[ 8 / 40 ] loss: 0.697
[ 9 / 40 ] loss: 0.692
[ 10 / 40 ] loss: 0.672
[ 11 / 40 ] loss: 0.685
[ 12 / 40 ] loss: 0.672
[ 13 / 40 ] loss: 0.670
[ 14 / 40 ] loss: 0.693
[ 15 / 40 ] loss: 0.693
[ 16 / 40 ] loss: 0.664
[ 17 / 40 ] loss: 0.648
[ 18 / 40 ] loss: 0.660
[ 19 / 40 ] loss: 0.599
[ 20 / 40 ] loss: 0.649
[ 21 / 40 ] loss: 0.613
[ 22 / 40 ] loss: 0.619
[ 23 / 40 ] loss: 0.633
[ 24 / 40 ] loss: 0.737
[ 25 / 40 ] loss: 0.726
[ 26 / 40 ] loss: 0.797
[ 27 / 40 ] loss: 0.695
[ 28 / 40 ] loss: 0.703
[ 29 / 40 ] loss: 0.666
[ 30 / 40 ] loss: 0.710
[ 31 / 40 ] loss: 0.684
[ 32 / 40 ] loss: 0.703
[ 33 / 40 ] loss: 0.701
[ 34 / 40 ] loss: 0.685
[ 35 / 40 ] loss: 0.692
[ 36 / 40 ] loss: 0.688
[ 37 / 40 ] loss: 0.682
[ 38 / 40 ] loss: 0.685
[ 39 / 40 ] loss: 0.670
[ 40 / 40 ] loss: 0.647
0.6807488650083542
Accuracy: 0.594937 -- Precision: 0.582524 -- Recall: 0.923077 -- F1: 0.714286 -- AUC: 0.681955
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[174, 62, 44, 101, 76, 164, 235, 221, 12, 59, 80, 118, 181, 27, 8, 6, 37, 176, 223, 90, 107, 102, 158, 98, 77, 122, 214, 71, 186, 157, 45, 220, 43, 201, 115, 20, 139, 4, 19, 146, 166, 24, 16, 95, 14, 170, 61, 18, 123, 96, 132, 110, 39, 91, 126, 216, 2, 204, 190, 173, 178, 154, 108, 79, 218, 63, 125, 215, 72, 67, 197, 227, 183, 165, 159, 49, 116, 103, 192, 222, 177, 208, 128, 162, 210, 23, 200, 82, 129, 47, 7, 26, 225, 169, 29, 42, 229, 188, 137, 135, 97, 233, 147, 150, 185, 179, 56, 211, 196, 88, 73, 230, 86, 180, 99, 205, 156, 22, 148, 38, 83, 54, 112, 105, 17, 191, 36, 21, 168, 207, 28, 92, 93, 30, 48, 117, 33, 143, 127, 53, 15, 237, 121, 152, 163, 69, 109, 57, 228, 10, 212, 89, 202, 130, 172, 217, 32, 199, 149, 232, 138, 34, 50, 209, 140, 175, 13, 171, 136, 144, 25, 184, 194, 226, 78, 9, 87, 134, 106, 151, 66, 231, 60, 84, 75, 224, 94, 187, 41, 68, 65, 155, 64, 40, 31, 203, 213, 131, 120, 51, 198, 193, 55, 1, 161, 5, 114, 234, 206, 141, 100, 74, 104, 11, 167, 133, 145, 111, 85, 52, 119, 46, 236, 35, 189, 182, 70, 160, 58, 219, 81, 142, 195, 124, 113, 153, 3]
[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1]
[1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 2 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.619
[ 2 / 40 ] loss: 0.723
[ 3 / 40 ] loss: 0.633
[ 4 / 40 ] loss: 0.582
[ 5 / 40 ] loss: 0.646
[ 6 / 40 ] loss: 0.660
[ 7 / 40 ] loss: 0.646
[ 8 / 40 ] loss: 0.719
[ 9 / 40 ] loss: 0.652
[ 10 / 40 ] loss: 0.688
[ 11 / 40 ] loss: 0.678
[ 12 / 40 ] loss: 0.667
[ 13 / 40 ] loss: 0.767
[ 14 / 40 ] loss: 0.551
[ 15 / 40 ] loss: 0.642
[ 16 / 40 ] loss: 0.670
[ 17 / 40 ] loss: 0.741
[ 18 / 40 ] loss: 0.753
[ 19 / 40 ] loss: 0.707
[ 20 / 40 ] loss: 0.664
[ 21 / 40 ] loss: 0.651
[ 22 / 40 ] loss: 0.705
[ 23 / 40 ] loss: 0.640
[ 24 / 40 ] loss: 0.701
[ 25 / 40 ] loss: 0.673
[ 26 / 40 ] loss: 0.674
[ 27 / 40 ] loss: 0.652
[ 28 / 40 ] loss: 0.690
[ 29 / 40 ] loss: 0.646
[ 30 / 40 ] loss: 0.689
[ 31 / 40 ] loss: 0.739
[ 32 / 40 ] loss: 0.629
[ 33 / 40 ] loss: 0.654
[ 34 / 40 ] loss: 0.681
[ 35 / 40 ] loss: 0.732
[ 36 / 40 ] loss: 0.687
[ 37 / 40 ] loss: 0.691
[ 38 / 40 ] loss: 0.655
[ 39 / 40 ] loss: 0.708
[ 40 / 40 ] loss: 0.680
0.6746723160147667
Accuracy: 0.641350 -- Precision: 0.621622 -- Recall: 0.884615 -- F1: 0.730159 -- AUC: 0.714953
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[171, 92, 23, 169, 131, 93, 183, 26, 207, 138, 210, 4, 197, 80, 79, 69, 82, 9, 85, 185, 114, 107, 173, 152, 178, 148, 84, 206, 66, 147, 55, 153, 113, 25, 168, 224, 5, 13, 63, 112, 19, 50, 129, 160, 123, 86, 91, 124, 70, 57, 221, 125, 228, 48, 103, 77, 18, 78, 167, 119, 133, 219, 126, 12, 120, 165, 20, 177, 102, 83, 64, 71, 204, 230, 15, 222, 236, 90, 128, 43, 58, 28, 75, 130, 237, 44, 45, 176, 68, 141, 223, 89, 47, 94, 161, 166, 101, 170, 53, 205, 96, 144, 132, 232, 21, 225, 217, 81, 191, 121, 212, 187, 154, 188, 29, 184, 159, 76, 8, 65, 196, 231, 214, 189, 37, 49, 134, 195, 52, 118, 61, 140, 74, 100, 10, 14, 155, 104, 181, 164, 216, 30, 46, 186, 158, 174, 192, 156, 3, 208, 200, 172, 157, 229, 67, 34, 136, 211, 202, 6, 234, 24, 27, 98, 122, 162, 87, 1, 233, 194, 137, 179, 97, 150, 105, 106, 88, 139, 145, 40, 16, 95, 213, 142, 7, 115, 143, 38, 149, 201, 198, 109, 151, 190, 203, 227, 35, 163, 22, 110, 99, 111, 72, 36, 42, 56, 218, 116, 226, 62, 60, 41, 215, 146, 31, 39, 11, 193, 182, 199, 117, 127, 17, 209, 135, 175, 32, 51, 2, 54, 73, 220, 33, 108, 235, 59, 180]
[1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]
[1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 3 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.634
[ 2 / 40 ] loss: 0.659
[ 3 / 40 ] loss: 0.638
[ 4 / 40 ] loss: 0.629
[ 5 / 40 ] loss: 0.713
[ 6 / 40 ] loss: 0.661
[ 7 / 40 ] loss: 0.683
[ 8 / 40 ] loss: 0.714
[ 9 / 40 ] loss: 0.580
[ 10 / 40 ] loss: 0.644
[ 11 / 40 ] loss: 0.605
[ 12 / 40 ] loss: 0.644
[ 13 / 40 ] loss: 0.651
[ 14 / 40 ] loss: 0.609
[ 15 / 40 ] loss: 0.570
[ 16 / 40 ] loss: 0.703
[ 17 / 40 ] loss: 0.611
[ 18 / 40 ] loss: 0.596
[ 19 / 40 ] loss: 0.603
[ 20 / 40 ] loss: 0.535
[ 21 / 40 ] loss: 0.629
[ 22 / 40 ] loss: 0.640
[ 23 / 40 ] loss: 0.613
[ 24 / 40 ] loss: 0.609
[ 25 / 40 ] loss: 0.610
[ 26 / 40 ] loss: 0.624
[ 27 / 40 ] loss: 0.567
[ 28 / 40 ] loss: 0.560
[ 29 / 40 ] loss: 0.659
[ 30 / 40 ] loss: 0.577
[ 31 / 40 ] loss: 0.703
[ 32 / 40 ] loss: 0.595
[ 33 / 40 ] loss: 0.600
[ 34 / 40 ] loss: 0.473
[ 35 / 40 ] loss: 0.618
[ 36 / 40 ] loss: 0.515
[ 37 / 40 ] loss: 0.565
[ 38 / 40 ] loss: 0.629
[ 39 / 40 ] loss: 0.627
[ 40 / 40 ] loss: 0.339
0.610815017670393
Accuracy: 0.637131 -- Precision: 0.615789 -- Recall: 0.900000 -- F1: 0.731250 -- AUC: 0.760388
========= epoch: 4 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.525
[ 2 / 40 ] loss: 0.575
[ 3 / 40 ] loss: 0.587
[ 4 / 40 ] loss: 0.556
[ 5 / 40 ] loss: 0.474
[ 6 / 40 ] loss: 0.689
[ 7 / 40 ] loss: 0.580
[ 8 / 40 ] loss: 0.600
[ 9 / 40 ] loss: 0.665
[ 10 / 40 ] loss: 0.521
[ 11 / 40 ] loss: 0.738
[ 12 / 40 ] loss: 0.746
[ 13 / 40 ] loss: 0.686
[ 14 / 40 ] loss: 0.578
[ 15 / 40 ] loss: 0.719
[ 16 / 40 ] loss: 0.600
[ 17 / 40 ] loss: 0.609
[ 18 / 40 ] loss: 0.502
[ 19 / 40 ] loss: 0.573
[ 20 / 40 ] loss: 0.563
[ 21 / 40 ] loss: 0.442
[ 22 / 40 ] loss: 0.514
[ 23 / 40 ] loss: 0.582
[ 24 / 40 ] loss: 0.538
[ 25 / 40 ] loss: 0.508
[ 26 / 40 ] loss: 0.465
[ 27 / 40 ] loss: 0.349
[ 28 / 40 ] loss: 0.463
[ 29 / 40 ] loss: 0.746
[ 30 / 40 ] loss: 0.495
[ 31 / 40 ] loss: 0.771
[ 32 / 40 ] loss: 0.624
[ 33 / 40 ] loss: 0.528
[ 34 / 40 ] loss: 0.712
[ 35 / 40 ] loss: 0.596
[ 36 / 40 ] loss: 0.736
[ 37 / 40 ] loss: 0.575
[ 38 / 40 ] loss: 0.697
[ 39 / 40 ] loss: 0.518
[ 40 / 40 ] loss: 0.804
0.5937503039836883
Accuracy: 0.624473 -- Precision: 0.597156 -- Recall: 0.969231 -- F1: 0.739003 -- AUC: 0.719123
========= epoch: 5 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.603
[ 2 / 40 ] loss: 0.669
[ 3 / 40 ] loss: 0.682
[ 4 / 40 ] loss: 0.620
[ 5 / 40 ] loss: 0.599
[ 6 / 40 ] loss: 0.591
[ 7 / 40 ] loss: 0.605
[ 8 / 40 ] loss: 0.667
[ 9 / 40 ] loss: 0.496
[ 10 / 40 ] loss: 0.523
[ 11 / 40 ] loss: 0.474
[ 12 / 40 ] loss: 0.481
[ 13 / 40 ] loss: 0.427
[ 14 / 40 ] loss: 0.535
[ 15 / 40 ] loss: 0.551
[ 16 / 40 ] loss: 0.528
[ 17 / 40 ] loss: 0.548
[ 18 / 40 ] loss: 0.622
[ 19 / 40 ] loss: 0.531
[ 20 / 40 ] loss: 0.501
[ 21 / 40 ] loss: 0.728
[ 22 / 40 ] loss: 0.612
[ 23 / 40 ] loss: 0.494
[ 24 / 40 ] loss: 0.780
[ 25 / 40 ] loss: 0.533
[ 26 / 40 ] loss: 0.363
[ 27 / 40 ] loss: 0.678
[ 28 / 40 ] loss: 0.568
[ 29 / 40 ] loss: 0.442
[ 30 / 40 ] loss: 0.671
[ 31 / 40 ] loss: 0.498
[ 32 / 40 ] loss: 0.399
[ 33 / 40 ] loss: 0.353
[ 34 / 40 ] loss: 0.399
[ 35 / 40 ] loss: 0.380
[ 36 / 40 ] loss: 0.564
[ 37 / 40 ] loss: 0.567
[ 38 / 40 ] loss: 0.445
[ 39 / 40 ] loss: 0.367
[ 40 / 40 ] loss: 0.557
0.5413013249635696
Accuracy: 0.679325 -- Precision: 0.690141 -- Recall: 0.753846 -- F1: 0.720588 -- AUC: 0.758447
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[44, 112, 42, 56, 210, 236, 87, 79, 33, 108, 97, 139, 61, 103, 172, 156, 205, 17, 191, 35, 149, 7, 102, 14, 132, 24, 23, 13, 64, 134, 146, 34, 230, 154, 18, 12, 150, 54, 80, 229, 98, 187, 153, 131, 85, 1, 117, 114, 223, 2, 82, 116, 138, 186, 197, 52, 111, 219, 174, 155, 221, 15, 70, 182, 51, 209, 72, 184, 206, 104, 28, 71, 203, 235, 110, 214, 45, 189, 231, 77, 9, 11, 216, 207, 142, 121, 84, 212, 119, 143, 178, 148, 38, 169, 147, 202, 173, 158, 66, 19, 199, 124, 120, 122, 179, 96, 224, 126, 226, 160, 198, 129, 168, 234, 145, 211, 217, 16, 196, 99, 208, 78, 109, 136, 188, 6, 60, 106, 73, 41, 49, 193, 107, 125, 21, 37, 29, 201, 53, 171, 164, 113, 218, 3, 95, 159, 47, 135, 166, 144, 194, 152, 137, 228, 183, 90, 40, 93, 94, 157, 128, 192, 69, 175, 177, 20, 115, 62, 233, 26, 81, 92, 227, 39, 130, 127, 63, 86, 46, 190, 181, 213, 89, 30, 222, 167, 57, 91, 83, 55, 237, 43, 163, 161, 204, 25, 151, 100, 75, 74, 27, 67, 58, 48, 68, 123, 10, 101, 170, 185, 65, 22, 200, 180, 88, 140, 32, 162, 105, 232, 215, 36, 76, 118, 4, 225, 50, 8, 141, 133, 59, 195, 176, 220, 5, 165, 31]
[1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0]
[0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 6 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.441
[ 2 / 40 ] loss: 0.466
[ 3 / 40 ] loss: 0.463
[ 4 / 40 ] loss: 0.454
[ 5 / 40 ] loss: 0.592
[ 6 / 40 ] loss: 0.404
[ 7 / 40 ] loss: 0.609
[ 8 / 40 ] loss: 0.502
[ 9 / 40 ] loss: 0.509
[ 10 / 40 ] loss: 0.375
[ 11 / 40 ] loss: 0.454
[ 12 / 40 ] loss: 0.477
[ 13 / 40 ] loss: 0.368
[ 14 / 40 ] loss: 0.574
[ 15 / 40 ] loss: 0.655
[ 16 / 40 ] loss: 0.540
[ 17 / 40 ] loss: 0.373
[ 18 / 40 ] loss: 0.427
[ 19 / 40 ] loss: 0.425
[ 20 / 40 ] loss: 0.344
[ 21 / 40 ] loss: 0.653
[ 22 / 40 ] loss: 0.372
[ 23 / 40 ] loss: 0.387
[ 24 / 40 ] loss: 0.406
[ 25 / 40 ] loss: 0.283
[ 26 / 40 ] loss: 0.420
[ 27 / 40 ] loss: 0.375
[ 28 / 40 ] loss: 0.731
[ 29 / 40 ] loss: 0.723
[ 30 / 40 ] loss: 0.653
[ 31 / 40 ] loss: 0.393
[ 32 / 40 ] loss: 0.411
[ 33 / 40 ] loss: 0.473
[ 34 / 40 ] loss: 0.447
[ 35 / 40 ] loss: 0.325
[ 36 / 40 ] loss: 0.379
[ 37 / 40 ] loss: 0.550
[ 38 / 40 ] loss: 0.613
[ 39 / 40 ] loss: 0.657
[ 40 / 40 ] loss: 0.578
0.4819589599967003
Accuracy: 0.713080 -- Precision: 0.781818 -- Recall: 0.661538 -- F1: 0.716667 -- AUC: 0.809130
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[20, 133, 97, 2, 24, 168, 228, 46, 204, 13, 116, 6, 136, 128, 22, 95, 108, 214, 134, 162, 110, 153, 117, 236, 219, 45, 207, 75, 193, 50, 118, 85, 3, 43, 127, 23, 88, 106, 93, 125, 185, 206, 208, 15, 91, 74, 21, 157, 179, 120, 51, 129, 180, 184, 215, 100, 114, 217, 79, 78, 71, 202, 187, 87, 63, 86, 203, 176, 26, 27, 72, 235, 39, 66, 29, 216, 99, 156, 213, 192, 149, 190, 101, 70, 198, 36, 8, 139, 150, 154, 94, 164, 144, 11, 211, 143, 221, 14, 171, 16, 232, 33, 231, 165, 61, 130, 31, 5, 105, 177, 226, 38, 9, 234, 90, 124, 68, 194, 145, 181, 123, 142, 47, 183, 83, 37, 122, 138, 230, 195, 113, 169, 197, 102, 1, 67, 18, 220, 224, 178, 166, 56, 12, 17, 54, 135, 64, 225, 10, 148, 84, 112, 172, 52, 201, 28, 41, 188, 174, 25, 126, 57, 131, 59, 159, 40, 212, 152, 137, 186, 170, 229, 115, 167, 218, 233, 35, 163, 132, 205, 98, 121, 82, 173, 119, 237, 182, 200, 62, 107, 209, 109, 111, 155, 30, 65, 76, 199, 55, 147, 58, 19, 175, 77, 42, 34, 141, 81, 69, 7, 227, 49, 223, 89, 104, 146, 158, 96, 73, 60, 161, 48, 160, 53, 80, 32, 210, 191, 222, 4, 189, 151, 44, 92, 140, 196, 103]
[0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1]
[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 7 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.465
[ 2 / 40 ] loss: 0.506
[ 3 / 40 ] loss: 0.386
[ 4 / 40 ] loss: 0.501
[ 5 / 40 ] loss: 0.376
[ 6 / 40 ] loss: 0.464
[ 7 / 40 ] loss: 0.670
[ 8 / 40 ] loss: 0.458
[ 9 / 40 ] loss: 0.552
[ 10 / 40 ] loss: 0.546
[ 11 / 40 ] loss: 0.244
[ 12 / 40 ] loss: 0.424
[ 13 / 40 ] loss: 0.328
[ 14 / 40 ] loss: 0.216
[ 15 / 40 ] loss: 0.348
[ 16 / 40 ] loss: 0.412
[ 17 / 40 ] loss: 0.535
[ 18 / 40 ] loss: 0.552
[ 19 / 40 ] loss: 0.254
[ 20 / 40 ] loss: 0.373
[ 21 / 40 ] loss: 0.336
[ 22 / 40 ] loss: 0.366
[ 23 / 40 ] loss: 0.685
[ 24 / 40 ] loss: 0.444
[ 25 / 40 ] loss: 0.389
[ 26 / 40 ] loss: 0.391
[ 27 / 40 ] loss: 0.448
[ 28 / 40 ] loss: 0.584
[ 29 / 40 ] loss: 0.740
[ 30 / 40 ] loss: 0.352
[ 31 / 40 ] loss: 0.775
[ 32 / 40 ] loss: 0.529
[ 33 / 40 ] loss: 0.566
[ 34 / 40 ] loss: 0.394
[ 35 / 40 ] loss: 0.465
[ 36 / 40 ] loss: 0.450
[ 37 / 40 ] loss: 0.415
[ 38 / 40 ] loss: 0.391
[ 39 / 40 ] loss: 0.558
[ 40 / 40 ] loss: 0.555
0.4610670380294323
Accuracy: 0.738397 -- Precision: 0.769841 -- Recall: 0.746154 -- F1: 0.757813 -- AUC: 0.799281
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[34, 122, 132, 37, 5, 219, 178, 55, 68, 108, 1, 220, 105, 81, 226, 217, 12, 29, 97, 90, 160, 113, 185, 89, 223, 106, 25, 205, 235, 154, 206, 73, 102, 155, 186, 136, 13, 180, 125, 19, 222, 120, 202, 228, 162, 112, 7, 14, 38, 172, 198, 32, 35, 54, 94, 199, 3, 142, 137, 216, 193, 164, 44, 58, 30, 145, 96, 80, 161, 111, 144, 236, 166, 139, 192, 8, 43, 163, 31, 39, 107, 194, 103, 177, 109, 100, 62, 63, 18, 200, 152, 201, 45, 56, 232, 9, 135, 209, 48, 171, 20, 151, 150, 110, 42, 23, 204, 27, 16, 46, 158, 196, 224, 214, 212, 51, 215, 75, 225, 52, 78, 187, 50, 74, 183, 87, 91, 117, 49, 237, 61, 175, 66, 190, 86, 92, 4, 173, 208, 211, 234, 98, 227, 71, 218, 153, 15, 188, 79, 36, 167, 33, 101, 129, 191, 127, 174, 24, 17, 26, 203, 231, 53, 207, 157, 134, 123, 126, 131, 121, 143, 21, 165, 146, 76, 195, 147, 93, 179, 6, 28, 149, 115, 65, 184, 59, 85, 181, 2, 99, 70, 230, 176, 130, 69, 114, 170, 221, 77, 213, 119, 168, 116, 210, 182, 229, 60, 11, 233, 128, 22, 197, 40, 138, 156, 64, 95, 47, 41, 82, 169, 141, 88, 84, 148, 189, 118, 159, 57, 124, 140, 10, 133, 83, 104, 67, 72]
[0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0]
[0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 8 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.405
[ 2 / 40 ] loss: 0.519
[ 3 / 40 ] loss: 0.536
[ 4 / 40 ] loss: 0.474
[ 5 / 40 ] loss: 0.464
[ 6 / 40 ] loss: 0.396
[ 7 / 40 ] loss: 0.453
[ 8 / 40 ] loss: 0.340
[ 9 / 40 ] loss: 0.527
[ 10 / 40 ] loss: 0.482
[ 11 / 40 ] loss: 0.405
[ 12 / 40 ] loss: 0.244
[ 13 / 40 ] loss: 0.355
[ 14 / 40 ] loss: 0.606
[ 15 / 40 ] loss: 0.389
[ 16 / 40 ] loss: 0.509
[ 17 / 40 ] loss: 0.364
[ 18 / 40 ] loss: 0.259
[ 19 / 40 ] loss: 0.240
[ 20 / 40 ] loss: 0.394
[ 21 / 40 ] loss: 0.371
[ 22 / 40 ] loss: 0.433
[ 23 / 40 ] loss: 0.357
[ 24 / 40 ] loss: 0.213
[ 25 / 40 ] loss: 0.507
[ 26 / 40 ] loss: 0.246
[ 27 / 40 ] loss: 0.453
[ 28 / 40 ] loss: 0.433
[ 29 / 40 ] loss: 0.206
[ 30 / 40 ] loss: 0.518
[ 31 / 40 ] loss: 0.405
[ 32 / 40 ] loss: 0.331
[ 33 / 40 ] loss: 0.320
[ 34 / 40 ] loss: 0.584
[ 35 / 40 ] loss: 0.280
[ 36 / 40 ] loss: 0.684
[ 37 / 40 ] loss: 0.292
[ 38 / 40 ] loss: 0.492
[ 39 / 40 ] loss: 0.374
[ 40 / 40 ] loss: 0.202
0.40152355171740056
Accuracy: 0.717300 -- Precision: 0.711409 -- Recall: 0.815385 -- F1: 0.759857 -- AUC: 0.759885
========= epoch: 9 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.422
[ 2 / 40 ] loss: 0.267
[ 3 / 40 ] loss: 0.130
[ 4 / 40 ] loss: 0.200
[ 5 / 40 ] loss: 0.313
[ 6 / 40 ] loss: 0.333
[ 7 / 40 ] loss: 0.422
[ 8 / 40 ] loss: 0.602
[ 9 / 40 ] loss: 0.396
[ 10 / 40 ] loss: 0.473
[ 11 / 40 ] loss: 0.632
[ 12 / 40 ] loss: 0.410
[ 13 / 40 ] loss: 0.376
[ 14 / 40 ] loss: 0.351
[ 15 / 40 ] loss: 0.434
[ 16 / 40 ] loss: 0.389
[ 17 / 40 ] loss: 0.437
[ 18 / 40 ] loss: 0.425
[ 19 / 40 ] loss: 0.408
[ 20 / 40 ] loss: 0.659
[ 21 / 40 ] loss: 0.230
[ 22 / 40 ] loss: 0.557
[ 23 / 40 ] loss: 0.517
[ 24 / 40 ] loss: 0.245
[ 25 / 40 ] loss: 0.317
[ 26 / 40 ] loss: 0.247
[ 27 / 40 ] loss: 0.521
[ 28 / 40 ] loss: 0.592
[ 29 / 40 ] loss: 0.313
[ 30 / 40 ] loss: 0.507
[ 31 / 40 ] loss: 0.358
[ 32 / 40 ] loss: 0.320
[ 33 / 40 ] loss: 0.329
[ 34 / 40 ] loss: 0.450
[ 35 / 40 ] loss: 0.710
[ 36 / 40 ] loss: 0.258
[ 37 / 40 ] loss: 0.285
[ 38 / 40 ] loss: 0.478
[ 39 / 40 ] loss: 0.181
[ 40 / 40 ] loss: 0.251
0.39363133870065214
Accuracy: 0.734177 -- Precision: 0.768000 -- Recall: 0.738462 -- F1: 0.752941 -- AUC: 0.793134
========= epoch: 10 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.197
[ 2 / 40 ] loss: 0.383
[ 3 / 40 ] loss: 0.304
[ 4 / 40 ] loss: 0.403
[ 5 / 40 ] loss: 0.391
[ 6 / 40 ] loss: 0.401
[ 7 / 40 ] loss: 0.480
[ 8 / 40 ] loss: 0.406
[ 9 / 40 ] loss: 0.459
[ 10 / 40 ] loss: 0.258
[ 11 / 40 ] loss: 0.216
[ 12 / 40 ] loss: 0.433
[ 13 / 40 ] loss: 0.493
[ 14 / 40 ] loss: 0.457
[ 15 / 40 ] loss: 0.389
[ 16 / 40 ] loss: 0.306
[ 17 / 40 ] loss: 0.443
[ 18 / 40 ] loss: 0.302
[ 19 / 40 ] loss: 0.199
[ 20 / 40 ] loss: 0.403
[ 21 / 40 ] loss: 0.320
[ 22 / 40 ] loss: 0.423
[ 23 / 40 ] loss: 0.289
[ 24 / 40 ] loss: 0.468
[ 25 / 40 ] loss: 0.228
[ 26 / 40 ] loss: 0.495
[ 27 / 40 ] loss: 0.358
[ 28 / 40 ] loss: 0.387
[ 29 / 40 ] loss: 0.307
[ 30 / 40 ] loss: 0.379
[ 31 / 40 ] loss: 0.564
[ 32 / 40 ] loss: 0.330
[ 33 / 40 ] loss: 0.586
[ 34 / 40 ] loss: 0.347
[ 35 / 40 ] loss: 0.462
[ 36 / 40 ] loss: 0.215
[ 37 / 40 ] loss: 0.461
[ 38 / 40 ] loss: 0.434
[ 39 / 40 ] loss: 0.212
[ 40 / 40 ] loss: 0.364
0.3737888980656862
Accuracy: 0.704641 -- Precision: 0.685185 -- Recall: 0.853846 -- F1: 0.760274 -- AUC: 0.805248
========= epoch: 11 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.627
[ 2 / 40 ] loss: 0.358
[ 3 / 40 ] loss: 0.411
[ 4 / 40 ] loss: 0.211
[ 5 / 40 ] loss: 0.337
[ 6 / 40 ] loss: 0.252
[ 7 / 40 ] loss: 0.477
[ 8 / 40 ] loss: 0.292
[ 9 / 40 ] loss: 0.314
[ 10 / 40 ] loss: 0.245
[ 11 / 40 ] loss: 0.315
[ 12 / 40 ] loss: 0.358
[ 13 / 40 ] loss: 0.411
[ 14 / 40 ] loss: 0.328
[ 15 / 40 ] loss: 0.311
[ 16 / 40 ] loss: 0.459
[ 17 / 40 ] loss: 0.370
[ 18 / 40 ] loss: 0.362
[ 19 / 40 ] loss: 0.395
[ 20 / 40 ] loss: 0.256
[ 21 / 40 ] loss: 0.293
[ 22 / 40 ] loss: 0.308
[ 23 / 40 ] loss: 0.149
[ 24 / 40 ] loss: 0.315
[ 25 / 40 ] loss: 0.196
[ 26 / 40 ] loss: 0.546
[ 27 / 40 ] loss: 0.410
[ 28 / 40 ] loss: 0.366
[ 29 / 40 ] loss: 0.251
[ 30 / 40 ] loss: 0.577
[ 31 / 40 ] loss: 0.288
[ 32 / 40 ] loss: 0.356
[ 33 / 40 ] loss: 0.470
[ 34 / 40 ] loss: 0.383
[ 35 / 40 ] loss: 0.193
[ 36 / 40 ] loss: 0.164
[ 37 / 40 ] loss: 0.256
[ 38 / 40 ] loss: 0.431
[ 39 / 40 ] loss: 0.470
[ 40 / 40 ] loss: 0.336
0.34613760747015476
Accuracy: 0.738397 -- Precision: 0.783333 -- Recall: 0.723077 -- F1: 0.752000 -- AUC: 0.796046
========= epoch: 12 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.269
[ 2 / 40 ] loss: 0.560
[ 3 / 40 ] loss: 0.337
[ 4 / 40 ] loss: 0.298
[ 5 / 40 ] loss: 0.299
[ 6 / 40 ] loss: 0.609
[ 7 / 40 ] loss: 0.394
[ 8 / 40 ] loss: 0.205
[ 9 / 40 ] loss: 0.502
[ 10 / 40 ] loss: 0.297
[ 11 / 40 ] loss: 0.118
[ 12 / 40 ] loss: 0.374
[ 13 / 40 ] loss: 0.466
[ 14 / 40 ] loss: 0.594
[ 15 / 40 ] loss: 0.282
[ 16 / 40 ] loss: 0.131
[ 17 / 40 ] loss: 0.351
[ 18 / 40 ] loss: 0.343
[ 19 / 40 ] loss: 0.304
[ 20 / 40 ] loss: 0.274
[ 21 / 40 ] loss: 0.228
[ 22 / 40 ] loss: 0.193
[ 23 / 40 ] loss: 0.223
[ 24 / 40 ] loss: 0.277
[ 25 / 40 ] loss: 0.526
[ 26 / 40 ] loss: 0.394
[ 27 / 40 ] loss: 0.205
[ 28 / 40 ] loss: 0.293
[ 29 / 40 ] loss: 0.666
[ 30 / 40 ] loss: 0.274
[ 31 / 40 ] loss: 0.582
[ 32 / 40 ] loss: 0.337
[ 33 / 40 ] loss: 0.237
[ 34 / 40 ] loss: 0.146
[ 35 / 40 ] loss: 0.342
[ 36 / 40 ] loss: 0.375
[ 37 / 40 ] loss: 0.323
[ 38 / 40 ] loss: 0.394
[ 39 / 40 ] loss: 0.337
[ 40 / 40 ] loss: 0.317
0.341900384798646
Accuracy: 0.729958 -- Precision: 0.742647 -- Recall: 0.776923 -- F1: 0.759398 -- AUC: 0.794968
========= epoch: 13 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.303
[ 2 / 40 ] loss: 0.373
[ 3 / 40 ] loss: 0.471
[ 4 / 40 ] loss: 0.230
[ 5 / 40 ] loss: 0.601
[ 6 / 40 ] loss: 0.247
[ 7 / 40 ] loss: 0.396
[ 8 / 40 ] loss: 0.308
[ 9 / 40 ] loss: 0.288
[ 10 / 40 ] loss: 0.216
[ 11 / 40 ] loss: 0.194
[ 12 / 40 ] loss: 0.386
[ 13 / 40 ] loss: 0.361
[ 14 / 40 ] loss: 0.201
[ 15 / 40 ] loss: 0.352
[ 16 / 40 ] loss: 0.165
[ 17 / 40 ] loss: 0.364
[ 18 / 40 ] loss: 0.223
[ 19 / 40 ] loss: 0.384
[ 20 / 40 ] loss: 0.191
[ 21 / 40 ] loss: 0.487
[ 22 / 40 ] loss: 0.200
[ 23 / 40 ] loss: 0.300
[ 24 / 40 ] loss: 0.214
[ 25 / 40 ] loss: 0.280
[ 26 / 40 ] loss: 0.349
[ 27 / 40 ] loss: 0.185
[ 28 / 40 ] loss: 0.221
[ 29 / 40 ] loss: 0.301
[ 30 / 40 ] loss: 0.197
[ 31 / 40 ] loss: 0.528
[ 32 / 40 ] loss: 0.451
[ 33 / 40 ] loss: 0.485
[ 34 / 40 ] loss: 0.232
[ 35 / 40 ] loss: 0.276
[ 36 / 40 ] loss: 0.535
[ 37 / 40 ] loss: 0.299
[ 38 / 40 ] loss: 0.509
[ 39 / 40 ] loss: 0.192
[ 40 / 40 ] loss: 0.153
0.3162077620625496
Accuracy: 0.734177 -- Precision: 0.727891 -- Recall: 0.823077 -- F1: 0.772563 -- AUC: 0.779152
========= epoch: 14 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.471
[ 2 / 40 ] loss: 0.335
[ 3 / 40 ] loss: 0.182
[ 4 / 40 ] loss: 0.189
[ 5 / 40 ] loss: 0.198
[ 6 / 40 ] loss: 0.239
[ 7 / 40 ] loss: 0.177
[ 8 / 40 ] loss: 0.103
[ 9 / 40 ] loss: 0.222
[ 10 / 40 ] loss: 0.499
[ 11 / 40 ] loss: 0.219
[ 12 / 40 ] loss: 0.315
[ 13 / 40 ] loss: 0.402
[ 14 / 40 ] loss: 0.087
[ 15 / 40 ] loss: 0.305
[ 16 / 40 ] loss: 0.360
[ 17 / 40 ] loss: 0.334
[ 18 / 40 ] loss: 0.361
[ 19 / 40 ] loss: 0.176
[ 20 / 40 ] loss: 0.362
[ 21 / 40 ] loss: 0.212
[ 22 / 40 ] loss: 0.173
[ 23 / 40 ] loss: 0.557
[ 24 / 40 ] loss: 0.404
[ 25 / 40 ] loss: 0.323
[ 26 / 40 ] loss: 0.391
[ 27 / 40 ] loss: 0.605
[ 28 / 40 ] loss: 0.270
[ 29 / 40 ] loss: 0.089
[ 30 / 40 ] loss: 0.709
[ 31 / 40 ] loss: 0.468
[ 32 / 40 ] loss: 0.199
[ 33 / 40 ] loss: 0.456
[ 34 / 40 ] loss: 0.189
[ 35 / 40 ] loss: 0.377
[ 36 / 40 ] loss: 0.365
[ 37 / 40 ] loss: 0.217
[ 38 / 40 ] loss: 0.199
[ 39 / 40 ] loss: 0.348
[ 40 / 40 ] loss: 0.333
0.31048631239682434
Accuracy: 0.746835 -- Precision: 0.736486 -- Recall: 0.838462 -- F1: 0.784173 -- AUC: 0.810208
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[158, 181, 195, 117, 108, 20, 96, 90, 7, 56, 126, 2, 182, 171, 205, 132, 64, 12, 21, 23, 89, 47, 201, 211, 1, 207, 147, 234, 83, 146, 41, 36, 105, 155, 213, 190, 106, 236, 17, 111, 52, 61, 11, 214, 138, 78, 93, 164, 177, 120, 15, 221, 8, 5, 153, 216, 68, 136, 149, 202, 70, 88, 3, 187, 6, 109, 51, 40, 16, 4, 69, 162, 133, 208, 172, 156, 91, 35, 77, 196, 84, 74, 193, 170, 176, 31, 233, 9, 103, 150, 151, 97, 225, 92, 85, 140, 229, 143, 13, 145, 178, 82, 25, 113, 135, 237, 30, 80, 137, 49, 34, 98, 72, 33, 235, 218, 228, 125, 48, 183, 43, 42, 129, 99, 86, 32, 209, 212, 79, 119, 76, 185, 198, 175, 73, 121, 191, 102, 124, 38, 95, 134, 148, 14, 203, 57, 10, 197, 194, 27, 59, 112, 114, 157, 101, 131, 130, 144, 123, 26, 53, 115, 161, 169, 223, 231, 227, 167, 50, 58, 232, 184, 87, 160, 63, 206, 66, 192, 62, 188, 154, 107, 46, 189, 60, 29, 210, 118, 19, 110, 215, 174, 45, 173, 55, 186, 200, 199, 217, 44, 22, 222, 220, 204, 166, 139, 163, 142, 18, 128, 39, 168, 159, 100, 54, 219, 179, 224, 104, 152, 71, 180, 141, 122, 127, 75, 65, 67, 37, 28, 94, 230, 24, 116, 165, 81, 226]
[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 15 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.278
[ 2 / 40 ] loss: 0.216
[ 3 / 40 ] loss: 0.464
[ 4 / 40 ] loss: 0.200
[ 5 / 40 ] loss: 0.338
[ 6 / 40 ] loss: 0.350
[ 7 / 40 ] loss: 0.156
[ 8 / 40 ] loss: 0.268
[ 9 / 40 ] loss: 0.453
[ 10 / 40 ] loss: 0.109
[ 11 / 40 ] loss: 0.106
[ 12 / 40 ] loss: 0.685
[ 13 / 40 ] loss: 0.322
[ 14 / 40 ] loss: 0.177
[ 15 / 40 ] loss: 0.223
[ 16 / 40 ] loss: 0.221
[ 17 / 40 ] loss: 0.118
[ 18 / 40 ] loss: 0.262
[ 19 / 40 ] loss: 0.458
[ 20 / 40 ] loss: 0.127
[ 21 / 40 ] loss: 0.454
[ 22 / 40 ] loss: 0.294
[ 23 / 40 ] loss: 0.532
[ 24 / 40 ] loss: 0.262
[ 25 / 40 ] loss: 0.218
[ 26 / 40 ] loss: 0.179
[ 27 / 40 ] loss: 0.177
[ 28 / 40 ] loss: 0.431
[ 29 / 40 ] loss: 0.092
[ 30 / 40 ] loss: 0.256
[ 31 / 40 ] loss: 0.226
[ 32 / 40 ] loss: 0.212
[ 33 / 40 ] loss: 0.323
[ 34 / 40 ] loss: 0.164
[ 35 / 40 ] loss: 0.115
[ 36 / 40 ] loss: 0.476
[ 37 / 40 ] loss: 0.193
[ 38 / 40 ] loss: 0.476
[ 39 / 40 ] loss: 0.514
[ 40 / 40 ] loss: 0.090
0.28047166094183923
Accuracy: 0.751055 -- Precision: 0.759124 -- Recall: 0.800000 -- F1: 0.779026 -- AUC: 0.815816
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[77, 86, 148, 127, 166, 32, 228, 75, 76, 222, 158, 10, 213, 139, 122, 188, 101, 181, 60, 230, 210, 45, 131, 135, 46, 121, 224, 237, 42, 34, 61, 132, 236, 124, 12, 66, 170, 189, 6, 152, 44, 87, 187, 204, 146, 103, 150, 183, 182, 156, 214, 235, 202, 212, 169, 201, 179, 8, 118, 215, 223, 47, 80, 154, 104, 141, 56, 197, 7, 117, 206, 168, 62, 51, 27, 78, 20, 38, 120, 221, 73, 74, 52, 82, 159, 36, 178, 140, 96, 19, 157, 153, 233, 220, 15, 161, 3, 231, 95, 165, 232, 198, 43, 176, 174, 218, 108, 24, 99, 164, 119, 89, 105, 123, 97, 147, 48, 155, 59, 64, 14, 2, 138, 84, 142, 11, 106, 116, 115, 50, 79, 205, 137, 114, 217, 129, 94, 151, 225, 160, 28, 91, 88, 171, 162, 133, 85, 33, 186, 194, 172, 112, 70, 18, 37, 5, 144, 35, 40, 163, 13, 22, 216, 227, 100, 136, 109, 41, 53, 55, 98, 190, 111, 184, 134, 219, 175, 29, 31, 226, 93, 1, 68, 200, 63, 196, 113, 57, 145, 173, 229, 67, 54, 17, 49, 26, 107, 39, 193, 209, 192, 208, 4, 199, 92, 58, 9, 90, 65, 195, 21, 23, 69, 207, 72, 130, 191, 81, 185, 167, 71, 110, 128, 180, 203, 30, 102, 83, 234, 143, 126, 16, 177, 149, 211, 125, 25]
[0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0]
[0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 16 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.084
[ 2 / 40 ] loss: 0.429
[ 3 / 40 ] loss: 0.437
[ 4 / 40 ] loss: 0.246
[ 5 / 40 ] loss: 0.262
[ 6 / 40 ] loss: 0.373
[ 7 / 40 ] loss: 0.444
[ 8 / 40 ] loss: 0.292
[ 9 / 40 ] loss: 0.185
[ 10 / 40 ] loss: 0.239
[ 11 / 40 ] loss: 0.228
[ 12 / 40 ] loss: 0.187
[ 13 / 40 ] loss: 0.370
[ 14 / 40 ] loss: 0.326
[ 15 / 40 ] loss: 0.210
[ 16 / 40 ] loss: 0.236
[ 17 / 40 ] loss: 0.221
[ 18 / 40 ] loss: 0.264
[ 19 / 40 ] loss: 0.243
[ 20 / 40 ] loss: 0.362
[ 21 / 40 ] loss: 0.233
[ 22 / 40 ] loss: 0.134
[ 23 / 40 ] loss: 0.209
[ 24 / 40 ] loss: 0.181
[ 25 / 40 ] loss: 0.134
[ 26 / 40 ] loss: 0.518
[ 27 / 40 ] loss: 0.071
[ 28 / 40 ] loss: 0.545
[ 29 / 40 ] loss: 0.246
[ 30 / 40 ] loss: 0.184
[ 31 / 40 ] loss: 0.087
[ 32 / 40 ] loss: 0.354
[ 33 / 40 ] loss: 0.287
[ 34 / 40 ] loss: 0.341
[ 35 / 40 ] loss: 0.339
[ 36 / 40 ] loss: 0.329
[ 37 / 40 ] loss: 0.137
[ 38 / 40 ] loss: 0.286
[ 39 / 40 ] loss: 0.263
[ 40 / 40 ] loss: 0.056
0.2642523069866002
Accuracy: 0.755274 -- Precision: 0.764706 -- Recall: 0.800000 -- F1: 0.781955 -- AUC: 0.830841
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[128, 14, 187, 68, 210, 135, 77, 108, 205, 131, 83, 46, 109, 87, 162, 207, 85, 178, 56, 71, 194, 223, 48, 8, 222, 235, 177, 72, 125, 163, 236, 61, 69, 40, 5, 103, 12, 44, 226, 64, 75, 49, 55, 41, 141, 154, 172, 121, 95, 204, 96, 182, 150, 165, 53, 224, 192, 58, 155, 123, 129, 213, 181, 94, 120, 214, 112, 190, 106, 184, 42, 57, 110, 144, 26, 156, 84, 86, 164, 122, 179, 9, 147, 20, 206, 36, 233, 193, 70, 183, 21, 81, 107, 158, 230, 169, 7, 152, 167, 43, 209, 220, 145, 32, 143, 4, 200, 52, 211, 34, 201, 24, 10, 146, 149, 88, 195, 98, 82, 1, 234, 218, 117, 208, 39, 29, 217, 111, 18, 99, 3, 38, 216, 54, 59, 139, 91, 231, 198, 168, 97, 212, 93, 92, 202, 160, 215, 102, 25, 137, 62, 225, 191, 127, 140, 130, 60, 101, 33, 74, 45, 104, 151, 188, 227, 157, 189, 31, 67, 35, 197, 30, 174, 100, 229, 173, 73, 17, 27, 79, 159, 76, 180, 16, 185, 132, 105, 63, 113, 136, 6, 138, 228, 161, 199, 2, 114, 153, 22, 13, 175, 196, 124, 219, 66, 171, 116, 50, 80, 65, 170, 51, 186, 15, 203, 11, 221, 232, 37, 237, 47, 126, 19, 78, 118, 134, 28, 166, 133, 89, 119, 90, 142, 115, 148, 23, 176]
[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]
[1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 17 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.323
[ 2 / 40 ] loss: 0.320
[ 3 / 40 ] loss: 0.174
[ 4 / 40 ] loss: 0.276
[ 5 / 40 ] loss: 0.192
[ 6 / 40 ] loss: 0.246
[ 7 / 40 ] loss: 0.256
[ 8 / 40 ] loss: 0.122
[ 9 / 40 ] loss: 0.262
[ 10 / 40 ] loss: 0.077
[ 11 / 40 ] loss: 0.404
[ 12 / 40 ] loss: 0.243
[ 13 / 40 ] loss: 0.109
[ 14 / 40 ] loss: 0.324
[ 15 / 40 ] loss: 0.227
[ 16 / 40 ] loss: 0.212
[ 17 / 40 ] loss: 0.332
[ 18 / 40 ] loss: 0.496
[ 19 / 40 ] loss: 0.500
[ 20 / 40 ] loss: 0.341
[ 21 / 40 ] loss: 0.247
[ 22 / 40 ] loss: 0.405
[ 23 / 40 ] loss: 0.327
[ 24 / 40 ] loss: 0.335
[ 25 / 40 ] loss: 0.168
[ 26 / 40 ] loss: 0.202
[ 27 / 40 ] loss: 0.177
[ 28 / 40 ] loss: 0.142
[ 29 / 40 ] loss: 0.092
[ 30 / 40 ] loss: 0.253
[ 31 / 40 ] loss: 0.284
[ 32 / 40 ] loss: 0.243
[ 33 / 40 ] loss: 0.240
[ 34 / 40 ] loss: 0.295
[ 35 / 40 ] loss: 0.314
[ 36 / 40 ] loss: 0.134
[ 37 / 40 ] loss: 0.160
[ 38 / 40 ] loss: 0.071
[ 39 / 40 ] loss: 0.322
[ 40 / 40 ] loss: 0.383
0.2558520928025246
Accuracy: 0.751055 -- Precision: 0.779528 -- Recall: 0.761538 -- F1: 0.770428 -- AUC: 0.807189
========= epoch: 18 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.157
[ 2 / 40 ] loss: 0.260
[ 3 / 40 ] loss: 0.467
[ 4 / 40 ] loss: 0.257
[ 5 / 40 ] loss: 0.330
[ 6 / 40 ] loss: 0.082
[ 7 / 40 ] loss: 0.152
[ 8 / 40 ] loss: 0.077
[ 9 / 40 ] loss: 0.168
[ 10 / 40 ] loss: 0.129
[ 11 / 40 ] loss: 0.237
[ 12 / 40 ] loss: 0.250
[ 13 / 40 ] loss: 0.192
[ 14 / 40 ] loss: 0.116
[ 15 / 40 ] loss: 0.469
[ 16 / 40 ] loss: 0.052
[ 17 / 40 ] loss: 0.284
[ 18 / 40 ] loss: 0.072
[ 19 / 40 ] loss: 0.409
[ 20 / 40 ] loss: 0.439
[ 21 / 40 ] loss: 0.199
[ 22 / 40 ] loss: 0.301
[ 23 / 40 ] loss: 0.144
[ 24 / 40 ] loss: 0.055
[ 25 / 40 ] loss: 0.220
[ 26 / 40 ] loss: 0.094
[ 27 / 40 ] loss: 0.215
[ 28 / 40 ] loss: 0.284
[ 29 / 40 ] loss: 0.110
[ 30 / 40 ] loss: 0.069
[ 31 / 40 ] loss: 0.204
[ 32 / 40 ] loss: 0.324
[ 33 / 40 ] loss: 0.116
[ 34 / 40 ] loss: 0.448
[ 35 / 40 ] loss: 0.244
[ 36 / 40 ] loss: 0.583
[ 37 / 40 ] loss: 0.197
[ 38 / 40 ] loss: 0.261
[ 39 / 40 ] loss: 0.209
[ 40 / 40 ] loss: 0.148
0.22561517050489782
Accuracy: 0.759494 -- Precision: 0.828829 -- Recall: 0.707692 -- F1: 0.763485 -- AUC: 0.820345
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[231, 6, 217, 182, 120, 164, 98, 23, 2, 54, 166, 201, 34, 227, 210, 184, 213, 186, 59, 5, 41, 95, 32, 228, 90, 128, 147, 174, 61, 211, 146, 169, 226, 99, 194, 112, 183, 57, 165, 188, 18, 125, 39, 198, 50, 170, 72, 94, 189, 81, 40, 230, 168, 127, 7, 123, 187, 102, 176, 172, 11, 218, 171, 214, 175, 136, 77, 154, 13, 199, 110, 49, 145, 65, 191, 67, 221, 93, 233, 200, 197, 104, 119, 205, 56, 163, 43, 207, 153, 160, 208, 212, 223, 144, 161, 155, 15, 129, 70, 113, 111, 33, 42, 114, 167, 48, 225, 203, 26, 66, 45, 52, 10, 3, 185, 63, 47, 134, 196, 30, 151, 117, 60, 148, 35, 76, 177, 235, 87, 24, 4, 53, 142, 69, 29, 224, 27, 86, 192, 105, 21, 75, 216, 79, 83, 152, 97, 22, 140, 14, 237, 92, 20, 107, 141, 139, 209, 74, 71, 135, 132, 178, 162, 51, 202, 234, 8, 36, 58, 131, 91, 28, 25, 121, 215, 206, 100, 179, 85, 190, 195, 31, 126, 106, 109, 46, 143, 193, 232, 181, 38, 9, 73, 68, 17, 159, 96, 44, 236, 116, 220, 219, 124, 173, 19, 130, 62, 64, 16, 158, 78, 55, 12, 88, 229, 84, 204, 82, 137, 1, 37, 101, 115, 133, 89, 222, 149, 118, 150, 156, 138, 122, 103, 108, 180, 157, 80]
[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]
[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 19 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.123
[ 2 / 40 ] loss: 0.220
[ 3 / 40 ] loss: 0.291
[ 4 / 40 ] loss: 0.136
[ 5 / 40 ] loss: 0.178
[ 6 / 40 ] loss: 0.414
[ 7 / 40 ] loss: 0.053
[ 8 / 40 ] loss: 0.111
[ 9 / 40 ] loss: 0.372
[ 10 / 40 ] loss: 0.187
[ 11 / 40 ] loss: 0.070
[ 12 / 40 ] loss: 0.247
[ 13 / 40 ] loss: 0.130
[ 14 / 40 ] loss: 0.189
[ 15 / 40 ] loss: 0.202
[ 16 / 40 ] loss: 0.381
[ 17 / 40 ] loss: 0.251
[ 18 / 40 ] loss: 0.210
[ 19 / 40 ] loss: 0.285
[ 20 / 40 ] loss: 0.172
[ 21 / 40 ] loss: 0.345
[ 22 / 40 ] loss: 0.480
[ 23 / 40 ] loss: 0.272
[ 24 / 40 ] loss: 0.302
[ 25 / 40 ] loss: 0.277
[ 26 / 40 ] loss: 0.162
[ 27 / 40 ] loss: 0.179
[ 28 / 40 ] loss: 0.252
[ 29 / 40 ] loss: 0.080
[ 30 / 40 ] loss: 0.140
[ 31 / 40 ] loss: 0.251
[ 32 / 40 ] loss: 0.393
[ 33 / 40 ] loss: 0.081
[ 34 / 40 ] loss: 0.226
[ 35 / 40 ] loss: 0.173
[ 36 / 40 ] loss: 0.364
[ 37 / 40 ] loss: 0.229
[ 38 / 40 ] loss: 0.286
[ 39 / 40 ] loss: 0.313
[ 40 / 40 ] loss: 0.361
0.2346950639039278
Accuracy: 0.772152 -- Precision: 0.767606 -- Recall: 0.838462 -- F1: 0.801471 -- AUC: 0.811287
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[80, 89, 150, 87, 221, 147, 219, 116, 121, 214, 186, 196, 25, 187, 67, 96, 233, 195, 8, 207, 182, 139, 228, 191, 23, 33, 206, 125, 201, 94, 163, 26, 220, 107, 205, 30, 37, 211, 204, 60, 157, 114, 34, 103, 154, 166, 12, 151, 176, 61, 52, 73, 10, 237, 48, 72, 142, 36, 1, 2, 17, 86, 208, 118, 170, 217, 66, 45, 76, 38, 123, 63, 93, 199, 119, 41, 129, 126, 65, 59, 227, 149, 225, 194, 7, 85, 152, 122, 168, 39, 132, 102, 232, 51, 49, 92, 159, 173, 165, 112, 177, 218, 55, 115, 127, 222, 15, 77, 160, 172, 22, 167, 13, 4, 113, 162, 19, 175, 231, 58, 32, 184, 185, 156, 190, 155, 24, 197, 43, 144, 57, 81, 111, 98, 68, 90, 224, 128, 28, 5, 42, 198, 213, 145, 120, 56, 161, 234, 78, 181, 20, 100, 108, 11, 216, 148, 27, 229, 174, 16, 53, 203, 54, 47, 3, 83, 82, 130, 88, 105, 158, 136, 110, 99, 131, 212, 140, 188, 40, 235, 46, 146, 124, 101, 209, 106, 202, 138, 180, 226, 62, 215, 230, 223, 97, 21, 210, 236, 200, 64, 189, 29, 141, 84, 133, 79, 135, 143, 14, 44, 70, 193, 171, 71, 169, 137, 179, 74, 6, 95, 183, 50, 153, 178, 91, 69, 75, 164, 134, 117, 31, 18, 35, 109, 192, 104, 9]
[1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0]
[0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 20 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.262
[ 2 / 40 ] loss: 0.193
[ 3 / 40 ] loss: 0.291
[ 4 / 40 ] loss: 0.130
[ 5 / 40 ] loss: 0.129
[ 6 / 40 ] loss: 0.298
[ 7 / 40 ] loss: 0.211
[ 8 / 40 ] loss: 0.377
[ 9 / 40 ] loss: 0.146
[ 10 / 40 ] loss: 0.307
[ 11 / 40 ] loss: 0.300
[ 12 / 40 ] loss: 0.120
[ 13 / 40 ] loss: 0.149
[ 14 / 40 ] loss: 0.109
[ 15 / 40 ] loss: 0.092
[ 16 / 40 ] loss: 0.116
[ 17 / 40 ] loss: 0.242
[ 18 / 40 ] loss: 0.109
[ 19 / 40 ] loss: 0.039
[ 20 / 40 ] loss: 0.166
[ 21 / 40 ] loss: 0.244
[ 22 / 40 ] loss: 0.180
[ 23 / 40 ] loss: 0.066
[ 24 / 40 ] loss: 0.197
[ 25 / 40 ] loss: 0.330
[ 26 / 40 ] loss: 0.032
[ 27 / 40 ] loss: 0.258
[ 28 / 40 ] loss: 0.365
[ 29 / 40 ] loss: 0.066
[ 30 / 40 ] loss: 0.268
[ 31 / 40 ] loss: 0.226
[ 32 / 40 ] loss: 0.144
[ 33 / 40 ] loss: 0.291
[ 34 / 40 ] loss: 0.267
[ 35 / 40 ] loss: 0.290
[ 36 / 40 ] loss: 0.247
[ 37 / 40 ] loss: 0.204
[ 38 / 40 ] loss: 0.265
[ 39 / 40 ] loss: 0.245
[ 40 / 40 ] loss: 0.171
0.2035374565050006
Accuracy: 0.759494 -- Precision: 0.817391 -- Recall: 0.723077 -- F1: 0.767347 -- AUC: 0.811574
========= epoch: 21 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.218
[ 2 / 40 ] loss: 0.217
[ 3 / 40 ] loss: 0.206
[ 4 / 40 ] loss: 0.068
[ 5 / 40 ] loss: 0.227
[ 6 / 40 ] loss: 0.221
[ 7 / 40 ] loss: 0.106
[ 8 / 40 ] loss: 0.071
[ 9 / 40 ] loss: 0.213
[ 10 / 40 ] loss: 0.091
[ 11 / 40 ] loss: 0.056
[ 12 / 40 ] loss: 0.259
[ 13 / 40 ] loss: 0.230
[ 14 / 40 ] loss: 0.088
[ 15 / 40 ] loss: 0.306
[ 16 / 40 ] loss: 0.383
[ 17 / 40 ] loss: 0.319
[ 18 / 40 ] loss: 0.052
[ 19 / 40 ] loss: 0.116
[ 20 / 40 ] loss: 0.304
[ 21 / 40 ] loss: 0.112
[ 22 / 40 ] loss: 0.309
[ 23 / 40 ] loss: 0.185
[ 24 / 40 ] loss: 0.103
[ 25 / 40 ] loss: 0.242
[ 26 / 40 ] loss: 0.068
[ 27 / 40 ] loss: 0.115
[ 28 / 40 ] loss: 0.317
[ 29 / 40 ] loss: 0.191
[ 30 / 40 ] loss: 0.499
[ 31 / 40 ] loss: 0.226
[ 32 / 40 ] loss: 0.030
[ 33 / 40 ] loss: 0.068
[ 34 / 40 ] loss: 0.177
[ 35 / 40 ] loss: 0.196
[ 36 / 40 ] loss: 0.134
[ 37 / 40 ] loss: 0.038
[ 38 / 40 ] loss: 0.192
[ 39 / 40 ] loss: 0.187
[ 40 / 40 ] loss: 0.079
0.18052267730236055
Accuracy: 0.742616 -- Precision: 0.744681 -- Recall: 0.807692 -- F1: 0.774908 -- AUC: 0.808483
========= epoch: 22 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.274
[ 2 / 40 ] loss: 0.244
[ 3 / 40 ] loss: 0.636
[ 4 / 40 ] loss: 0.130
[ 5 / 40 ] loss: 0.064
[ 6 / 40 ] loss: 0.125
[ 7 / 40 ] loss: 0.127
[ 8 / 40 ] loss: 0.368
[ 9 / 40 ] loss: 0.119
[ 10 / 40 ] loss: 0.192
[ 11 / 40 ] loss: 0.217
[ 12 / 40 ] loss: 0.071
[ 13 / 40 ] loss: 0.061
[ 14 / 40 ] loss: 0.049
[ 15 / 40 ] loss: 0.078
[ 16 / 40 ] loss: 0.039
[ 17 / 40 ] loss: 0.263
[ 18 / 40 ] loss: 0.128
[ 19 / 40 ] loss: 0.438
[ 20 / 40 ] loss: 0.048
[ 21 / 40 ] loss: 0.031
[ 22 / 40 ] loss: 0.073
[ 23 / 40 ] loss: 0.436
[ 24 / 40 ] loss: 0.147
[ 25 / 40 ] loss: 0.299
[ 26 / 40 ] loss: 0.090
[ 27 / 40 ] loss: 0.183
[ 28 / 40 ] loss: 0.132
[ 29 / 40 ] loss: 0.225
[ 30 / 40 ] loss: 0.114
[ 31 / 40 ] loss: 0.478
[ 32 / 40 ] loss: 0.037
[ 33 / 40 ] loss: 0.191
[ 34 / 40 ] loss: 0.054
[ 35 / 40 ] loss: 0.038
[ 36 / 40 ] loss: 0.103
[ 37 / 40 ] loss: 0.131
[ 38 / 40 ] loss: 0.086
[ 39 / 40 ] loss: 0.050
[ 40 / 40 ] loss: 0.035
0.16509328382089733
Accuracy: 0.759494 -- Precision: 0.811966 -- Recall: 0.730769 -- F1: 0.769231 -- AUC: 0.804529
========= epoch: 23 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.070
[ 2 / 40 ] loss: 0.045
[ 3 / 40 ] loss: 0.037
[ 4 / 40 ] loss: 0.023
[ 5 / 40 ] loss: 0.023
[ 6 / 40 ] loss: 0.050
[ 7 / 40 ] loss: 0.129
[ 8 / 40 ] loss: 0.204
[ 9 / 40 ] loss: 0.202
[ 10 / 40 ] loss: 0.260
[ 11 / 40 ] loss: 0.620
[ 12 / 40 ] loss: 0.058
[ 13 / 40 ] loss: 0.425
[ 14 / 40 ] loss: 0.146
[ 15 / 40 ] loss: 0.214
[ 16 / 40 ] loss: 0.259
[ 17 / 40 ] loss: 0.356
[ 18 / 40 ] loss: 0.054
[ 19 / 40 ] loss: 0.315
[ 20 / 40 ] loss: 0.069
[ 21 / 40 ] loss: 0.225
[ 22 / 40 ] loss: 0.124
[ 23 / 40 ] loss: 0.083
[ 24 / 40 ] loss: 0.063
[ 25 / 40 ] loss: 0.312
[ 26 / 40 ] loss: 0.242
[ 27 / 40 ] loss: 0.078
[ 28 / 40 ] loss: 0.207
[ 29 / 40 ] loss: 0.250
[ 30 / 40 ] loss: 0.205
[ 31 / 40 ] loss: 0.324
[ 32 / 40 ] loss: 0.205
[ 33 / 40 ] loss: 0.041
[ 34 / 40 ] loss: 0.187
[ 35 / 40 ] loss: 0.034
[ 36 / 40 ] loss: 0.193
[ 37 / 40 ] loss: 0.045
[ 38 / 40 ] loss: 0.186
[ 39 / 40 ] loss: 0.272
[ 40 / 40 ] loss: 0.154
0.17473604711703955
Accuracy: 0.776371 -- Precision: 0.798450 -- Recall: 0.792308 -- F1: 0.795367 -- AUC: 0.806219
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[188, 215, 86, 200, 201, 18, 109, 77, 125, 59, 123, 164, 35, 67, 133, 213, 58, 15, 87, 206, 167, 110, 42, 126, 101, 179, 79, 199, 66, 69, 1, 237, 157, 29, 145, 13, 176, 95, 217, 24, 214, 100, 34, 41, 104, 94, 10, 197, 190, 92, 175, 6, 189, 152, 172, 229, 141, 165, 97, 82, 107, 49, 40, 231, 65, 218, 93, 75, 220, 196, 137, 131, 144, 204, 232, 111, 83, 57, 26, 224, 202, 212, 99, 85, 52, 225, 74, 160, 148, 155, 11, 36, 16, 161, 102, 166, 5, 127, 139, 48, 71, 171, 37, 27, 173, 168, 120, 43, 230, 154, 8, 182, 129, 64, 142, 162, 159, 130, 209, 23, 78, 47, 143, 156, 30, 226, 136, 140, 124, 181, 38, 31, 113, 216, 187, 108, 51, 132, 163, 53, 184, 90, 117, 70, 76, 178, 169, 17, 115, 39, 151, 153, 4, 73, 149, 135, 62, 19, 146, 56, 91, 50, 118, 150, 106, 180, 2, 222, 54, 138, 114, 193, 233, 55, 46, 12, 105, 112, 185, 63, 80, 170, 158, 45, 211, 147, 219, 208, 223, 177, 122, 20, 68, 44, 84, 60, 89, 98, 227, 134, 25, 174, 103, 3, 116, 191, 221, 203, 194, 33, 22, 205, 228, 28, 235, 236, 195, 88, 183, 119, 21, 234, 14, 210, 81, 198, 186, 128, 192, 72, 32, 207, 9, 61, 7, 96, 121]
[1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]
[1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 24 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.530
[ 2 / 40 ] loss: 0.245
[ 3 / 40 ] loss: 0.040
[ 4 / 40 ] loss: 0.029
[ 5 / 40 ] loss: 0.176
[ 6 / 40 ] loss: 0.034
[ 7 / 40 ] loss: 0.060
[ 8 / 40 ] loss: 0.203
[ 9 / 40 ] loss: 0.355
[ 10 / 40 ] loss: 0.085
[ 11 / 40 ] loss: 0.327
[ 12 / 40 ] loss: 0.032
[ 13 / 40 ] loss: 0.096
[ 14 / 40 ] loss: 0.354
[ 15 / 40 ] loss: 0.183
[ 16 / 40 ] loss: 0.088
[ 17 / 40 ] loss: 0.057
[ 18 / 40 ] loss: 0.348
[ 19 / 40 ] loss: 0.174
[ 20 / 40 ] loss: 0.172
[ 21 / 40 ] loss: 0.237
[ 22 / 40 ] loss: 0.117
[ 23 / 40 ] loss: 0.271
[ 24 / 40 ] loss: 0.332
[ 25 / 40 ] loss: 0.044
[ 26 / 40 ] loss: 0.112
[ 27 / 40 ] loss: 0.165
[ 28 / 40 ] loss: 0.112
[ 29 / 40 ] loss: 0.079
[ 30 / 40 ] loss: 0.141
[ 31 / 40 ] loss: 0.086
[ 32 / 40 ] loss: 0.038
[ 33 / 40 ] loss: 0.031
[ 34 / 40 ] loss: 0.315
[ 35 / 40 ] loss: 0.179
[ 36 / 40 ] loss: 0.245
[ 37 / 40 ] loss: 0.244
[ 38 / 40 ] loss: 0.340
[ 39 / 40 ] loss: 0.061
[ 40 / 40 ] loss: 0.033
0.16925041927024723
Accuracy: 0.780591 -- Precision: 0.825000 -- Recall: 0.761538 -- F1: 0.792000 -- AUC: 0.811862
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[226, 56, 120, 177, 233, 50, 185, 23, 161, 26, 201, 87, 210, 137, 129, 178, 119, 40, 202, 22, 212, 82, 208, 219, 20, 58, 179, 43, 223, 182, 27, 222, 71, 64, 25, 113, 166, 57, 9, 118, 206, 6, 114, 30, 63, 136, 163, 123, 190, 94, 41, 97, 29, 154, 110, 138, 203, 232, 209, 217, 186, 99, 231, 188, 167, 145, 53, 174, 107, 74, 69, 16, 189, 142, 1, 34, 151, 44, 54, 111, 117, 47, 124, 96, 214, 195, 204, 95, 150, 42, 76, 93, 72, 89, 79, 19, 169, 205, 164, 122, 52, 35, 183, 84, 85, 66, 86, 192, 49, 101, 3, 225, 127, 224, 81, 162, 184, 168, 13, 33, 88, 15, 139, 133, 194, 65, 106, 91, 5, 230, 24, 171, 207, 61, 131, 200, 8, 115, 228, 234, 55, 181, 148, 176, 235, 4, 128, 32, 104, 175, 236, 12, 38, 213, 73, 80, 125, 147, 153, 132, 78, 144, 130, 103, 10, 196, 159, 141, 48, 160, 102, 90, 134, 152, 229, 60, 140, 193, 36, 14, 227, 197, 143, 216, 116, 108, 172, 62, 75, 7, 126, 45, 59, 77, 100, 112, 146, 165, 198, 211, 187, 199, 98, 46, 67, 215, 149, 39, 105, 180, 156, 68, 218, 51, 17, 83, 121, 92, 2, 28, 237, 109, 21, 170, 135, 37, 191, 221, 31, 18, 155, 11, 220, 70, 173, 157, 158]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1]
[1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 25 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.221
[ 2 / 40 ] loss: 0.106
[ 3 / 40 ] loss: 0.306
[ 4 / 40 ] loss: 0.141
[ 5 / 40 ] loss: 0.112
[ 6 / 40 ] loss: 0.118
[ 7 / 40 ] loss: 0.034
[ 8 / 40 ] loss: 0.225
[ 9 / 40 ] loss: 0.031
[ 10 / 40 ] loss: 0.097
[ 11 / 40 ] loss: 0.199
[ 12 / 40 ] loss: 0.524
[ 13 / 40 ] loss: 0.055
[ 14 / 40 ] loss: 0.101
[ 15 / 40 ] loss: 0.189
[ 16 / 40 ] loss: 0.112
[ 17 / 40 ] loss: 0.049
[ 18 / 40 ] loss: 0.296
[ 19 / 40 ] loss: 0.032
[ 20 / 40 ] loss: 0.053
[ 21 / 40 ] loss: 0.117
[ 22 / 40 ] loss: 0.259
[ 23 / 40 ] loss: 0.043
[ 24 / 40 ] loss: 0.035
[ 25 / 40 ] loss: 0.070
[ 26 / 40 ] loss: 0.069
[ 27 / 40 ] loss: 0.225
[ 28 / 40 ] loss: 0.232
[ 29 / 40 ] loss: 0.037
[ 30 / 40 ] loss: 0.114
[ 31 / 40 ] loss: 0.279
[ 32 / 40 ] loss: 0.313
[ 33 / 40 ] loss: 0.111
[ 34 / 40 ] loss: 0.021
[ 35 / 40 ] loss: 0.370
[ 36 / 40 ] loss: 0.069
[ 37 / 40 ] loss: 0.093
[ 38 / 40 ] loss: 0.259
[ 39 / 40 ] loss: 0.190
[ 40 / 40 ] loss: 0.152
0.15145717510022222
Accuracy: 0.763713 -- Precision: 0.789062 -- Recall: 0.776923 -- F1: 0.782946 -- AUC: 0.812725
========= epoch: 26 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.256
[ 2 / 40 ] loss: 0.212
[ 3 / 40 ] loss: 0.065
[ 4 / 40 ] loss: 0.127
[ 5 / 40 ] loss: 0.492
[ 6 / 40 ] loss: 0.121
[ 7 / 40 ] loss: 0.173
[ 8 / 40 ] loss: 0.116
[ 9 / 40 ] loss: 0.153
[ 10 / 40 ] loss: 0.356
[ 11 / 40 ] loss: 0.132
[ 12 / 40 ] loss: 0.167
[ 13 / 40 ] loss: 0.112
[ 14 / 40 ] loss: 0.158
[ 15 / 40 ] loss: 0.158
[ 16 / 40 ] loss: 0.062
[ 17 / 40 ] loss: 0.176
[ 18 / 40 ] loss: 0.107
[ 19 / 40 ] loss: 0.131
[ 20 / 40 ] loss: 0.171
[ 21 / 40 ] loss: 0.200
[ 22 / 40 ] loss: 0.505
[ 23 / 40 ] loss: 0.259
[ 24 / 40 ] loss: 0.152
[ 25 / 40 ] loss: 0.207
[ 26 / 40 ] loss: 0.073
[ 27 / 40 ] loss: 0.270
[ 28 / 40 ] loss: 0.098
[ 29 / 40 ] loss: 0.067
[ 30 / 40 ] loss: 0.394
[ 31 / 40 ] loss: 0.094
[ 32 / 40 ] loss: 0.183
[ 33 / 40 ] loss: 0.345
[ 34 / 40 ] loss: 0.098
[ 35 / 40 ] loss: 0.211
[ 36 / 40 ] loss: 0.067
[ 37 / 40 ] loss: 0.462
[ 38 / 40 ] loss: 0.064
[ 39 / 40 ] loss: 0.282
[ 40 / 40 ] loss: 0.543
0.20055659441277385
Accuracy: 0.746835 -- Precision: 0.796610 -- Recall: 0.723077 -- F1: 0.758065 -- AUC: 0.804026
========= epoch: 27 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.122
[ 2 / 40 ] loss: 0.220
[ 3 / 40 ] loss: 0.310
[ 4 / 40 ] loss: 0.225
[ 5 / 40 ] loss: 0.222
[ 6 / 40 ] loss: 0.241
[ 7 / 40 ] loss: 0.220
[ 8 / 40 ] loss: 0.033
[ 9 / 40 ] loss: 0.045
[ 10 / 40 ] loss: 0.207
[ 11 / 40 ] loss: 0.454
[ 12 / 40 ] loss: 0.045
[ 13 / 40 ] loss: 0.234
[ 14 / 40 ] loss: 0.400
[ 15 / 40 ] loss: 0.336
[ 16 / 40 ] loss: 0.383
[ 17 / 40 ] loss: 0.098
[ 18 / 40 ] loss: 0.122
[ 19 / 40 ] loss: 0.160
[ 20 / 40 ] loss: 0.119
[ 21 / 40 ] loss: 0.117
[ 22 / 40 ] loss: 0.281
[ 23 / 40 ] loss: 0.183
[ 24 / 40 ] loss: 0.338
[ 25 / 40 ] loss: 0.253
[ 26 / 40 ] loss: 0.048
[ 27 / 40 ] loss: 0.171
[ 28 / 40 ] loss: 0.085
[ 29 / 40 ] loss: 0.196
[ 30 / 40 ] loss: 0.168
[ 31 / 40 ] loss: 0.149
[ 32 / 40 ] loss: 0.200
[ 33 / 40 ] loss: 0.508
[ 34 / 40 ] loss: 0.098
[ 35 / 40 ] loss: 0.141
[ 36 / 40 ] loss: 0.100
[ 37 / 40 ] loss: 0.086
[ 38 / 40 ] loss: 0.040
[ 39 / 40 ] loss: 0.089
[ 40 / 40 ] loss: 0.059
0.18766269758343695
Accuracy: 0.776371 -- Precision: 0.813008 -- Recall: 0.769231 -- F1: 0.790514 -- AUC: 0.826743
========= epoch: 28 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.073
[ 2 / 40 ] loss: 0.084
[ 3 / 40 ] loss: 0.032
[ 4 / 40 ] loss: 0.142
[ 5 / 40 ] loss: 0.068
[ 6 / 40 ] loss: 0.198
[ 7 / 40 ] loss: 0.048
[ 8 / 40 ] loss: 0.018
[ 9 / 40 ] loss: 0.083
[ 10 / 40 ] loss: 0.071
[ 11 / 40 ] loss: 0.049
[ 12 / 40 ] loss: 0.081
[ 13 / 40 ] loss: 0.438
[ 14 / 40 ] loss: 0.102
[ 15 / 40 ] loss: 0.085
[ 16 / 40 ] loss: 0.293
[ 17 / 40 ] loss: 0.135
[ 18 / 40 ] loss: 0.211
[ 19 / 40 ] loss: 0.180
[ 20 / 40 ] loss: 0.111
[ 21 / 40 ] loss: 0.304
[ 22 / 40 ] loss: 0.198
[ 23 / 40 ] loss: 0.454
[ 24 / 40 ] loss: 0.159
[ 25 / 40 ] loss: 0.159
[ 26 / 40 ] loss: 0.078
[ 27 / 40 ] loss: 0.095
[ 28 / 40 ] loss: 0.153
[ 29 / 40 ] loss: 0.266
[ 30 / 40 ] loss: 0.129
[ 31 / 40 ] loss: 0.092
[ 32 / 40 ] loss: 0.090
[ 33 / 40 ] loss: 0.215
[ 34 / 40 ] loss: 0.055
[ 35 / 40 ] loss: 0.108
[ 36 / 40 ] loss: 0.362
[ 37 / 40 ] loss: 0.154
[ 38 / 40 ] loss: 0.138
[ 39 / 40 ] loss: 0.084
[ 40 / 40 ] loss: 0.171
0.14908962016925215
Accuracy: 0.767932 -- Precision: 0.800000 -- Recall: 0.769231 -- F1: 0.784314 -- AUC: 0.811790
========= epoch: 29 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.127
[ 2 / 40 ] loss: 0.184
[ 3 / 40 ] loss: 0.124
[ 4 / 40 ] loss: 0.069
[ 5 / 40 ] loss: 0.235
[ 6 / 40 ] loss: 0.078
[ 7 / 40 ] loss: 0.037
[ 8 / 40 ] loss: 0.089
[ 9 / 40 ] loss: 0.035
[ 10 / 40 ] loss: 0.200
[ 11 / 40 ] loss: 0.360
[ 12 / 40 ] loss: 0.222
[ 13 / 40 ] loss: 0.039
[ 14 / 40 ] loss: 0.116
[ 15 / 40 ] loss: 0.087
[ 16 / 40 ] loss: 0.195
[ 17 / 40 ] loss: 0.028
[ 18 / 40 ] loss: 0.227
[ 19 / 40 ] loss: 0.025
[ 20 / 40 ] loss: 0.036
[ 21 / 40 ] loss: 0.206
[ 22 / 40 ] loss: 0.039
[ 23 / 40 ] loss: 0.070
[ 24 / 40 ] loss: 0.038
[ 25 / 40 ] loss: 0.137
[ 26 / 40 ] loss: 0.041
[ 27 / 40 ] loss: 0.089
[ 28 / 40 ] loss: 0.095
[ 29 / 40 ] loss: 0.057
[ 30 / 40 ] loss: 0.106
[ 31 / 40 ] loss: 0.095
[ 32 / 40 ] loss: 0.120
[ 33 / 40 ] loss: 0.207
[ 34 / 40 ] loss: 0.281
[ 35 / 40 ] loss: 0.122
[ 36 / 40 ] loss: 0.436
[ 37 / 40 ] loss: 0.025
[ 38 / 40 ] loss: 0.165
[ 39 / 40 ] loss: 0.103
[ 40 / 40 ] loss: 0.057
0.1250050085131079
Accuracy: 0.776371 -- Precision: 0.818182 -- Recall: 0.761538 -- F1: 0.788845 -- AUC: 0.809633
========= epoch: 30 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.241
[ 2 / 40 ] loss: 0.217
[ 3 / 40 ] loss: 0.037
[ 4 / 40 ] loss: 0.141
[ 5 / 40 ] loss: 0.043
[ 6 / 40 ] loss: 0.146
[ 7 / 40 ] loss: 0.148
[ 8 / 40 ] loss: 0.038
[ 9 / 40 ] loss: 0.048
[ 10 / 40 ] loss: 0.120
[ 11 / 40 ] loss: 0.056
[ 12 / 40 ] loss: 0.261
[ 13 / 40 ] loss: 0.102
[ 14 / 40 ] loss: 0.058
[ 15 / 40 ] loss: 0.313
[ 16 / 40 ] loss: 0.148
[ 17 / 40 ] loss: 0.250
[ 18 / 40 ] loss: 0.265
[ 19 / 40 ] loss: 0.144
[ 20 / 40 ] loss: 0.310
[ 21 / 40 ] loss: 0.131
[ 22 / 40 ] loss: 0.023
[ 23 / 40 ] loss: 0.041
[ 24 / 40 ] loss: 0.031
[ 25 / 40 ] loss: 0.247
[ 26 / 40 ] loss: 0.031
[ 27 / 40 ] loss: 0.023
[ 28 / 40 ] loss: 0.070
[ 29 / 40 ] loss: 0.300
[ 30 / 40 ] loss: 0.210
[ 31 / 40 ] loss: 0.040
[ 32 / 40 ] loss: 0.140
[ 33 / 40 ] loss: 0.031
[ 34 / 40 ] loss: 0.042
[ 35 / 40 ] loss: 0.035
[ 36 / 40 ] loss: 0.155
[ 37 / 40 ] loss: 0.149
[ 38 / 40 ] loss: 0.103
[ 39 / 40 ] loss: 0.081
[ 40 / 40 ] loss: 0.013
0.12461165487766265
Accuracy: 0.738397 -- Precision: 0.814815 -- Recall: 0.676923 -- F1: 0.739496 -- AUC: 0.813228
========= epoch: 31 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.257
[ 2 / 40 ] loss: 0.179
[ 3 / 40 ] loss: 0.070
[ 4 / 40 ] loss: 0.218
[ 5 / 40 ] loss: 0.164
[ 6 / 40 ] loss: 0.038
[ 7 / 40 ] loss: 0.194
[ 8 / 40 ] loss: 0.073
[ 9 / 40 ] loss: 0.053
[ 10 / 40 ] loss: 0.182
[ 11 / 40 ] loss: 0.100
[ 12 / 40 ] loss: 0.057
[ 13 / 40 ] loss: 0.217
[ 14 / 40 ] loss: 0.178
[ 15 / 40 ] loss: 0.208
[ 16 / 40 ] loss: 0.183
[ 17 / 40 ] loss: 0.031
[ 18 / 40 ] loss: 0.165
[ 19 / 40 ] loss: 0.159
[ 20 / 40 ] loss: 0.025
[ 21 / 40 ] loss: 0.465
[ 22 / 40 ] loss: 0.103
[ 23 / 40 ] loss: 0.275
[ 24 / 40 ] loss: 0.051
[ 25 / 40 ] loss: 0.077
[ 26 / 40 ] loss: 0.074
[ 27 / 40 ] loss: 0.035
[ 28 / 40 ] loss: 0.475
[ 29 / 40 ] loss: 0.080
[ 30 / 40 ] loss: 0.032
[ 31 / 40 ] loss: 0.083
[ 32 / 40 ] loss: 0.221
[ 33 / 40 ] loss: 0.202
[ 34 / 40 ] loss: 0.069
[ 35 / 40 ] loss: 0.045
[ 36 / 40 ] loss: 0.043
[ 37 / 40 ] loss: 0.334
[ 38 / 40 ] loss: 0.123
[ 39 / 40 ] loss: 0.073
[ 40 / 40 ] loss: 0.185
0.14488964611664415
Accuracy: 0.751055 -- Precision: 0.748252 -- Recall: 0.823077 -- F1: 0.783883 -- AUC: 0.791733
========= epoch: 32 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.175
[ 2 / 40 ] loss: 0.079
[ 3 / 40 ] loss: 0.178
[ 4 / 40 ] loss: 0.090
[ 5 / 40 ] loss: 0.070
[ 6 / 40 ] loss: 0.174
[ 7 / 40 ] loss: 0.282
[ 8 / 40 ] loss: 0.519
[ 9 / 40 ] loss: 0.231
[ 10 / 40 ] loss: 0.029
[ 11 / 40 ] loss: 0.032
[ 12 / 40 ] loss: 0.174
[ 13 / 40 ] loss: 0.275
[ 14 / 40 ] loss: 0.787
[ 15 / 40 ] loss: 0.354
[ 16 / 40 ] loss: 0.240
[ 17 / 40 ] loss: 0.248
[ 18 / 40 ] loss: 0.182
[ 19 / 40 ] loss: 0.216
[ 20 / 40 ] loss: 0.144
[ 21 / 40 ] loss: 0.063
[ 22 / 40 ] loss: 0.044
[ 23 / 40 ] loss: 0.255
[ 24 / 40 ] loss: 0.019
[ 25 / 40 ] loss: 0.134
[ 26 / 40 ] loss: 0.045
[ 27 / 40 ] loss: 0.037
[ 28 / 40 ] loss: 0.281
[ 29 / 40 ] loss: 0.183
[ 30 / 40 ] loss: 0.096
[ 31 / 40 ] loss: 0.037
[ 32 / 40 ] loss: 0.026
[ 33 / 40 ] loss: 0.035
[ 34 / 40 ] loss: 0.041
[ 35 / 40 ] loss: 0.380
[ 36 / 40 ] loss: 0.199
[ 37 / 40 ] loss: 0.223
[ 38 / 40 ] loss: 0.108
[ 39 / 40 ] loss: 0.200
[ 40 / 40 ] loss: 0.416
0.18252450246363877
Accuracy: 0.755274 -- Precision: 0.785714 -- Recall: 0.761538 -- F1: 0.773438 -- AUC: 0.796693
========= epoch: 33 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.188
[ 2 / 40 ] loss: 0.245
[ 3 / 40 ] loss: 0.223
[ 4 / 40 ] loss: 0.132
[ 5 / 40 ] loss: 0.035
[ 6 / 40 ] loss: 0.238
[ 7 / 40 ] loss: 0.035
[ 8 / 40 ] loss: 0.069
[ 9 / 40 ] loss: 0.038
[ 10 / 40 ] loss: 0.048
[ 11 / 40 ] loss: 0.085
[ 12 / 40 ] loss: 0.189
[ 13 / 40 ] loss: 0.162
[ 14 / 40 ] loss: 0.219
[ 15 / 40 ] loss: 0.085
[ 16 / 40 ] loss: 0.111
[ 17 / 40 ] loss: 0.559
[ 18 / 40 ] loss: 0.198
[ 19 / 40 ] loss: 0.051
[ 20 / 40 ] loss: 0.092
[ 21 / 40 ] loss: 0.047
[ 22 / 40 ] loss: 0.068
[ 23 / 40 ] loss: 0.307
[ 24 / 40 ] loss: 0.160
[ 25 / 40 ] loss: 0.199
[ 26 / 40 ] loss: 0.041
[ 27 / 40 ] loss: 0.171
[ 28 / 40 ] loss: 0.116
[ 29 / 40 ] loss: 0.080
[ 30 / 40 ] loss: 0.501
[ 31 / 40 ] loss: 0.234
[ 32 / 40 ] loss: 0.218
[ 33 / 40 ] loss: 0.064
[ 34 / 40 ] loss: 0.060
[ 35 / 40 ] loss: 0.087
[ 36 / 40 ] loss: 0.318
[ 37 / 40 ] loss: 0.184
[ 38 / 40 ] loss: 0.095
[ 39 / 40 ] loss: 0.093
[ 40 / 40 ] loss: 0.887
0.17327259257435798
Accuracy: 0.746835 -- Precision: 0.761194 -- Recall: 0.784615 -- F1: 0.772727 -- AUC: 0.788785
========= epoch: 34 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.041
[ 2 / 40 ] loss: 0.151
[ 3 / 40 ] loss: 0.119
[ 4 / 40 ] loss: 0.105
[ 5 / 40 ] loss: 0.183
[ 6 / 40 ] loss: 0.097
[ 7 / 40 ] loss: 0.123
[ 8 / 40 ] loss: 0.201
[ 9 / 40 ] loss: 0.252
[ 10 / 40 ] loss: 0.089
[ 11 / 40 ] loss: 0.072
[ 12 / 40 ] loss: 0.168
[ 13 / 40 ] loss: 0.163
[ 14 / 40 ] loss: 0.161
[ 15 / 40 ] loss: 0.039
[ 16 / 40 ] loss: 0.044
[ 17 / 40 ] loss: 0.053
[ 18 / 40 ] loss: 0.158
[ 19 / 40 ] loss: 0.272
[ 20 / 40 ] loss: 0.364
[ 21 / 40 ] loss: 0.210
[ 22 / 40 ] loss: 0.206
[ 23 / 40 ] loss: 0.109
[ 24 / 40 ] loss: 0.174
[ 25 / 40 ] loss: 0.228
[ 26 / 40 ] loss: 0.073
[ 27 / 40 ] loss: 0.189
[ 28 / 40 ] loss: 0.148
[ 29 / 40 ] loss: 0.358
[ 30 / 40 ] loss: 0.389
[ 31 / 40 ] loss: 0.180
[ 32 / 40 ] loss: 0.096
[ 33 / 40 ] loss: 0.040
[ 34 / 40 ] loss: 0.302
[ 35 / 40 ] loss: 0.087
[ 36 / 40 ] loss: 0.090
[ 37 / 40 ] loss: 0.078
[ 38 / 40 ] loss: 0.164
[ 39 / 40 ] loss: 0.138
[ 40 / 40 ] loss: 0.022
0.153393493918702
Accuracy: 0.751055 -- Precision: 0.759124 -- Recall: 0.800000 -- F1: 0.779026 -- AUC: 0.811718
========= epoch: 35 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.299
[ 2 / 40 ] loss: 0.154
[ 3 / 40 ] loss: 0.208
[ 4 / 40 ] loss: 0.018
[ 5 / 40 ] loss: 0.135
[ 6 / 40 ] loss: 0.025
[ 7 / 40 ] loss: 0.030
[ 8 / 40 ] loss: 0.148
[ 9 / 40 ] loss: 0.041
[ 10 / 40 ] loss: 0.194
[ 11 / 40 ] loss: 0.049
[ 12 / 40 ] loss: 0.128
[ 13 / 40 ] loss: 0.081
[ 14 / 40 ] loss: 0.026
[ 15 / 40 ] loss: 0.087
[ 16 / 40 ] loss: 0.032
[ 17 / 40 ] loss: 0.317
[ 18 / 40 ] loss: 0.150
[ 19 / 40 ] loss: 0.312
[ 20 / 40 ] loss: 0.135
[ 21 / 40 ] loss: 0.239
[ 22 / 40 ] loss: 0.267
[ 23 / 40 ] loss: 0.178
[ 24 / 40 ] loss: 0.086
[ 25 / 40 ] loss: 0.057
[ 26 / 40 ] loss: 0.210
[ 27 / 40 ] loss: 0.250
[ 28 / 40 ] loss: 0.259
[ 29 / 40 ] loss: 0.104
[ 30 / 40 ] loss: 0.312
[ 31 / 40 ] loss: 0.135
[ 32 / 40 ] loss: 0.044
[ 33 / 40 ] loss: 0.036
[ 34 / 40 ] loss: 0.079
[ 35 / 40 ] loss: 0.070
[ 36 / 40 ] loss: 0.060
[ 37 / 40 ] loss: 0.182
[ 38 / 40 ] loss: 0.127
[ 39 / 40 ] loss: 0.138
[ 40 / 40 ] loss: 0.099
0.13755340143106878
Accuracy: 0.759494 -- Precision: 0.782946 -- Recall: 0.776923 -- F1: 0.779923 -- AUC: 0.791876
========= epoch: 36 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.040
[ 2 / 40 ] loss: 0.187
[ 3 / 40 ] loss: 0.075
[ 4 / 40 ] loss: 0.186
[ 5 / 40 ] loss: 0.128
[ 6 / 40 ] loss: 0.538
[ 7 / 40 ] loss: 0.056
[ 8 / 40 ] loss: 0.093
[ 9 / 40 ] loss: 0.036
[ 10 / 40 ] loss: 0.028
[ 11 / 40 ] loss: 0.087
[ 12 / 40 ] loss: 0.231
[ 13 / 40 ] loss: 0.065
[ 14 / 40 ] loss: 0.157
[ 15 / 40 ] loss: 0.145
[ 16 / 40 ] loss: 0.154
[ 17 / 40 ] loss: 0.105
[ 18 / 40 ] loss: 0.029
[ 19 / 40 ] loss: 0.115
[ 20 / 40 ] loss: 0.037
[ 21 / 40 ] loss: 0.202
[ 22 / 40 ] loss: 0.065
[ 23 / 40 ] loss: 0.055
[ 24 / 40 ] loss: 0.356
[ 25 / 40 ] loss: 0.081
[ 26 / 40 ] loss: 0.170
[ 27 / 40 ] loss: 0.056
[ 28 / 40 ] loss: 0.045
[ 29 / 40 ] loss: 0.115
[ 30 / 40 ] loss: 0.138
[ 31 / 40 ] loss: 0.068
[ 32 / 40 ] loss: 0.124
[ 33 / 40 ] loss: 0.067
[ 34 / 40 ] loss: 0.038
[ 35 / 40 ] loss: 0.074
[ 36 / 40 ] loss: 0.121
[ 37 / 40 ] loss: 0.052
[ 38 / 40 ] loss: 0.083
[ 39 / 40 ] loss: 0.191
[ 40 / 40 ] loss: 0.433
0.12560875075869263
Accuracy: 0.780591 -- Precision: 0.814516 -- Recall: 0.776923 -- F1: 0.795276 -- AUC: 0.805679
========= epoch: 37 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.025
[ 2 / 40 ] loss: 0.037
[ 3 / 40 ] loss: 0.176
[ 4 / 40 ] loss: 0.045
[ 5 / 40 ] loss: 0.033
[ 6 / 40 ] loss: 0.176
[ 7 / 40 ] loss: 0.105
[ 8 / 40 ] loss: 0.050
[ 9 / 40 ] loss: 0.167
[ 10 / 40 ] loss: 0.223
[ 11 / 40 ] loss: 0.106
[ 12 / 40 ] loss: 0.041
[ 13 / 40 ] loss: 0.018
[ 14 / 40 ] loss: 0.091
[ 15 / 40 ] loss: 0.022
[ 16 / 40 ] loss: 0.055
[ 17 / 40 ] loss: 0.182
[ 18 / 40 ] loss: 0.119
[ 19 / 40 ] loss: 0.112
[ 20 / 40 ] loss: 0.034
[ 21 / 40 ] loss: 0.057
[ 22 / 40 ] loss: 0.031
[ 23 / 40 ] loss: 0.080
[ 24 / 40 ] loss: 0.136
[ 25 / 40 ] loss: 0.084
[ 26 / 40 ] loss: 0.051
[ 27 / 40 ] loss: 0.179
[ 28 / 40 ] loss: 0.079
[ 29 / 40 ] loss: 0.025
[ 30 / 40 ] loss: 0.192
[ 31 / 40 ] loss: 0.015
[ 32 / 40 ] loss: 0.015
[ 33 / 40 ] loss: 0.156
[ 34 / 40 ] loss: 0.130
[ 35 / 40 ] loss: 0.306
[ 36 / 40 ] loss: 0.025
[ 37 / 40 ] loss: 0.053
[ 38 / 40 ] loss: 0.069
[ 39 / 40 ] loss: 0.034
[ 40 / 40 ] loss: 0.050
0.08966236314736306
Accuracy: 0.772152 -- Precision: 0.816667 -- Recall: 0.753846 -- F1: 0.784000 -- AUC: 0.805104
========= epoch: 38 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.044
[ 2 / 40 ] loss: 0.018
[ 3 / 40 ] loss: 0.019
[ 4 / 40 ] loss: 0.098
[ 5 / 40 ] loss: 0.143
[ 6 / 40 ] loss: 0.135
[ 7 / 40 ] loss: 0.081
[ 8 / 40 ] loss: 0.012
[ 9 / 40 ] loss: 0.079
[ 10 / 40 ] loss: 0.238
[ 11 / 40 ] loss: 0.220
[ 12 / 40 ] loss: 0.084
[ 13 / 40 ] loss: 0.088
[ 14 / 40 ] loss: 0.069
[ 15 / 40 ] loss: 0.021
[ 16 / 40 ] loss: 0.033
[ 17 / 40 ] loss: 0.010
[ 18 / 40 ] loss: 0.136
[ 19 / 40 ] loss: 0.168
[ 20 / 40 ] loss: 0.036
[ 21 / 40 ] loss: 0.068
[ 22 / 40 ] loss: 0.034
[ 23 / 40 ] loss: 0.079
[ 24 / 40 ] loss: 0.093
[ 25 / 40 ] loss: 0.179
[ 26 / 40 ] loss: 0.117
[ 27 / 40 ] loss: 0.021
[ 28 / 40 ] loss: 0.050
[ 29 / 40 ] loss: 0.227
[ 30 / 40 ] loss: 0.182
[ 31 / 40 ] loss: 0.066
[ 32 / 40 ] loss: 0.036
[ 33 / 40 ] loss: 0.025
[ 34 / 40 ] loss: 0.187
[ 35 / 40 ] loss: 0.013
[ 36 / 40 ] loss: 0.021
[ 37 / 40 ] loss: 0.235
[ 38 / 40 ] loss: 0.063
[ 39 / 40 ] loss: 0.017
[ 40 / 40 ] loss: 0.330
0.09436482293531298
Accuracy: 0.763713 -- Precision: 0.789062 -- Recall: 0.776923 -- F1: 0.782946 -- AUC: 0.805320
========= epoch: 39 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.071
[ 2 / 40 ] loss: 0.056
[ 3 / 40 ] loss: 0.131
[ 4 / 40 ] loss: 0.029
[ 5 / 40 ] loss: 0.054
[ 6 / 40 ] loss: 0.032
[ 7 / 40 ] loss: 0.163
[ 8 / 40 ] loss: 0.070
[ 9 / 40 ] loss: 0.129
[ 10 / 40 ] loss: 0.043
[ 11 / 40 ] loss: 0.013
[ 12 / 40 ] loss: 0.014
[ 13 / 40 ] loss: 0.149
[ 14 / 40 ] loss: 0.024
[ 15 / 40 ] loss: 0.090
[ 16 / 40 ] loss: 0.055
[ 17 / 40 ] loss: 0.122
[ 18 / 40 ] loss: 0.056
[ 19 / 40 ] loss: 0.067
[ 20 / 40 ] loss: 0.046
[ 21 / 40 ] loss: 0.037
[ 22 / 40 ] loss: 0.047
[ 23 / 40 ] loss: 0.103
[ 24 / 40 ] loss: 0.009
[ 25 / 40 ] loss: 0.033
[ 26 / 40 ] loss: 0.041
[ 27 / 40 ] loss: 0.175
[ 28 / 40 ] loss: 0.123
[ 29 / 40 ] loss: 0.137
[ 30 / 40 ] loss: 0.176
[ 31 / 40 ] loss: 0.013
[ 32 / 40 ] loss: 0.025
[ 33 / 40 ] loss: 0.082
[ 34 / 40 ] loss: 0.175
[ 35 / 40 ] loss: 0.055
[ 36 / 40 ] loss: 0.033
[ 37 / 40 ] loss: 0.017
[ 38 / 40 ] loss: 0.124
[ 39 / 40 ] loss: 0.044
[ 40 / 40 ] loss: 0.043
0.07266247516963631
Accuracy: 0.776371 -- Precision: 0.793893 -- Recall: 0.800000 -- F1: 0.796935 -- AUC: 0.815672
========= epoch: 40 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.041
[ 2 / 40 ] loss: 0.017
[ 3 / 40 ] loss: 0.038
[ 4 / 40 ] loss: 0.059
[ 5 / 40 ] loss: 0.241
[ 6 / 40 ] loss: 0.125
[ 7 / 40 ] loss: 0.030
[ 8 / 40 ] loss: 0.085
[ 9 / 40 ] loss: 0.005
[ 10 / 40 ] loss: 0.149
[ 11 / 40 ] loss: 0.022
[ 12 / 40 ] loss: 0.016
[ 13 / 40 ] loss: 0.092
[ 14 / 40 ] loss: 0.070
[ 15 / 40 ] loss: 0.017
[ 16 / 40 ] loss: 0.018
[ 17 / 40 ] loss: 0.057
[ 18 / 40 ] loss: 0.150
[ 19 / 40 ] loss: 0.008
[ 20 / 40 ] loss: 0.016
[ 21 / 40 ] loss: 0.076
[ 22 / 40 ] loss: 0.098
[ 23 / 40 ] loss: 0.067
[ 24 / 40 ] loss: 0.023
[ 25 / 40 ] loss: 0.032
[ 26 / 40 ] loss: 0.012
[ 27 / 40 ] loss: 0.017
[ 28 / 40 ] loss: 0.030
[ 29 / 40 ] loss: 0.220
[ 30 / 40 ] loss: 0.005
[ 31 / 40 ] loss: 0.013
[ 32 / 40 ] loss: 0.025
[ 33 / 40 ] loss: 0.383
[ 34 / 40 ] loss: 0.046
[ 35 / 40 ] loss: 0.086
[ 36 / 40 ] loss: 0.205
[ 37 / 40 ] loss: 0.062
[ 38 / 40 ] loss: 0.089
[ 39 / 40 ] loss: 0.101
[ 40 / 40 ] loss: 0.264
0.07771632509538903
Accuracy: 0.780591 -- Precision: 0.819672 -- Recall: 0.769231 -- F1: 0.793651 -- AUC: 0.816463
========= epoch: 41 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.028
[ 2 / 40 ] loss: 0.028
[ 3 / 40 ] loss: 0.027
[ 4 / 40 ] loss: 0.009
[ 5 / 40 ] loss: 0.035
[ 6 / 40 ] loss: 0.017
[ 7 / 40 ] loss: 0.113
[ 8 / 40 ] loss: 0.077
[ 9 / 40 ] loss: 0.123
[ 10 / 40 ] loss: 0.117
[ 11 / 40 ] loss: 0.067
[ 12 / 40 ] loss: 0.050
[ 13 / 40 ] loss: 0.105
[ 14 / 40 ] loss: 0.379
[ 15 / 40 ] loss: 0.166
[ 16 / 40 ] loss: 0.167
[ 17 / 40 ] loss: 0.080
[ 18 / 40 ] loss: 0.126
[ 19 / 40 ] loss: 0.261
[ 20 / 40 ] loss: 0.263
[ 21 / 40 ] loss: 0.075
[ 22 / 40 ] loss: 0.283
[ 23 / 40 ] loss: 0.026
[ 24 / 40 ] loss: 0.084
[ 25 / 40 ] loss: 0.029
[ 26 / 40 ] loss: 0.045
[ 27 / 40 ] loss: 0.220
[ 28 / 40 ] loss: 0.052
[ 29 / 40 ] loss: 0.021
[ 30 / 40 ] loss: 0.021
[ 31 / 40 ] loss: 0.132
[ 32 / 40 ] loss: 0.015
[ 33 / 40 ] loss: 0.013
[ 34 / 40 ] loss: 0.187
[ 35 / 40 ] loss: 0.102
[ 36 / 40 ] loss: 0.006
[ 37 / 40 ] loss: 0.039
[ 38 / 40 ] loss: 0.023
[ 39 / 40 ] loss: 0.015
[ 40 / 40 ] loss: 0.298
0.09814331519883127
Accuracy: 0.776371 -- Precision: 0.785185 -- Recall: 0.815385 -- F1: 0.800000 -- AUC: 0.804385
========= epoch: 42 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.016
[ 2 / 40 ] loss: 0.006
[ 3 / 40 ] loss: 0.195
[ 4 / 40 ] loss: 0.011
[ 5 / 40 ] loss: 0.010
[ 6 / 40 ] loss: 0.264
[ 7 / 40 ] loss: 0.034
[ 8 / 40 ] loss: 0.178
[ 9 / 40 ] loss: 0.034
[ 10 / 40 ] loss: 0.177
[ 11 / 40 ] loss: 0.123
[ 12 / 40 ] loss: 0.129
[ 13 / 40 ] loss: 0.311
[ 14 / 40 ] loss: 0.020
[ 15 / 40 ] loss: 0.014
[ 16 / 40 ] loss: 0.018
[ 17 / 40 ] loss: 0.426
[ 18 / 40 ] loss: 0.157
[ 19 / 40 ] loss: 0.150
[ 20 / 40 ] loss: 0.018
[ 21 / 40 ] loss: 0.122
[ 22 / 40 ] loss: 0.036
[ 23 / 40 ] loss: 0.022
[ 24 / 40 ] loss: 0.060
[ 25 / 40 ] loss: 0.145
[ 26 / 40 ] loss: 0.056
[ 27 / 40 ] loss: 0.051
[ 28 / 40 ] loss: 0.097
[ 29 / 40 ] loss: 0.103
[ 30 / 40 ] loss: 0.235
[ 31 / 40 ] loss: 0.011
[ 32 / 40 ] loss: 0.033
[ 33 / 40 ] loss: 0.249
[ 34 / 40 ] loss: 0.023
[ 35 / 40 ] loss: 0.047
[ 36 / 40 ] loss: 0.047
[ 37 / 40 ] loss: 0.116
[ 38 / 40 ] loss: 0.058
[ 39 / 40 ] loss: 0.071
[ 40 / 40 ] loss: 0.415
0.10720165187958627
Accuracy: 0.755274 -- Precision: 0.805085 -- Recall: 0.730769 -- F1: 0.766129 -- AUC: 0.784400
========= epoch: 43 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.021
[ 2 / 40 ] loss: 0.151
[ 3 / 40 ] loss: 0.037
[ 4 / 40 ] loss: 0.184
[ 5 / 40 ] loss: 0.022
[ 6 / 40 ] loss: 0.149
[ 7 / 40 ] loss: 0.139
[ 8 / 40 ] loss: 0.127
[ 9 / 40 ] loss: 0.016
[ 10 / 40 ] loss: 0.024
[ 11 / 40 ] loss: 0.043
[ 12 / 40 ] loss: 0.075
[ 13 / 40 ] loss: 0.142
[ 14 / 40 ] loss: 0.051
[ 15 / 40 ] loss: 0.123
[ 16 / 40 ] loss: 0.014
[ 17 / 40 ] loss: 0.036
[ 18 / 40 ] loss: 0.191
[ 19 / 40 ] loss: 0.048
[ 20 / 40 ] loss: 0.038
[ 21 / 40 ] loss: 0.152
[ 22 / 40 ] loss: 0.095
[ 23 / 40 ] loss: 0.020
[ 24 / 40 ] loss: 0.131
[ 25 / 40 ] loss: 0.057
[ 26 / 40 ] loss: 0.029
[ 27 / 40 ] loss: 0.067
[ 28 / 40 ] loss: 0.078
[ 29 / 40 ] loss: 0.072
[ 30 / 40 ] loss: 0.142
[ 31 / 40 ] loss: 0.048
[ 32 / 40 ] loss: 0.016
[ 33 / 40 ] loss: 0.126
[ 34 / 40 ] loss: 0.011
[ 35 / 40 ] loss: 0.048
[ 36 / 40 ] loss: 0.293
[ 37 / 40 ] loss: 0.131
[ 38 / 40 ] loss: 0.012
[ 39 / 40 ] loss: 0.070
[ 40 / 40 ] loss: 0.444
0.09185971571132541
Accuracy: 0.784810 -- Precision: 0.801527 -- Recall: 0.807692 -- F1: 0.804598 -- AUC: 0.792883
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[156, 127, 91, 61, 15, 124, 44, 130, 216, 235, 170, 66, 53, 67, 12, 14, 154, 196, 126, 176, 168, 76, 54, 88, 129, 201, 166, 94, 161, 200, 158, 78, 199, 211, 81, 29, 73, 184, 237, 26, 179, 45, 32, 134, 116, 75, 164, 20, 56, 24, 84, 8, 132, 183, 59, 57, 58, 202, 1, 192, 46, 42, 121, 34, 206, 217, 48, 155, 207, 13, 223, 18, 185, 140, 23, 107, 37, 138, 52, 172, 30, 106, 22, 227, 149, 47, 82, 125, 112, 114, 95, 188, 175, 115, 69, 16, 70, 25, 225, 177, 79, 195, 74, 109, 146, 186, 232, 43, 137, 80, 77, 3, 27, 105, 83, 60, 190, 220, 101, 10, 113, 210, 40, 122, 193, 39, 141, 108, 98, 221, 49, 197, 171, 150, 87, 119, 50, 62, 139, 99, 167, 93, 21, 209, 55, 173, 65, 38, 194, 204, 2, 189, 41, 102, 152, 36, 213, 131, 203, 228, 162, 198, 9, 143, 218, 11, 157, 219, 144, 230, 234, 224, 111, 96, 5, 120, 85, 51, 71, 136, 180, 117, 169, 229, 7, 208, 222, 19, 214, 226, 178, 147, 103, 133, 165, 236, 31, 187, 4, 104, 72, 148, 68, 215, 63, 35, 151, 191, 182, 110, 174, 145, 64, 89, 123, 142, 17, 90, 100, 233, 6, 159, 135, 231, 160, 128, 28, 212, 97, 118, 163, 153, 92, 181, 205, 33, 86]
[0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0]
[1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 44 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.012
[ 2 / 40 ] loss: 0.143
[ 3 / 40 ] loss: 0.149
[ 4 / 40 ] loss: 0.030
[ 5 / 40 ] loss: 0.114
[ 6 / 40 ] loss: 0.071
[ 7 / 40 ] loss: 0.074
[ 8 / 40 ] loss: 0.101
[ 9 / 40 ] loss: 0.112
[ 10 / 40 ] loss: 0.049
[ 11 / 40 ] loss: 0.009
[ 12 / 40 ] loss: 0.049
[ 13 / 40 ] loss: 0.042
[ 14 / 40 ] loss: 0.048
[ 15 / 40 ] loss: 0.011
[ 16 / 40 ] loss: 0.237
[ 17 / 40 ] loss: 0.198
[ 18 / 40 ] loss: 0.097
[ 19 / 40 ] loss: 0.045
[ 20 / 40 ] loss: 0.349
[ 21 / 40 ] loss: 0.019
[ 22 / 40 ] loss: 0.088
[ 23 / 40 ] loss: 0.075
[ 24 / 40 ] loss: 0.038
[ 25 / 40 ] loss: 0.238
[ 26 / 40 ] loss: 0.109
[ 27 / 40 ] loss: 0.025
[ 28 / 40 ] loss: 0.060
[ 29 / 40 ] loss: 0.025
[ 30 / 40 ] loss: 0.049
[ 31 / 40 ] loss: 0.016
[ 32 / 40 ] loss: 0.210
[ 33 / 40 ] loss: 0.102
[ 34 / 40 ] loss: 0.037
[ 35 / 40 ] loss: 0.033
[ 36 / 40 ] loss: 0.119
[ 37 / 40 ] loss: 0.063
[ 38 / 40 ] loss: 0.319
[ 39 / 40 ] loss: 0.061
[ 40 / 40 ] loss: 0.023
0.09119161332491785
Accuracy: 0.767932 -- Precision: 0.777778 -- Recall: 0.807692 -- F1: 0.792453 -- AUC: 0.798347
========= epoch: 45 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.024
[ 2 / 40 ] loss: 0.115
[ 3 / 40 ] loss: 0.451
[ 4 / 40 ] loss: 0.047
[ 5 / 40 ] loss: 0.157
[ 6 / 40 ] loss: 0.024
[ 7 / 40 ] loss: 0.391
[ 8 / 40 ] loss: 0.019
[ 9 / 40 ] loss: 0.084
[ 10 / 40 ] loss: 0.170
[ 11 / 40 ] loss: 0.012
[ 12 / 40 ] loss: 0.052
[ 13 / 40 ] loss: 0.066
[ 14 / 40 ] loss: 0.329
[ 15 / 40 ] loss: 0.016
[ 16 / 40 ] loss: 0.103
[ 17 / 40 ] loss: 0.027
[ 18 / 40 ] loss: 0.105
[ 19 / 40 ] loss: 0.025
[ 20 / 40 ] loss: 0.189
[ 21 / 40 ] loss: 0.082
[ 22 / 40 ] loss: 0.104
[ 23 / 40 ] loss: 0.013
[ 24 / 40 ] loss: 0.076
[ 25 / 40 ] loss: 0.036
[ 26 / 40 ] loss: 0.117
[ 27 / 40 ] loss: 0.015
[ 28 / 40 ] loss: 0.052
[ 29 / 40 ] loss: 0.014
[ 30 / 40 ] loss: 0.060
[ 31 / 40 ] loss: 0.012
[ 32 / 40 ] loss: 0.226
[ 33 / 40 ] loss: 0.166
[ 34 / 40 ] loss: 0.071
[ 35 / 40 ] loss: 0.027
[ 36 / 40 ] loss: 0.030
[ 37 / 40 ] loss: 0.026
[ 38 / 40 ] loss: 0.073
[ 39 / 40 ] loss: 0.045
[ 40 / 40 ] loss: 0.018
0.09177420276682824
Accuracy: 0.763713 -- Precision: 0.798387 -- Recall: 0.761538 -- F1: 0.779528 -- AUC: 0.797879
========= epoch: 46 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.146
[ 2 / 40 ] loss: 0.033
[ 3 / 40 ] loss: 0.015
[ 4 / 40 ] loss: 0.015
[ 5 / 40 ] loss: 0.258
[ 6 / 40 ] loss: 0.055
[ 7 / 40 ] loss: 0.033
[ 8 / 40 ] loss: 0.057
[ 9 / 40 ] loss: 0.010
[ 10 / 40 ] loss: 0.010
[ 11 / 40 ] loss: 0.173
[ 12 / 40 ] loss: 0.039
[ 13 / 40 ] loss: 0.022
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.063
[ 17 / 40 ] loss: 0.024
[ 18 / 40 ] loss: 0.007
[ 19 / 40 ] loss: 0.031
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.008
[ 23 / 40 ] loss: 0.005
[ 24 / 40 ] loss: 0.011
[ 25 / 40 ] loss: 0.195
[ 26 / 40 ] loss: 0.006
[ 27 / 40 ] loss: 0.083
[ 28 / 40 ] loss: 0.005
[ 29 / 40 ] loss: 0.138
[ 30 / 40 ] loss: 0.007
[ 31 / 40 ] loss: 0.007
[ 32 / 40 ] loss: 0.222
[ 33 / 40 ] loss: 0.022
[ 34 / 40 ] loss: 0.010
[ 35 / 40 ] loss: 0.418
[ 36 / 40 ] loss: 0.055
[ 37 / 40 ] loss: 0.105
[ 38 / 40 ] loss: 0.184
[ 39 / 40 ] loss: 0.023
[ 40 / 40 ] loss: 0.535
0.07633639639243484
Accuracy: 0.734177 -- Precision: 0.813084 -- Recall: 0.669231 -- F1: 0.734177 -- AUC: 0.780805
========= epoch: 47 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.092
[ 2 / 40 ] loss: 0.097
[ 3 / 40 ] loss: 0.127
[ 4 / 40 ] loss: 0.206
[ 5 / 40 ] loss: 0.194
[ 6 / 40 ] loss: 0.145
[ 7 / 40 ] loss: 0.235
[ 8 / 40 ] loss: 0.076
[ 9 / 40 ] loss: 0.030
[ 10 / 40 ] loss: 0.012
[ 11 / 40 ] loss: 0.154
[ 12 / 40 ] loss: 0.022
[ 13 / 40 ] loss: 0.070
[ 14 / 40 ] loss: 0.271
[ 15 / 40 ] loss: 0.172
[ 16 / 40 ] loss: 0.235
[ 17 / 40 ] loss: 0.116
[ 18 / 40 ] loss: 0.023
[ 19 / 40 ] loss: 0.021
[ 20 / 40 ] loss: 0.032
[ 21 / 40 ] loss: 0.018
[ 22 / 40 ] loss: 0.128
[ 23 / 40 ] loss: 0.017
[ 24 / 40 ] loss: 0.010
[ 25 / 40 ] loss: 0.476
[ 26 / 40 ] loss: 0.282
[ 27 / 40 ] loss: 0.470
[ 28 / 40 ] loss: 0.134
[ 29 / 40 ] loss: 0.364
[ 30 / 40 ] loss: 0.042
[ 31 / 40 ] loss: 0.267
[ 32 / 40 ] loss: 0.020
[ 33 / 40 ] loss: 0.035
[ 34 / 40 ] loss: 0.030
[ 35 / 40 ] loss: 0.138
[ 36 / 40 ] loss: 0.315
[ 37 / 40 ] loss: 0.373
[ 38 / 40 ] loss: 0.315
[ 39 / 40 ] loss: 0.061
[ 40 / 40 ] loss: 0.165
0.1497450958704576
Accuracy: 0.759494 -- Precision: 0.758865 -- Recall: 0.823077 -- F1: 0.789668 -- AUC: 0.813803
========= epoch: 48 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.275
[ 2 / 40 ] loss: 0.024
[ 3 / 40 ] loss: 0.151
[ 4 / 40 ] loss: 0.032
[ 5 / 40 ] loss: 0.124
[ 6 / 40 ] loss: 0.060
[ 7 / 40 ] loss: 0.226
[ 8 / 40 ] loss: 0.225
[ 9 / 40 ] loss: 0.036
[ 10 / 40 ] loss: 0.333
[ 11 / 40 ] loss: 0.125
[ 12 / 40 ] loss: 0.130
[ 13 / 40 ] loss: 0.067
[ 14 / 40 ] loss: 0.110
[ 15 / 40 ] loss: 0.080
[ 16 / 40 ] loss: 0.069
[ 17 / 40 ] loss: 0.073
[ 18 / 40 ] loss: 0.097
[ 19 / 40 ] loss: 0.018
[ 20 / 40 ] loss: 0.165
[ 21 / 40 ] loss: 0.059
[ 22 / 40 ] loss: 0.012
[ 23 / 40 ] loss: 0.481
[ 24 / 40 ] loss: 0.065
[ 25 / 40 ] loss: 0.163
[ 26 / 40 ] loss: 0.162
[ 27 / 40 ] loss: 0.060
[ 28 / 40 ] loss: 0.019
[ 29 / 40 ] loss: 0.144
[ 30 / 40 ] loss: 0.198
[ 31 / 40 ] loss: 0.126
[ 32 / 40 ] loss: 0.011
[ 33 / 40 ] loss: 0.021
[ 34 / 40 ] loss: 0.070
[ 35 / 40 ] loss: 0.132
[ 36 / 40 ] loss: 0.064
[ 37 / 40 ] loss: 0.036
[ 38 / 40 ] loss: 0.034
[ 39 / 40 ] loss: 0.218
[ 40 / 40 ] loss: 0.058
0.11381268100813031
Accuracy: 0.725738 -- Precision: 0.787611 -- Recall: 0.684615 -- F1: 0.732510 -- AUC: 0.747664
========= epoch: 49 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.108
[ 2 / 40 ] loss: 0.055
[ 3 / 40 ] loss: 0.101
[ 4 / 40 ] loss: 0.113
[ 5 / 40 ] loss: 0.106
[ 6 / 40 ] loss: 0.055
[ 7 / 40 ] loss: 0.033
[ 8 / 40 ] loss: 0.016
[ 9 / 40 ] loss: 0.045
[ 10 / 40 ] loss: 0.043
[ 11 / 40 ] loss: 0.032
[ 12 / 40 ] loss: 0.329
[ 13 / 40 ] loss: 0.033
[ 14 / 40 ] loss: 0.071
[ 15 / 40 ] loss: 0.045
[ 16 / 40 ] loss: 0.024
[ 17 / 40 ] loss: 0.017
[ 18 / 40 ] loss: 0.157
[ 19 / 40 ] loss: 0.079
[ 20 / 40 ] loss: 0.077
[ 21 / 40 ] loss: 0.227
[ 22 / 40 ] loss: 0.096
[ 23 / 40 ] loss: 0.017
[ 24 / 40 ] loss: 0.142
[ 25 / 40 ] loss: 0.140
[ 26 / 40 ] loss: 0.017
[ 27 / 40 ] loss: 0.032
[ 28 / 40 ] loss: 0.018
[ 29 / 40 ] loss: 0.026
[ 30 / 40 ] loss: 0.234
[ 31 / 40 ] loss: 0.131
[ 32 / 40 ] loss: 0.242
[ 33 / 40 ] loss: 0.009
[ 34 / 40 ] loss: 0.248
[ 35 / 40 ] loss: 0.013
[ 36 / 40 ] loss: 0.014
[ 37 / 40 ] loss: 0.093
[ 38 / 40 ] loss: 0.106
[ 39 / 40 ] loss: 0.033
[ 40 / 40 ] loss: 0.015
0.08481126695405691
Accuracy: 0.738397 -- Precision: 0.778689 -- Recall: 0.730769 -- F1: 0.753968 -- AUC: 0.793961
0.310548523206751 0.3244079076140145 0.30923076923076925 0.3163851272744195 0.31791516894320637
[113, 82, 39, 57, 189, 58, 178, 218, 172, 152, 66, 188, 185, 181, 195, 51, 91, 146, 40, 215, 63, 72, 119, 184, 101, 194, 74, 23, 227, 201, 200, 60, 95, 233, 192, 123, 202, 237, 186, 31, 7, 18, 187, 171, 132, 204, 214, 125, 45, 16, 22, 124, 109, 34, 232, 41, 169, 167, 225, 234, 166, 55, 128, 61, 32, 68, 219, 2, 149, 76, 207, 35, 43, 209, 62, 158, 196, 73, 168, 11, 230, 115, 93, 129, 59, 220, 154, 121, 142, 208, 231, 116, 131, 42, 147, 37, 136, 87, 191, 112, 213, 173, 175, 150, 10, 139, 221, 75, 183, 15, 199, 24, 193, 143, 47, 176, 103, 38, 54, 81, 228, 56, 9, 8, 97, 96, 78, 27, 25, 85, 212, 50, 44, 110, 30, 71, 177, 14, 159, 84, 86, 46, 65, 100, 216, 26, 190, 67, 236, 134, 52, 117, 89, 222, 211, 210, 144, 137, 153, 94, 6, 140, 79, 223, 48, 12, 205, 120, 53, 17, 226, 19, 170, 36, 155, 174, 164, 3, 151, 69, 217, 180, 88, 13, 92, 99, 165, 163, 162, 64, 138, 28, 29, 98, 203, 130, 77, 104, 229, 107, 108, 21, 197, 156, 157, 114, 126, 127, 161, 179, 118, 224, 141, 105, 90, 70, 33, 182, 5, 106, 83, 133, 235, 20, 206, 135, 122, 145, 80, 198, 49, 148, 102, 111, 4, 160, 1, 156, 127, 91, 61, 15, 124, 44, 130, 216, 235, 170, 66, 53, 67, 12, 14, 154, 196, 126, 176, 168, 76, 54, 88, 129, 201, 166, 94, 161, 200, 158, 78, 199, 211, 81, 29, 73, 184, 237, 26, 179, 45, 32, 134, 116, 75, 164, 20, 56, 24, 84, 8, 132, 183, 59, 57, 58, 202, 1, 192, 46, 42, 121, 34, 206, 217, 48, 155, 207, 13, 223, 18, 185, 140, 23, 107, 37, 138, 52, 172, 30, 106, 22, 227, 149, 47, 82, 125, 112, 114, 95, 188, 175, 115, 69, 16, 70, 25, 225, 177, 79, 195, 74, 109, 146, 186, 232, 43, 137, 80, 77, 3, 27, 105, 83, 60, 190, 220, 101, 10, 113, 210, 40, 122, 193, 39, 141, 108, 98, 221, 49, 197, 171, 150, 87, 119, 50, 62, 139, 99, 167, 93, 21, 209, 55, 173, 65, 38, 194, 204, 2, 189, 41, 102, 152, 36, 213, 131, 203, 228, 162, 198, 9, 143, 218, 11, 157, 219, 144, 230, 234, 224, 111, 96, 5, 120, 85, 51, 71, 136, 180, 117, 169, 229, 7, 208, 222, 19, 214, 226, 178, 147, 103, 133, 165, 236, 31, 187, 4, 104, 72, 148, 68, 215, 63, 35, 151, 191, 182, 110, 174, 145, 64, 89, 123, 142, 17, 90, 100, 233, 6, 159, 135, 231, 160, 128, 28, 212, 97, 118, 163, 153, 92, 181, 205, 33, 86]
[1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0]
[1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0]
训练集: 946
测试集: 237
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 0 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.706
[ 2 / 40 ] loss: 0.690
[ 3 / 40 ] loss: 0.691
[ 4 / 40 ] loss: 0.694
[ 5 / 40 ] loss: 0.693
[ 6 / 40 ] loss: 0.693
[ 7 / 40 ] loss: 0.692
[ 8 / 40 ] loss: 0.687
[ 9 / 40 ] loss: 0.679
[ 10 / 40 ] loss: 0.688
[ 11 / 40 ] loss: 0.652
[ 12 / 40 ] loss: 0.705
[ 13 / 40 ] loss: 0.720
[ 14 / 40 ] loss: 0.708
[ 15 / 40 ] loss: 0.695
[ 16 / 40 ] loss: 0.661
[ 17 / 40 ] loss: 0.677
[ 18 / 40 ] loss: 0.683
[ 19 / 40 ] loss: 0.708
[ 20 / 40 ] loss: 0.704
[ 21 / 40 ] loss: 0.651
[ 22 / 40 ] loss: 0.669
[ 23 / 40 ] loss: 0.673
[ 24 / 40 ] loss: 0.660
[ 25 / 40 ] loss: 0.690
[ 26 / 40 ] loss: 0.655
[ 27 / 40 ] loss: 0.649
[ 28 / 40 ] loss: 0.631
[ 29 / 40 ] loss: 0.630
[ 30 / 40 ] loss: 0.690
[ 31 / 40 ] loss: 0.677
[ 32 / 40 ] loss: 0.658
[ 33 / 40 ] loss: 0.698
[ 34 / 40 ] loss: 0.685
[ 35 / 40 ] loss: 0.696
[ 36 / 40 ] loss: 0.700
[ 37 / 40 ] loss: 0.697
[ 38 / 40 ] loss: 0.667
[ 39 / 40 ] loss: 0.675
[ 40 / 40 ] loss: 0.645
0.6806142017245292
Accuracy: 0.481013 -- Precision: 0.585366 -- Recall: 0.184615 -- F1: 0.280702 -- AUC: 0.618907
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[231, 61, 171, 70, 15, 68, 94, 232, 24, 56, 69, 152, 159, 46, 226, 223, 176, 124, 163, 145, 191, 118, 173, 204, 128, 154, 213, 67, 127, 185, 64, 90, 184, 20, 111, 125, 206, 4, 71, 102, 60, 153, 42, 148, 37, 141, 74, 190, 54, 224, 66, 175, 150, 14, 151, 169, 73, 17, 237, 113, 2, 180, 188, 211, 41, 167, 121, 119, 43, 164, 234, 79, 143, 116, 114, 110, 182, 156, 158, 11, 207, 136, 65, 85, 47, 8, 33, 95, 201, 157, 99, 235, 87, 45, 109, 225, 216, 181, 34, 218, 123, 50, 57, 203, 91, 144, 227, 86, 138, 165, 35, 98, 130, 89, 18, 195, 142, 139, 131, 58, 178, 229, 62, 28, 179, 228, 162, 170, 149, 177, 196, 200, 52, 9, 103, 22, 55, 133, 198, 78, 230, 25, 210, 76, 32, 48, 236, 92, 187, 221, 197, 5, 132, 93, 53, 16, 19, 205, 77, 186, 183, 81, 233, 146, 40, 122, 160, 1, 117, 105, 202, 217, 120, 129, 59, 75, 21, 27, 189, 80, 220, 140, 96, 193, 155, 83, 134, 126, 212, 88, 222, 147, 72, 12, 63, 107, 199, 51, 112, 84, 168, 108, 104, 166, 13, 82, 29, 192, 10, 38, 214, 219, 49, 135, 161, 115, 31, 106, 172, 44, 215, 194, 26, 7, 208, 39, 30, 209, 97, 36, 137, 3, 6, 23, 174, 100, 101]
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]
[1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 1 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.720
[ 2 / 40 ] loss: 0.663
[ 3 / 40 ] loss: 0.671
[ 4 / 40 ] loss: 0.670
[ 5 / 40 ] loss: 0.656
[ 6 / 40 ] loss: 0.627
[ 7 / 40 ] loss: 0.636
[ 8 / 40 ] loss: 0.588
[ 9 / 40 ] loss: 0.631
[ 10 / 40 ] loss: 0.701
[ 11 / 40 ] loss: 0.656
[ 12 / 40 ] loss: 0.599
[ 13 / 40 ] loss: 0.589
[ 14 / 40 ] loss: 0.557
[ 15 / 40 ] loss: 0.603
[ 16 / 40 ] loss: 0.746
[ 17 / 40 ] loss: 0.644
[ 18 / 40 ] loss: 0.698
[ 19 / 40 ] loss: 0.573
[ 20 / 40 ] loss: 0.685
[ 21 / 40 ] loss: 0.791
[ 22 / 40 ] loss: 0.723
[ 23 / 40 ] loss: 0.727
[ 24 / 40 ] loss: 0.685
[ 25 / 40 ] loss: 0.638
[ 26 / 40 ] loss: 0.639
[ 27 / 40 ] loss: 0.625
[ 28 / 40 ] loss: 0.620
[ 29 / 40 ] loss: 0.596
[ 30 / 40 ] loss: 0.635
[ 31 / 40 ] loss: 0.793
[ 32 / 40 ] loss: 0.610
[ 33 / 40 ] loss: 0.661
[ 34 / 40 ] loss: 0.745
[ 35 / 40 ] loss: 0.722
[ 36 / 40 ] loss: 0.613
[ 37 / 40 ] loss: 0.628
[ 38 / 40 ] loss: 0.657
[ 39 / 40 ] loss: 0.690
[ 40 / 40 ] loss: 0.639
0.6588168814778328
Accuracy: 0.561181 -- Precision: 0.630000 -- Recall: 0.484615 -- F1: 0.547826 -- AUC: 0.618835
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[50, 87, 93, 19, 157, 205, 206, 229, 214, 82, 25, 201, 163, 230, 84, 109, 117, 136, 77, 204, 164, 90, 110, 105, 129, 15, 91, 115, 81, 165, 218, 237, 153, 137, 119, 4, 197, 76, 17, 120, 66, 140, 28, 46, 75, 69, 52, 160, 149, 220, 210, 236, 14, 58, 133, 51, 167, 96, 222, 41, 223, 195, 23, 108, 21, 189, 62, 193, 123, 98, 150, 30, 196, 187, 32, 158, 47, 199, 152, 181, 173, 170, 80, 228, 79, 54, 38, 49, 174, 177, 10, 6, 162, 27, 100, 122, 147, 68, 155, 225, 185, 39, 141, 148, 85, 190, 9, 83, 92, 63, 224, 24, 7, 22, 13, 113, 126, 132, 144, 130, 97, 202, 74, 106, 102, 183, 111, 116, 89, 48, 138, 53, 134, 139, 203, 118, 36, 64, 70, 94, 88, 5, 18, 78, 42, 86, 99, 71, 200, 20, 151, 104, 232, 65, 60, 56, 171, 154, 159, 131, 184, 145, 57, 191, 61, 178, 12, 231, 124, 55, 179, 72, 226, 188, 59, 233, 3, 215, 156, 128, 176, 169, 67, 217, 45, 37, 103, 11, 34, 198, 194, 40, 35, 121, 175, 168, 161, 16, 8, 107, 31, 172, 207, 216, 213, 192, 235, 234, 180, 186, 212, 125, 211, 73, 209, 142, 166, 101, 33, 127, 29, 135, 208, 112, 95, 146, 26, 44, 1, 227, 114, 143, 43, 182, 219, 2, 221]
[1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0]
[0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 2 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.676
[ 2 / 40 ] loss: 0.649
[ 3 / 40 ] loss: 0.664
[ 4 / 40 ] loss: 0.583
[ 5 / 40 ] loss: 0.600
[ 6 / 40 ] loss: 0.635
[ 7 / 40 ] loss: 0.569
[ 8 / 40 ] loss: 0.692
[ 9 / 40 ] loss: 0.620
[ 10 / 40 ] loss: 0.530
[ 11 / 40 ] loss: 0.673
[ 12 / 40 ] loss: 0.502
[ 13 / 40 ] loss: 0.642
[ 14 / 40 ] loss: 0.609
[ 15 / 40 ] loss: 0.505
[ 16 / 40 ] loss: 0.533
[ 17 / 40 ] loss: 0.688
[ 18 / 40 ] loss: 0.593
[ 19 / 40 ] loss: 0.695
[ 20 / 40 ] loss: 0.554
[ 21 / 40 ] loss: 0.624
[ 22 / 40 ] loss: 0.557
[ 23 / 40 ] loss: 0.503
[ 24 / 40 ] loss: 0.510
[ 25 / 40 ] loss: 0.621
[ 26 / 40 ] loss: 0.658
[ 27 / 40 ] loss: 0.646
[ 28 / 40 ] loss: 0.604
[ 29 / 40 ] loss: 0.652
[ 30 / 40 ] loss: 0.594
[ 31 / 40 ] loss: 0.620
[ 32 / 40 ] loss: 0.709
[ 33 / 40 ] loss: 0.719
[ 34 / 40 ] loss: 0.704
[ 35 / 40 ] loss: 0.585
[ 36 / 40 ] loss: 0.635
[ 37 / 40 ] loss: 0.728
[ 38 / 40 ] loss: 0.688
[ 39 / 40 ] loss: 0.676
[ 40 / 40 ] loss: 0.585
0.6208116874098778
Accuracy: 0.632911 -- Precision: 0.605911 -- Recall: 0.946154 -- F1: 0.738739 -- AUC: 0.737168
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[179, 133, 96, 212, 8, 187, 81, 185, 27, 142, 154, 117, 173, 219, 160, 128, 130, 13, 25, 56, 29, 147, 6, 86, 32, 231, 192, 28, 188, 225, 203, 113, 171, 205, 16, 215, 51, 221, 54, 82, 236, 224, 210, 65, 209, 44, 88, 141, 61, 182, 10, 137, 39, 197, 206, 11, 201, 59, 107, 3, 198, 207, 111, 186, 149, 15, 204, 112, 168, 92, 178, 42, 229, 193, 80, 153, 49, 145, 60, 227, 150, 156, 218, 199, 125, 12, 4, 226, 1, 118, 202, 172, 105, 170, 176, 155, 189, 18, 135, 233, 46, 109, 157, 163, 220, 139, 216, 67, 165, 84, 235, 52, 190, 89, 71, 140, 7, 134, 45, 191, 223, 83, 232, 58, 222, 99, 68, 14, 177, 78, 230, 9, 180, 48, 50, 70, 164, 30, 38, 23, 85, 19, 21, 73, 159, 41, 106, 76, 184, 63, 208, 53, 93, 174, 69, 22, 47, 64, 158, 43, 103, 35, 237, 114, 66, 26, 115, 138, 104, 97, 110, 234, 131, 57, 95, 119, 169, 100, 55, 144, 126, 36, 194, 166, 91, 136, 75, 148, 132, 74, 79, 146, 127, 152, 167, 34, 143, 87, 214, 17, 40, 20, 120, 213, 77, 72, 116, 24, 98, 101, 2, 122, 217, 121, 123, 181, 102, 151, 200, 211, 90, 37, 161, 62, 108, 228, 33, 175, 94, 5, 124, 129, 31, 195, 183, 196, 162]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]
[1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 3 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.539
[ 2 / 40 ] loss: 0.522
[ 3 / 40 ] loss: 0.550
[ 4 / 40 ] loss: 0.541
[ 5 / 40 ] loss: 0.617
[ 6 / 40 ] loss: 0.611
[ 7 / 40 ] loss: 0.501
[ 8 / 40 ] loss: 0.699
[ 9 / 40 ] loss: 0.467
[ 10 / 40 ] loss: 0.533
[ 11 / 40 ] loss: 0.605
[ 12 / 40 ] loss: 0.684
[ 13 / 40 ] loss: 0.520
[ 14 / 40 ] loss: 0.569
[ 15 / 40 ] loss: 0.388
[ 16 / 40 ] loss: 0.454
[ 17 / 40 ] loss: 0.440
[ 18 / 40 ] loss: 0.614
[ 19 / 40 ] loss: 0.600
[ 20 / 40 ] loss: 0.428
[ 21 / 40 ] loss: 0.634
[ 22 / 40 ] loss: 0.660
[ 23 / 40 ] loss: 0.448
[ 24 / 40 ] loss: 0.650
[ 25 / 40 ] loss: 0.588
[ 26 / 40 ] loss: 0.540
[ 27 / 40 ] loss: 0.424
[ 28 / 40 ] loss: 0.424
[ 29 / 40 ] loss: 0.626
[ 30 / 40 ] loss: 0.522
[ 31 / 40 ] loss: 0.808
[ 32 / 40 ] loss: 0.499
[ 33 / 40 ] loss: 0.445
[ 34 / 40 ] loss: 0.612
[ 35 / 40 ] loss: 0.425
[ 36 / 40 ] loss: 0.592
[ 37 / 40 ] loss: 0.463
[ 38 / 40 ] loss: 0.518
[ 39 / 40 ] loss: 0.784
[ 40 / 40 ] loss: 0.765
0.5576913796365262
Accuracy: 0.590717 -- Precision: 0.761905 -- Recall: 0.369231 -- F1: 0.497409 -- AUC: 0.757081
========= epoch: 4 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.542
[ 2 / 40 ] loss: 0.465
[ 3 / 40 ] loss: 0.611
[ 4 / 40 ] loss: 0.529
[ 5 / 40 ] loss: 0.457
[ 6 / 40 ] loss: 0.486
[ 7 / 40 ] loss: 0.495
[ 8 / 40 ] loss: 0.502
[ 9 / 40 ] loss: 0.417
[ 10 / 40 ] loss: 0.407
[ 11 / 40 ] loss: 0.327
[ 12 / 40 ] loss: 0.480
[ 13 / 40 ] loss: 0.497
[ 14 / 40 ] loss: 0.454
[ 15 / 40 ] loss: 0.370
[ 16 / 40 ] loss: 0.311
[ 17 / 40 ] loss: 0.602
[ 18 / 40 ] loss: 0.525
[ 19 / 40 ] loss: 0.594
[ 20 / 40 ] loss: 0.443
[ 21 / 40 ] loss: 0.524
[ 22 / 40 ] loss: 0.611
[ 23 / 40 ] loss: 0.367
[ 24 / 40 ] loss: 0.413
[ 25 / 40 ] loss: 0.540
[ 26 / 40 ] loss: 0.406
[ 27 / 40 ] loss: 0.231
[ 28 / 40 ] loss: 0.548
[ 29 / 40 ] loss: 0.591
[ 30 / 40 ] loss: 0.783
[ 31 / 40 ] loss: 0.487
[ 32 / 40 ] loss: 0.415
[ 33 / 40 ] loss: 0.559
[ 34 / 40 ] loss: 0.434
[ 35 / 40 ] loss: 0.575
[ 36 / 40 ] loss: 0.648
[ 37 / 40 ] loss: 0.365
[ 38 / 40 ] loss: 0.603
[ 39 / 40 ] loss: 0.578
[ 40 / 40 ] loss: 0.376
0.4892489410936832
Accuracy: 0.704641 -- Precision: 0.661290 -- Recall: 0.946154 -- F1: 0.778481 -- AUC: 0.801006
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[175, 129, 23, 25, 118, 160, 51, 169, 155, 209, 2, 195, 207, 167, 90, 191, 97, 163, 225, 196, 162, 218, 30, 49, 79, 128, 117, 10, 42, 228, 148, 70, 92, 231, 208, 64, 80, 84, 136, 33, 17, 12, 115, 5, 180, 102, 54, 223, 158, 65, 98, 177, 198, 11, 104, 99, 236, 214, 1, 114, 201, 66, 83, 106, 235, 185, 7, 213, 153, 69, 161, 76, 77, 133, 131, 164, 27, 13, 217, 94, 18, 119, 143, 181, 19, 182, 229, 202, 127, 124, 122, 101, 121, 151, 100, 227, 116, 109, 211, 170, 87, 221, 190, 168, 44, 141, 193, 108, 103, 149, 189, 74, 200, 14, 199, 36, 55, 186, 29, 113, 237, 146, 205, 20, 137, 135, 216, 132, 3, 123, 56, 222, 184, 179, 38, 230, 159, 139, 22, 203, 82, 59, 210, 176, 53, 41, 107, 32, 215, 57, 6, 26, 147, 173, 96, 120, 134, 28, 224, 48, 183, 71, 31, 45, 72, 47, 61, 86, 34, 232, 197, 75, 62, 50, 130, 93, 88, 233, 68, 204, 219, 145, 95, 78, 138, 220, 140, 166, 39, 174, 24, 81, 112, 157, 35, 91, 188, 172, 16, 105, 4, 89, 144, 52, 171, 40, 165, 125, 206, 126, 154, 60, 63, 73, 58, 226, 156, 111, 234, 142, 43, 46, 192, 8, 194, 150, 212, 37, 110, 178, 15, 152, 21, 9, 85, 67, 187]
[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1]
[1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 5 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.521
[ 2 / 40 ] loss: 0.362
[ 3 / 40 ] loss: 0.820
[ 4 / 40 ] loss: 0.715
[ 5 / 40 ] loss: 0.719
[ 6 / 40 ] loss: 0.651
[ 7 / 40 ] loss: 0.485
[ 8 / 40 ] loss: 0.441
[ 9 / 40 ] loss: 0.463
[ 10 / 40 ] loss: 0.318
[ 11 / 40 ] loss: 0.472
[ 12 / 40 ] loss: 0.449
[ 13 / 40 ] loss: 0.364
[ 14 / 40 ] loss: 0.304
[ 15 / 40 ] loss: 0.282
[ 16 / 40 ] loss: 0.433
[ 17 / 40 ] loss: 0.484
[ 18 / 40 ] loss: 0.289
[ 19 / 40 ] loss: 0.305
[ 20 / 40 ] loss: 0.428
[ 21 / 40 ] loss: 0.389
[ 22 / 40 ] loss: 0.478
[ 23 / 40 ] loss: 0.523
[ 24 / 40 ] loss: 0.269
[ 25 / 40 ] loss: 0.450
[ 26 / 40 ] loss: 0.340
[ 27 / 40 ] loss: 0.508
[ 28 / 40 ] loss: 0.214
[ 29 / 40 ] loss: 0.367
[ 30 / 40 ] loss: 0.626
[ 31 / 40 ] loss: 0.320
[ 32 / 40 ] loss: 0.515
[ 33 / 40 ] loss: 0.218
[ 34 / 40 ] loss: 0.321
[ 35 / 40 ] loss: 0.306
[ 36 / 40 ] loss: 0.616
[ 37 / 40 ] loss: 0.257
[ 38 / 40 ] loss: 0.487
[ 39 / 40 ] loss: 0.317
[ 40 / 40 ] loss: 0.145
0.42434159591794013
Accuracy: 0.767932 -- Precision: 0.735849 -- Recall: 0.900000 -- F1: 0.809689 -- AUC: 0.803882
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[42, 200, 203, 151, 199, 89, 53, 50, 97, 1, 231, 205, 196, 212, 26, 92, 107, 63, 185, 49, 3, 33, 105, 142, 17, 48, 20, 136, 24, 28, 189, 36, 225, 75, 76, 176, 47, 206, 46, 150, 184, 237, 132, 112, 70, 55, 57, 19, 152, 233, 108, 106, 35, 213, 156, 23, 153, 168, 169, 149, 214, 170, 71, 202, 40, 78, 52, 161, 83, 164, 182, 179, 56, 116, 44, 223, 177, 12, 194, 140, 29, 190, 16, 130, 4, 198, 234, 62, 216, 172, 197, 115, 80, 125, 215, 10, 154, 64, 79, 144, 31, 230, 186, 113, 159, 201, 219, 128, 207, 66, 204, 6, 98, 141, 221, 167, 117, 133, 15, 160, 210, 101, 147, 183, 235, 37, 73, 82, 114, 178, 90, 87, 34, 18, 94, 88, 195, 41, 45, 171, 91, 61, 236, 134, 163, 30, 22, 9, 224, 218, 102, 109, 95, 139, 129, 146, 145, 77, 192, 126, 68, 13, 54, 227, 232, 157, 222, 110, 59, 104, 211, 208, 229, 74, 166, 131, 7, 119, 226, 32, 81, 193, 14, 135, 180, 124, 5, 181, 38, 188, 84, 187, 137, 86, 165, 138, 43, 209, 60, 67, 155, 118, 2, 217, 51, 69, 228, 122, 96, 143, 103, 8, 27, 65, 25, 93, 121, 148, 173, 158, 72, 174, 21, 39, 120, 99, 123, 175, 11, 220, 58, 111, 127, 162, 191, 85, 100]
[0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 6 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.310
[ 2 / 40 ] loss: 0.228
[ 3 / 40 ] loss: 0.161
[ 4 / 40 ] loss: 0.294
[ 5 / 40 ] loss: 0.352
[ 6 / 40 ] loss: 0.277
[ 7 / 40 ] loss: 0.293
[ 8 / 40 ] loss: 0.131
[ 9 / 40 ] loss: 0.433
[ 10 / 40 ] loss: 0.270
[ 11 / 40 ] loss: 0.291
[ 12 / 40 ] loss: 0.360
[ 13 / 40 ] loss: 0.361
[ 14 / 40 ] loss: 0.595
[ 15 / 40 ] loss: 0.385
[ 16 / 40 ] loss: 0.569
[ 17 / 40 ] loss: 0.434
[ 18 / 40 ] loss: 0.408
[ 19 / 40 ] loss: 0.421
[ 20 / 40 ] loss: 0.380
[ 21 / 40 ] loss: 0.507
[ 22 / 40 ] loss: 0.429
[ 23 / 40 ] loss: 0.325
[ 24 / 40 ] loss: 0.504
[ 25 / 40 ] loss: 0.403
[ 26 / 40 ] loss: 0.443
[ 27 / 40 ] loss: 0.347
[ 28 / 40 ] loss: 0.411
[ 29 / 40 ] loss: 0.384
[ 30 / 40 ] loss: 0.314
[ 31 / 40 ] loss: 0.327
[ 32 / 40 ] loss: 0.360
[ 33 / 40 ] loss: 0.494
[ 34 / 40 ] loss: 0.362
[ 35 / 40 ] loss: 0.646
[ 36 / 40 ] loss: 0.287
[ 37 / 40 ] loss: 0.294
[ 38 / 40 ] loss: 0.322
[ 39 / 40 ] loss: 0.510
[ 40 / 40 ] loss: 0.366
0.37469131089746954
Accuracy: 0.755274 -- Precision: 0.757143 -- Recall: 0.815385 -- F1: 0.785185 -- AUC: 0.826168
========= epoch: 7 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.372
[ 2 / 40 ] loss: 0.410
[ 3 / 40 ] loss: 0.242
[ 4 / 40 ] loss: 0.220
[ 5 / 40 ] loss: 0.353
[ 6 / 40 ] loss: 0.214
[ 7 / 40 ] loss: 0.241
[ 8 / 40 ] loss: 0.187
[ 9 / 40 ] loss: 0.238
[ 10 / 40 ] loss: 0.396
[ 11 / 40 ] loss: 0.205
[ 12 / 40 ] loss: 0.459
[ 13 / 40 ] loss: 0.252
[ 14 / 40 ] loss: 0.410
[ 15 / 40 ] loss: 0.303
[ 16 / 40 ] loss: 0.223
[ 17 / 40 ] loss: 0.184
[ 18 / 40 ] loss: 0.393
[ 19 / 40 ] loss: 0.566
[ 20 / 40 ] loss: 0.348
[ 21 / 40 ] loss: 0.582
[ 22 / 40 ] loss: 0.571
[ 23 / 40 ] loss: 0.139
[ 24 / 40 ] loss: 0.351
[ 25 / 40 ] loss: 0.361
[ 26 / 40 ] loss: 0.244
[ 27 / 40 ] loss: 0.265
[ 28 / 40 ] loss: 0.344
[ 29 / 40 ] loss: 0.310
[ 30 / 40 ] loss: 0.584
[ 31 / 40 ] loss: 0.442
[ 32 / 40 ] loss: 0.269
[ 33 / 40 ] loss: 0.302
[ 34 / 40 ] loss: 0.432
[ 35 / 40 ] loss: 0.424
[ 36 / 40 ] loss: 0.315
[ 37 / 40 ] loss: 0.267
[ 38 / 40 ] loss: 0.381
[ 39 / 40 ] loss: 0.294
[ 40 / 40 ] loss: 0.279
0.3343042813241482
Accuracy: 0.759494 -- Precision: 0.811966 -- Recall: 0.730769 -- F1: 0.769231 -- AUC: 0.837815
========= epoch: 8 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.195
[ 2 / 40 ] loss: 0.416
[ 3 / 40 ] loss: 0.350
[ 4 / 40 ] loss: 0.174
[ 5 / 40 ] loss: 0.245
[ 6 / 40 ] loss: 0.261
[ 7 / 40 ] loss: 0.349
[ 8 / 40 ] loss: 0.156
[ 9 / 40 ] loss: 0.390
[ 10 / 40 ] loss: 0.372
[ 11 / 40 ] loss: 0.306
[ 12 / 40 ] loss: 0.153
[ 13 / 40 ] loss: 0.333
[ 14 / 40 ] loss: 0.186
[ 15 / 40 ] loss: 0.467
[ 16 / 40 ] loss: 0.551
[ 17 / 40 ] loss: 0.072
[ 18 / 40 ] loss: 0.354
[ 19 / 40 ] loss: 0.112
[ 20 / 40 ] loss: 0.231
[ 21 / 40 ] loss: 0.203
[ 22 / 40 ] loss: 0.267
[ 23 / 40 ] loss: 0.493
[ 24 / 40 ] loss: 0.398
[ 25 / 40 ] loss: 0.466
[ 26 / 40 ] loss: 0.394
[ 27 / 40 ] loss: 0.331
[ 28 / 40 ] loss: 0.145
[ 29 / 40 ] loss: 0.192
[ 30 / 40 ] loss: 0.586
[ 31 / 40 ] loss: 0.380
[ 32 / 40 ] loss: 0.112
[ 33 / 40 ] loss: 0.349
[ 34 / 40 ] loss: 0.250
[ 35 / 40 ] loss: 0.563
[ 36 / 40 ] loss: 0.274
[ 37 / 40 ] loss: 0.238
[ 38 / 40 ] loss: 0.460
[ 39 / 40 ] loss: 0.278
[ 40 / 40 ] loss: 0.151
0.3050876587629318
Accuracy: 0.763713 -- Precision: 0.789062 -- Recall: 0.776923 -- F1: 0.782946 -- AUC: 0.833357
========= epoch: 9 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.297
[ 2 / 40 ] loss: 0.103
[ 3 / 40 ] loss: 0.349
[ 4 / 40 ] loss: 0.201
[ 5 / 40 ] loss: 0.427
[ 6 / 40 ] loss: 0.408
[ 7 / 40 ] loss: 0.277
[ 8 / 40 ] loss: 0.296
[ 9 / 40 ] loss: 0.300
[ 10 / 40 ] loss: 0.104
[ 11 / 40 ] loss: 0.323
[ 12 / 40 ] loss: 0.385
[ 13 / 40 ] loss: 0.195
[ 14 / 40 ] loss: 0.188
[ 15 / 40 ] loss: 0.221
[ 16 / 40 ] loss: 0.103
[ 17 / 40 ] loss: 0.159
[ 18 / 40 ] loss: 0.291
[ 19 / 40 ] loss: 0.216
[ 20 / 40 ] loss: 0.127
[ 21 / 40 ] loss: 0.436
[ 22 / 40 ] loss: 0.212
[ 23 / 40 ] loss: 0.410
[ 24 / 40 ] loss: 0.234
[ 25 / 40 ] loss: 0.376
[ 26 / 40 ] loss: 0.162
[ 27 / 40 ] loss: 0.420
[ 28 / 40 ] loss: 0.384
[ 29 / 40 ] loss: 0.317
[ 30 / 40 ] loss: 0.248
[ 31 / 40 ] loss: 0.225
[ 32 / 40 ] loss: 0.248
[ 33 / 40 ] loss: 0.272
[ 34 / 40 ] loss: 0.304
[ 35 / 40 ] loss: 0.254
[ 36 / 40 ] loss: 0.193
[ 37 / 40 ] loss: 0.503
[ 38 / 40 ] loss: 0.273
[ 39 / 40 ] loss: 0.289
[ 40 / 40 ] loss: 0.115
0.2711299367249012
Accuracy: 0.784810 -- Precision: 0.788321 -- Recall: 0.830769 -- F1: 0.808989 -- AUC: 0.836017
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[147, 229, 219, 41, 92, 34, 134, 45, 200, 225, 195, 85, 210, 173, 8, 136, 54, 207, 35, 95, 108, 230, 90, 149, 227, 53, 129, 56, 26, 181, 28, 148, 143, 197, 20, 228, 166, 86, 9, 25, 224, 128, 96, 3, 87, 170, 161, 97, 174, 47, 142, 124, 62, 160, 198, 183, 145, 205, 138, 126, 152, 27, 164, 6, 191, 75, 12, 139, 175, 55, 84, 21, 201, 153, 22, 235, 192, 18, 154, 119, 93, 60, 51, 67, 237, 33, 24, 44, 104, 232, 151, 186, 133, 63, 109, 68, 176, 52, 13, 32, 203, 30, 71, 19, 169, 162, 80, 77, 39, 46, 188, 231, 66, 144, 167, 215, 59, 172, 193, 211, 209, 1, 40, 4, 184, 132, 146, 36, 204, 218, 212, 165, 31, 206, 48, 117, 214, 69, 82, 15, 141, 116, 114, 49, 23, 118, 182, 178, 81, 106, 185, 2, 158, 123, 159, 196, 64, 43, 157, 233, 125, 131, 220, 111, 213, 38, 208, 107, 190, 177, 171, 199, 105, 103, 127, 5, 102, 101, 155, 121, 29, 10, 76, 236, 83, 226, 78, 58, 217, 11, 57, 88, 194, 98, 50, 113, 137, 61, 110, 17, 74, 120, 202, 221, 7, 79, 65, 89, 222, 112, 150, 91, 168, 100, 156, 180, 14, 16, 99, 70, 73, 130, 42, 135, 122, 140, 216, 72, 189, 234, 223, 94, 179, 37, 187, 115, 163]
[1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1]
[1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 10 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.223
[ 2 / 40 ] loss: 0.200
[ 3 / 40 ] loss: 0.377
[ 4 / 40 ] loss: 0.281
[ 5 / 40 ] loss: 0.124
[ 6 / 40 ] loss: 0.193
[ 7 / 40 ] loss: 0.145
[ 8 / 40 ] loss: 0.530
[ 9 / 40 ] loss: 0.158
[ 10 / 40 ] loss: 0.182
[ 11 / 40 ] loss: 0.249
[ 12 / 40 ] loss: 0.258
[ 13 / 40 ] loss: 0.112
[ 14 / 40 ] loss: 0.305
[ 15 / 40 ] loss: 0.170
[ 16 / 40 ] loss: 0.350
[ 17 / 40 ] loss: 0.060
[ 18 / 40 ] loss: 0.303
[ 19 / 40 ] loss: 0.206
[ 20 / 40 ] loss: 0.168
[ 21 / 40 ] loss: 0.207
[ 22 / 40 ] loss: 0.137
[ 23 / 40 ] loss: 0.315
[ 24 / 40 ] loss: 0.143
[ 25 / 40 ] loss: 0.190
[ 26 / 40 ] loss: 0.302
[ 27 / 40 ] loss: 0.415
[ 28 / 40 ] loss: 0.304
[ 29 / 40 ] loss: 0.141
[ 30 / 40 ] loss: 0.109
[ 31 / 40 ] loss: 0.156
[ 32 / 40 ] loss: 0.187
[ 33 / 40 ] loss: 0.287
[ 34 / 40 ] loss: 0.281
[ 35 / 40 ] loss: 0.182
[ 36 / 40 ] loss: 0.086
[ 37 / 40 ] loss: 0.251
[ 38 / 40 ] loss: 0.103
[ 39 / 40 ] loss: 0.178
[ 40 / 40 ] loss: 0.610
0.2294917955994606
Accuracy: 0.755274 -- Precision: 0.781250 -- Recall: 0.769231 -- F1: 0.775194 -- AUC: 0.822861
========= epoch: 11 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.077
[ 2 / 40 ] loss: 0.527
[ 3 / 40 ] loss: 0.328
[ 4 / 40 ] loss: 0.309
[ 5 / 40 ] loss: 0.369
[ 6 / 40 ] loss: 0.415
[ 7 / 40 ] loss: 0.251
[ 8 / 40 ] loss: 0.228
[ 9 / 40 ] loss: 0.100
[ 10 / 40 ] loss: 0.201
[ 11 / 40 ] loss: 0.250
[ 12 / 40 ] loss: 0.195
[ 13 / 40 ] loss: 0.084
[ 14 / 40 ] loss: 0.128
[ 15 / 40 ] loss: 0.198
[ 16 / 40 ] loss: 0.271
[ 17 / 40 ] loss: 0.167
[ 18 / 40 ] loss: 0.184
[ 19 / 40 ] loss: 0.151
[ 20 / 40 ] loss: 0.492
[ 21 / 40 ] loss: 0.183
[ 22 / 40 ] loss: 0.243
[ 23 / 40 ] loss: 0.284
[ 24 / 40 ] loss: 0.181
[ 25 / 40 ] loss: 0.247
[ 26 / 40 ] loss: 0.322
[ 27 / 40 ] loss: 0.318
[ 28 / 40 ] loss: 0.059
[ 29 / 40 ] loss: 0.084
[ 30 / 40 ] loss: 0.146
[ 31 / 40 ] loss: 0.262
[ 32 / 40 ] loss: 0.179
[ 33 / 40 ] loss: 0.233
[ 34 / 40 ] loss: 0.043
[ 35 / 40 ] loss: 0.124
[ 36 / 40 ] loss: 0.175
[ 37 / 40 ] loss: 0.523
[ 38 / 40 ] loss: 0.228
[ 39 / 40 ] loss: 0.338
[ 40 / 40 ] loss: 0.413
0.2376968065276742
Accuracy: 0.759494 -- Precision: 0.755245 -- Recall: 0.830769 -- F1: 0.791209 -- AUC: 0.805536
========= epoch: 12 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.171
[ 2 / 40 ] loss: 0.253
[ 3 / 40 ] loss: 0.089
[ 4 / 40 ] loss: 0.446
[ 5 / 40 ] loss: 0.183
[ 6 / 40 ] loss: 0.194
[ 7 / 40 ] loss: 0.169
[ 8 / 40 ] loss: 0.078
[ 9 / 40 ] loss: 0.276
[ 10 / 40 ] loss: 0.125
[ 11 / 40 ] loss: 0.059
[ 12 / 40 ] loss: 0.107
[ 13 / 40 ] loss: 0.194
[ 14 / 40 ] loss: 0.776
[ 15 / 40 ] loss: 0.026
[ 16 / 40 ] loss: 0.035
[ 17 / 40 ] loss: 0.214
[ 18 / 40 ] loss: 0.208
[ 19 / 40 ] loss: 0.317
[ 20 / 40 ] loss: 0.323
[ 21 / 40 ] loss: 0.166
[ 22 / 40 ] loss: 0.132
[ 23 / 40 ] loss: 0.101
[ 24 / 40 ] loss: 0.194
[ 25 / 40 ] loss: 0.640
[ 26 / 40 ] loss: 0.184
[ 27 / 40 ] loss: 0.236
[ 28 / 40 ] loss: 0.139
[ 29 / 40 ] loss: 0.222
[ 30 / 40 ] loss: 0.179
[ 31 / 40 ] loss: 0.221
[ 32 / 40 ] loss: 0.334
[ 33 / 40 ] loss: 0.240
[ 34 / 40 ] loss: 0.096
[ 35 / 40 ] loss: 0.186
[ 36 / 40 ] loss: 0.348
[ 37 / 40 ] loss: 0.301
[ 38 / 40 ] loss: 0.173
[ 39 / 40 ] loss: 0.349
[ 40 / 40 ] loss: 0.100
0.21964639499783517
Accuracy: 0.746835 -- Precision: 0.736486 -- Recall: 0.838462 -- F1: 0.784173 -- AUC: 0.809058
========= epoch: 13 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.095
[ 2 / 40 ] loss: 0.219
[ 3 / 40 ] loss: 0.279
[ 4 / 40 ] loss: 0.153
[ 5 / 40 ] loss: 0.192
[ 6 / 40 ] loss: 0.255
[ 7 / 40 ] loss: 0.172
[ 8 / 40 ] loss: 0.273
[ 9 / 40 ] loss: 0.267
[ 10 / 40 ] loss: 0.170
[ 11 / 40 ] loss: 0.297
[ 12 / 40 ] loss: 0.218
[ 13 / 40 ] loss: 0.188
[ 14 / 40 ] loss: 0.292
[ 15 / 40 ] loss: 0.244
[ 16 / 40 ] loss: 0.260
[ 17 / 40 ] loss: 0.168
[ 18 / 40 ] loss: 0.400
[ 19 / 40 ] loss: 0.248
[ 20 / 40 ] loss: 0.194
[ 21 / 40 ] loss: 0.283
[ 22 / 40 ] loss: 0.331
[ 23 / 40 ] loss: 0.176
[ 24 / 40 ] loss: 0.044
[ 25 / 40 ] loss: 0.094
[ 26 / 40 ] loss: 0.042
[ 27 / 40 ] loss: 0.072
[ 28 / 40 ] loss: 0.099
[ 29 / 40 ] loss: 0.456
[ 30 / 40 ] loss: 0.041
[ 31 / 40 ] loss: 0.081
[ 32 / 40 ] loss: 0.150
[ 33 / 40 ] loss: 0.521
[ 34 / 40 ] loss: 0.173
[ 35 / 40 ] loss: 0.352
[ 36 / 40 ] loss: 0.207
[ 37 / 40 ] loss: 0.377
[ 38 / 40 ] loss: 0.193
[ 39 / 40 ] loss: 0.298
[ 40 / 40 ] loss: 0.298
0.22184432502835988
Accuracy: 0.759494 -- Precision: 0.782946 -- Recall: 0.776923 -- F1: 0.779923 -- AUC: 0.822142
========= epoch: 14 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.098
[ 2 / 40 ] loss: 0.247
[ 3 / 40 ] loss: 0.358
[ 4 / 40 ] loss: 0.281
[ 5 / 40 ] loss: 0.145
[ 6 / 40 ] loss: 0.079
[ 7 / 40 ] loss: 0.167
[ 8 / 40 ] loss: 0.207
[ 9 / 40 ] loss: 0.072
[ 10 / 40 ] loss: 0.333
[ 11 / 40 ] loss: 0.247
[ 12 / 40 ] loss: 0.094
[ 13 / 40 ] loss: 0.252
[ 14 / 40 ] loss: 0.100
[ 15 / 40 ] loss: 0.170
[ 16 / 40 ] loss: 0.183
[ 17 / 40 ] loss: 0.088
[ 18 / 40 ] loss: 0.210
[ 19 / 40 ] loss: 0.258
[ 20 / 40 ] loss: 0.087
[ 21 / 40 ] loss: 0.061
[ 22 / 40 ] loss: 0.086
[ 23 / 40 ] loss: 0.094
[ 24 / 40 ] loss: 0.560
[ 25 / 40 ] loss: 0.070
[ 26 / 40 ] loss: 0.460
[ 27 / 40 ] loss: 0.048
[ 28 / 40 ] loss: 0.083
[ 29 / 40 ] loss: 0.188
[ 30 / 40 ] loss: 0.455
[ 31 / 40 ] loss: 0.140
[ 32 / 40 ] loss: 0.322
[ 33 / 40 ] loss: 0.334
[ 34 / 40 ] loss: 0.245
[ 35 / 40 ] loss: 0.189
[ 36 / 40 ] loss: 0.220
[ 37 / 40 ] loss: 0.338
[ 38 / 40 ] loss: 0.319
[ 39 / 40 ] loss: 0.215
[ 40 / 40 ] loss: 0.102
0.20514379050582648
Accuracy: 0.767932 -- Precision: 0.762238 -- Recall: 0.838462 -- F1: 0.798535 -- AUC: 0.818188
========= epoch: 15 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.119
[ 2 / 40 ] loss: 0.316
[ 3 / 40 ] loss: 0.066
[ 4 / 40 ] loss: 0.039
[ 5 / 40 ] loss: 0.056
[ 6 / 40 ] loss: 0.085
[ 7 / 40 ] loss: 0.111
[ 8 / 40 ] loss: 0.210
[ 9 / 40 ] loss: 0.120
[ 10 / 40 ] loss: 0.314
[ 11 / 40 ] loss: 0.070
[ 12 / 40 ] loss: 0.223
[ 13 / 40 ] loss: 0.100
[ 14 / 40 ] loss: 0.245
[ 15 / 40 ] loss: 0.225
[ 16 / 40 ] loss: 0.045
[ 17 / 40 ] loss: 0.114
[ 18 / 40 ] loss: 0.303
[ 19 / 40 ] loss: 0.220
[ 20 / 40 ] loss: 0.105
[ 21 / 40 ] loss: 0.078
[ 22 / 40 ] loss: 0.114
[ 23 / 40 ] loss: 0.730
[ 24 / 40 ] loss: 0.104
[ 25 / 40 ] loss: 0.062
[ 26 / 40 ] loss: 0.094
[ 27 / 40 ] loss: 0.126
[ 28 / 40 ] loss: 0.111
[ 29 / 40 ] loss: 0.337
[ 30 / 40 ] loss: 0.256
[ 31 / 40 ] loss: 0.162
[ 32 / 40 ] loss: 0.188
[ 33 / 40 ] loss: 0.179
[ 34 / 40 ] loss: 0.529
[ 35 / 40 ] loss: 0.431
[ 36 / 40 ] loss: 0.111
[ 37 / 40 ] loss: 0.073
[ 38 / 40 ] loss: 0.062
[ 39 / 40 ] loss: 0.102
[ 40 / 40 ] loss: 0.601
0.18845376903191208
Accuracy: 0.780591 -- Precision: 0.804688 -- Recall: 0.792308 -- F1: 0.798450 -- AUC: 0.826096
========= epoch: 16 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.082
[ 2 / 40 ] loss: 0.444
[ 3 / 40 ] loss: 0.070
[ 4 / 40 ] loss: 0.112
[ 5 / 40 ] loss: 0.034
[ 6 / 40 ] loss: 0.220
[ 7 / 40 ] loss: 0.378
[ 8 / 40 ] loss: 0.387
[ 9 / 40 ] loss: 0.055
[ 10 / 40 ] loss: 0.077
[ 11 / 40 ] loss: 0.294
[ 12 / 40 ] loss: 0.243
[ 13 / 40 ] loss: 0.120
[ 14 / 40 ] loss: 0.084
[ 15 / 40 ] loss: 0.042
[ 16 / 40 ] loss: 0.306
[ 17 / 40 ] loss: 0.099
[ 18 / 40 ] loss: 0.280
[ 19 / 40 ] loss: 0.157
[ 20 / 40 ] loss: 0.047
[ 21 / 40 ] loss: 0.187
[ 22 / 40 ] loss: 0.214
[ 23 / 40 ] loss: 0.152
[ 24 / 40 ] loss: 0.150
[ 25 / 40 ] loss: 0.378
[ 26 / 40 ] loss: 0.193
[ 27 / 40 ] loss: 0.205
[ 28 / 40 ] loss: 0.089
[ 29 / 40 ] loss: 0.066
[ 30 / 40 ] loss: 0.190
[ 31 / 40 ] loss: 0.245
[ 32 / 40 ] loss: 0.182
[ 33 / 40 ] loss: 0.110
[ 34 / 40 ] loss: 0.350
[ 35 / 40 ] loss: 0.104
[ 36 / 40 ] loss: 0.112
[ 37 / 40 ] loss: 0.227
[ 38 / 40 ] loss: 0.064
[ 39 / 40 ] loss: 0.066
[ 40 / 40 ] loss: 0.126
0.173503821156919
Accuracy: 0.780591 -- Precision: 0.782609 -- Recall: 0.830769 -- F1: 0.805970 -- AUC: 0.825665
========= epoch: 17 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.141
[ 2 / 40 ] loss: 0.059
[ 3 / 40 ] loss: 0.048
[ 4 / 40 ] loss: 0.067
[ 5 / 40 ] loss: 0.065
[ 6 / 40 ] loss: 0.045
[ 7 / 40 ] loss: 0.054
[ 8 / 40 ] loss: 0.101
[ 9 / 40 ] loss: 0.048
[ 10 / 40 ] loss: 0.066
[ 11 / 40 ] loss: 0.175
[ 12 / 40 ] loss: 0.022
[ 13 / 40 ] loss: 0.532
[ 14 / 40 ] loss: 0.014
[ 15 / 40 ] loss: 0.082
[ 16 / 40 ] loss: 0.113
[ 17 / 40 ] loss: 0.062
[ 18 / 40 ] loss: 0.041
[ 19 / 40 ] loss: 0.034
[ 20 / 40 ] loss: 0.072
[ 21 / 40 ] loss: 0.113
[ 22 / 40 ] loss: 0.109
[ 23 / 40 ] loss: 0.332
[ 24 / 40 ] loss: 0.202
[ 25 / 40 ] loss: 0.195
[ 26 / 40 ] loss: 0.019
[ 27 / 40 ] loss: 0.356
[ 28 / 40 ] loss: 0.051
[ 29 / 40 ] loss: 0.028
[ 30 / 40 ] loss: 0.107
[ 31 / 40 ] loss: 0.086
[ 32 / 40 ] loss: 0.122
[ 33 / 40 ] loss: 0.199
[ 34 / 40 ] loss: 0.230
[ 35 / 40 ] loss: 0.136
[ 36 / 40 ] loss: 0.115
[ 37 / 40 ] loss: 0.143
[ 38 / 40 ] loss: 0.207
[ 39 / 40 ] loss: 0.093
[ 40 / 40 ] loss: 0.039
0.11803627361077815
Accuracy: 0.784810 -- Precision: 0.792593 -- Recall: 0.823077 -- F1: 0.807547 -- AUC: 0.813156
========= epoch: 18 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.041
[ 2 / 40 ] loss: 0.144
[ 3 / 40 ] loss: 0.177
[ 4 / 40 ] loss: 0.113
[ 5 / 40 ] loss: 0.110
[ 6 / 40 ] loss: 0.032
[ 7 / 40 ] loss: 0.166
[ 8 / 40 ] loss: 0.074
[ 9 / 40 ] loss: 0.046
[ 10 / 40 ] loss: 0.022
[ 11 / 40 ] loss: 0.088
[ 12 / 40 ] loss: 0.065
[ 13 / 40 ] loss: 0.043
[ 14 / 40 ] loss: 0.153
[ 15 / 40 ] loss: 0.241
[ 16 / 40 ] loss: 0.028
[ 17 / 40 ] loss: 0.017
[ 18 / 40 ] loss: 0.282
[ 19 / 40 ] loss: 0.051
[ 20 / 40 ] loss: 0.040
[ 21 / 40 ] loss: 0.149
[ 22 / 40 ] loss: 0.104
[ 23 / 40 ] loss: 0.178
[ 24 / 40 ] loss: 0.014
[ 25 / 40 ] loss: 0.257
[ 26 / 40 ] loss: 0.258
[ 27 / 40 ] loss: 0.114
[ 28 / 40 ] loss: 0.166
[ 29 / 40 ] loss: 0.122
[ 30 / 40 ] loss: 0.216
[ 31 / 40 ] loss: 0.097
[ 32 / 40 ] loss: 0.063
[ 33 / 40 ] loss: 0.545
[ 34 / 40 ] loss: 0.478
[ 35 / 40 ] loss: 0.232
[ 36 / 40 ] loss: 0.150
[ 37 / 40 ] loss: 0.572
[ 38 / 40 ] loss: 0.319
[ 39 / 40 ] loss: 0.215
[ 40 / 40 ] loss: 0.233
0.16035095541737973
Accuracy: 0.751055 -- Precision: 0.766917 -- Recall: 0.784615 -- F1: 0.775665 -- AUC: 0.780661
========= epoch: 19 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.245
[ 2 / 40 ] loss: 0.397
[ 3 / 40 ] loss: 0.079
[ 4 / 40 ] loss: 0.224
[ 5 / 40 ] loss: 0.136
[ 6 / 40 ] loss: 0.069
[ 7 / 40 ] loss: 0.503
[ 8 / 40 ] loss: 0.201
[ 9 / 40 ] loss: 0.345
[ 10 / 40 ] loss: 0.251
[ 11 / 40 ] loss: 0.280
[ 12 / 40 ] loss: 0.109
[ 13 / 40 ] loss: 0.188
[ 14 / 40 ] loss: 0.256
[ 15 / 40 ] loss: 0.362
[ 16 / 40 ] loss: 0.201
[ 17 / 40 ] loss: 0.168
[ 18 / 40 ] loss: 0.218
[ 19 / 40 ] loss: 0.109
[ 20 / 40 ] loss: 0.220
[ 21 / 40 ] loss: 0.051
[ 22 / 40 ] loss: 0.090
[ 23 / 40 ] loss: 0.108
[ 24 / 40 ] loss: 0.207
[ 25 / 40 ] loss: 0.230
[ 26 / 40 ] loss: 0.210
[ 27 / 40 ] loss: 0.457
[ 28 / 40 ] loss: 0.292
[ 29 / 40 ] loss: 0.250
[ 30 / 40 ] loss: 0.316
[ 31 / 40 ] loss: 0.085
[ 32 / 40 ] loss: 0.202
[ 33 / 40 ] loss: 0.273
[ 34 / 40 ] loss: 0.177
[ 35 / 40 ] loss: 0.304
[ 36 / 40 ] loss: 0.129
[ 37 / 40 ] loss: 0.254
[ 38 / 40 ] loss: 0.153
[ 39 / 40 ] loss: 0.105
[ 40 / 40 ] loss: 0.530
0.2245947782881558
Accuracy: 0.751055 -- Precision: 0.808696 -- Recall: 0.715385 -- F1: 0.759184 -- AUC: 0.797052
========= epoch: 20 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.119
[ 2 / 40 ] loss: 0.256
[ 3 / 40 ] loss: 0.308
[ 4 / 40 ] loss: 0.299
[ 5 / 40 ] loss: 0.207
[ 6 / 40 ] loss: 0.132
[ 7 / 40 ] loss: 0.316
[ 8 / 40 ] loss: 0.343
[ 9 / 40 ] loss: 0.166
[ 10 / 40 ] loss: 0.253
[ 11 / 40 ] loss: 0.161
[ 12 / 40 ] loss: 0.103
[ 13 / 40 ] loss: 0.257
[ 14 / 40 ] loss: 0.219
[ 15 / 40 ] loss: 0.125
[ 16 / 40 ] loss: 0.170
[ 17 / 40 ] loss: 0.088
[ 18 / 40 ] loss: 0.271
[ 19 / 40 ] loss: 0.122
[ 20 / 40 ] loss: 0.284
[ 21 / 40 ] loss: 0.389
[ 22 / 40 ] loss: 0.079
[ 23 / 40 ] loss: 0.241
[ 24 / 40 ] loss: 0.175
[ 25 / 40 ] loss: 0.231
[ 26 / 40 ] loss: 0.425
[ 27 / 40 ] loss: 0.363
[ 28 / 40 ] loss: 0.468
[ 29 / 40 ] loss: 0.167
[ 30 / 40 ] loss: 0.166
[ 31 / 40 ] loss: 0.302
[ 32 / 40 ] loss: 0.118
[ 33 / 40 ] loss: 0.118
[ 34 / 40 ] loss: 0.198
[ 35 / 40 ] loss: 0.061
[ 36 / 40 ] loss: 0.163
[ 37 / 40 ] loss: 0.182
[ 38 / 40 ] loss: 0.212
[ 39 / 40 ] loss: 0.059
[ 40 / 40 ] loss: 0.436
0.21883723679929973
Accuracy: 0.759494 -- Precision: 0.782946 -- Recall: 0.776923 -- F1: 0.779923 -- AUC: 0.798275
========= epoch: 21 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.162
[ 2 / 40 ] loss: 0.170
[ 3 / 40 ] loss: 0.148
[ 4 / 40 ] loss: 0.026
[ 5 / 40 ] loss: 0.309
[ 6 / 40 ] loss: 0.061
[ 7 / 40 ] loss: 0.257
[ 8 / 40 ] loss: 0.057
[ 9 / 40 ] loss: 0.028
[ 10 / 40 ] loss: 0.166
[ 11 / 40 ] loss: 0.150
[ 12 / 40 ] loss: 0.217
[ 13 / 40 ] loss: 0.201
[ 14 / 40 ] loss: 0.238
[ 15 / 40 ] loss: 0.111
[ 16 / 40 ] loss: 0.073
[ 17 / 40 ] loss: 0.167
[ 18 / 40 ] loss: 0.020
[ 19 / 40 ] loss: 0.043
[ 20 / 40 ] loss: 0.258
[ 21 / 40 ] loss: 0.094
[ 22 / 40 ] loss: 0.170
[ 23 / 40 ] loss: 0.156
[ 24 / 40 ] loss: 0.131
[ 25 / 40 ] loss: 0.260
[ 26 / 40 ] loss: 0.119
[ 27 / 40 ] loss: 0.070
[ 28 / 40 ] loss: 0.214
[ 29 / 40 ] loss: 0.101
[ 30 / 40 ] loss: 0.103
[ 31 / 40 ] loss: 0.309
[ 32 / 40 ] loss: 0.424
[ 33 / 40 ] loss: 0.386
[ 34 / 40 ] loss: 0.089
[ 35 / 40 ] loss: 0.088
[ 36 / 40 ] loss: 0.021
[ 37 / 40 ] loss: 0.226
[ 38 / 40 ] loss: 0.199
[ 39 / 40 ] loss: 0.108
[ 40 / 40 ] loss: 0.058
0.15476335920393466
Accuracy: 0.738397 -- Precision: 0.709877 -- Recall: 0.884615 -- F1: 0.787671 -- AUC: 0.764055
========= epoch: 22 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.256
[ 2 / 40 ] loss: 0.212
[ 3 / 40 ] loss: 0.048
[ 4 / 40 ] loss: 0.199
[ 5 / 40 ] loss: 0.360
[ 6 / 40 ] loss: 0.114
[ 7 / 40 ] loss: 0.032
[ 8 / 40 ] loss: 0.140
[ 9 / 40 ] loss: 0.115
[ 10 / 40 ] loss: 0.129
[ 11 / 40 ] loss: 0.203
[ 12 / 40 ] loss: 0.030
[ 13 / 40 ] loss: 0.099
[ 14 / 40 ] loss: 0.442
[ 15 / 40 ] loss: 0.098
[ 16 / 40 ] loss: 0.048
[ 17 / 40 ] loss: 0.224
[ 18 / 40 ] loss: 0.124
[ 19 / 40 ] loss: 0.066
[ 20 / 40 ] loss: 0.037
[ 21 / 40 ] loss: 0.064
[ 22 / 40 ] loss: 0.092
[ 23 / 40 ] loss: 0.182
[ 24 / 40 ] loss: 0.083
[ 25 / 40 ] loss: 0.056
[ 26 / 40 ] loss: 0.203
[ 27 / 40 ] loss: 0.078
[ 28 / 40 ] loss: 0.028
[ 29 / 40 ] loss: 0.173
[ 30 / 40 ] loss: 0.204
[ 31 / 40 ] loss: 0.047
[ 32 / 40 ] loss: 0.042
[ 33 / 40 ] loss: 0.164
[ 34 / 40 ] loss: 0.035
[ 35 / 40 ] loss: 0.038
[ 36 / 40 ] loss: 0.070
[ 37 / 40 ] loss: 0.316
[ 38 / 40 ] loss: 0.068
[ 39 / 40 ] loss: 0.209
[ 40 / 40 ] loss: 0.385
0.13783860560506583
Accuracy: 0.767932 -- Precision: 0.804878 -- Recall: 0.761538 -- F1: 0.782609 -- AUC: 0.803307
========= epoch: 23 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.342
[ 2 / 40 ] loss: 0.061
[ 3 / 40 ] loss: 0.135
[ 4 / 40 ] loss: 0.095
[ 5 / 40 ] loss: 0.105
[ 6 / 40 ] loss: 0.107
[ 7 / 40 ] loss: 0.053
[ 8 / 40 ] loss: 0.058
[ 9 / 40 ] loss: 0.095
[ 10 / 40 ] loss: 0.054
[ 11 / 40 ] loss: 0.113
[ 12 / 40 ] loss: 0.132
[ 13 / 40 ] loss: 0.065
[ 14 / 40 ] loss: 0.057
[ 15 / 40 ] loss: 0.336
[ 16 / 40 ] loss: 0.152
[ 17 / 40 ] loss: 0.068
[ 18 / 40 ] loss: 0.046
[ 19 / 40 ] loss: 0.026
[ 20 / 40 ] loss: 0.059
[ 21 / 40 ] loss: 0.062
[ 22 / 40 ] loss: 0.249
[ 23 / 40 ] loss: 0.030
[ 24 / 40 ] loss: 0.063
[ 25 / 40 ] loss: 0.042
[ 26 / 40 ] loss: 0.245
[ 27 / 40 ] loss: 0.146
[ 28 / 40 ] loss: 0.156
[ 29 / 40 ] loss: 0.108
[ 30 / 40 ] loss: 0.051
[ 31 / 40 ] loss: 0.098
[ 32 / 40 ] loss: 0.070
[ 33 / 40 ] loss: 0.322
[ 34 / 40 ] loss: 0.099
[ 35 / 40 ] loss: 0.170
[ 36 / 40 ] loss: 0.101
[ 37 / 40 ] loss: 0.045
[ 38 / 40 ] loss: 0.336
[ 39 / 40 ] loss: 0.069
[ 40 / 40 ] loss: 0.037
0.11651217080652713
Accuracy: 0.734177 -- Precision: 0.819048 -- Recall: 0.661538 -- F1: 0.731915 -- AUC: 0.793027
========= epoch: 24 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.389
[ 2 / 40 ] loss: 0.258
[ 3 / 40 ] loss: 0.158
[ 4 / 40 ] loss: 0.026
[ 5 / 40 ] loss: 0.261
[ 6 / 40 ] loss: 0.073
[ 7 / 40 ] loss: 0.217
[ 8 / 40 ] loss: 0.181
[ 9 / 40 ] loss: 0.103
[ 10 / 40 ] loss: 0.088
[ 11 / 40 ] loss: 0.161
[ 12 / 40 ] loss: 0.248
[ 13 / 40 ] loss: 0.031
[ 14 / 40 ] loss: 0.039
[ 15 / 40 ] loss: 0.039
[ 16 / 40 ] loss: 0.071
[ 17 / 40 ] loss: 0.047
[ 18 / 40 ] loss: 0.200
[ 19 / 40 ] loss: 0.231
[ 20 / 40 ] loss: 0.058
[ 21 / 40 ] loss: 0.021
[ 22 / 40 ] loss: 0.087
[ 23 / 40 ] loss: 0.131
[ 24 / 40 ] loss: 0.047
[ 25 / 40 ] loss: 0.241
[ 26 / 40 ] loss: 0.100
[ 27 / 40 ] loss: 0.258
[ 28 / 40 ] loss: 0.253
[ 29 / 40 ] loss: 0.095
[ 30 / 40 ] loss: 0.048
[ 31 / 40 ] loss: 0.023
[ 32 / 40 ] loss: 0.056
[ 33 / 40 ] loss: 0.109
[ 34 / 40 ] loss: 0.076
[ 35 / 40 ] loss: 0.036
[ 36 / 40 ] loss: 0.137
[ 37 / 40 ] loss: 0.246
[ 38 / 40 ] loss: 0.024
[ 39 / 40 ] loss: 0.109
[ 40 / 40 ] loss: 0.029
0.12521486096084117
Accuracy: 0.780591 -- Precision: 0.791045 -- Recall: 0.815385 -- F1: 0.803030 -- AUC: 0.805895
========= epoch: 25 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.036
[ 2 / 40 ] loss: 0.075
[ 3 / 40 ] loss: 0.076
[ 4 / 40 ] loss: 0.044
[ 5 / 40 ] loss: 0.093
[ 6 / 40 ] loss: 0.094
[ 7 / 40 ] loss: 0.069
[ 8 / 40 ] loss: 0.048
[ 9 / 40 ] loss: 0.213
[ 10 / 40 ] loss: 0.112
[ 11 / 40 ] loss: 0.230
[ 12 / 40 ] loss: 0.011
[ 13 / 40 ] loss: 0.025
[ 14 / 40 ] loss: 0.015
[ 15 / 40 ] loss: 0.145
[ 16 / 40 ] loss: 0.221
[ 17 / 40 ] loss: 0.292
[ 18 / 40 ] loss: 0.044
[ 19 / 40 ] loss: 0.055
[ 20 / 40 ] loss: 0.029
[ 21 / 40 ] loss: 0.029
[ 22 / 40 ] loss: 0.151
[ 23 / 40 ] loss: 0.012
[ 24 / 40 ] loss: 0.038
[ 25 / 40 ] loss: 0.072
[ 26 / 40 ] loss: 0.048
[ 27 / 40 ] loss: 0.226
[ 28 / 40 ] loss: 0.098
[ 29 / 40 ] loss: 0.222
[ 30 / 40 ] loss: 0.610
[ 31 / 40 ] loss: 0.297
[ 32 / 40 ] loss: 0.035
[ 33 / 40 ] loss: 0.123
[ 34 / 40 ] loss: 0.174
[ 35 / 40 ] loss: 0.034
[ 36 / 40 ] loss: 0.205
[ 37 / 40 ] loss: 0.140
[ 38 / 40 ] loss: 0.038
[ 39 / 40 ] loss: 0.074
[ 40 / 40 ] loss: 0.221
0.11942474639508874
Accuracy: 0.713080 -- Precision: 0.678161 -- Recall: 0.907692 -- F1: 0.776316 -- AUC: 0.739971
========= epoch: 26 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.128
[ 2 / 40 ] loss: 0.088
[ 3 / 40 ] loss: 0.044
[ 4 / 40 ] loss: 0.513
[ 5 / 40 ] loss: 0.178
[ 6 / 40 ] loss: 0.294
[ 7 / 40 ] loss: 0.300
[ 8 / 40 ] loss: 0.182
[ 9 / 40 ] loss: 0.248
[ 10 / 40 ] loss: 0.090
[ 11 / 40 ] loss: 0.091
[ 12 / 40 ] loss: 0.054
[ 13 / 40 ] loss: 0.083
[ 14 / 40 ] loss: 0.142
[ 15 / 40 ] loss: 0.084
[ 16 / 40 ] loss: 0.205
[ 17 / 40 ] loss: 0.163
[ 18 / 40 ] loss: 0.137
[ 19 / 40 ] loss: 0.221
[ 20 / 40 ] loss: 0.212
[ 21 / 40 ] loss: 0.086
[ 22 / 40 ] loss: 0.511
[ 23 / 40 ] loss: 0.216
[ 24 / 40 ] loss: 0.026
[ 25 / 40 ] loss: 0.077
[ 26 / 40 ] loss: 0.019
[ 27 / 40 ] loss: 0.019
[ 28 / 40 ] loss: 0.210
[ 29 / 40 ] loss: 0.034
[ 30 / 40 ] loss: 0.099
[ 31 / 40 ] loss: 0.059
[ 32 / 40 ] loss: 0.073
[ 33 / 40 ] loss: 0.175
[ 34 / 40 ] loss: 0.080
[ 35 / 40 ] loss: 0.129
[ 36 / 40 ] loss: 0.199
[ 37 / 40 ] loss: 0.086
[ 38 / 40 ] loss: 0.244
[ 39 / 40 ] loss: 0.016
[ 40 / 40 ] loss: 0.250
0.15161438290961088
Accuracy: 0.746835 -- Precision: 0.761194 -- Recall: 0.784615 -- F1: 0.772727 -- AUC: 0.797556
========= epoch: 27 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.029
[ 2 / 40 ] loss: 0.017
[ 3 / 40 ] loss: 0.045
[ 4 / 40 ] loss: 0.054
[ 5 / 40 ] loss: 0.202
[ 6 / 40 ] loss: 0.134
[ 7 / 40 ] loss: 0.010
[ 8 / 40 ] loss: 0.210
[ 9 / 40 ] loss: 0.069
[ 10 / 40 ] loss: 0.151
[ 11 / 40 ] loss: 0.012
[ 12 / 40 ] loss: 0.202
[ 13 / 40 ] loss: 0.029
[ 14 / 40 ] loss: 0.020
[ 15 / 40 ] loss: 0.042
[ 16 / 40 ] loss: 0.208
[ 17 / 40 ] loss: 0.021
[ 18 / 40 ] loss: 0.023
[ 19 / 40 ] loss: 0.067
[ 20 / 40 ] loss: 0.113
[ 21 / 40 ] loss: 0.023
[ 22 / 40 ] loss: 0.018
[ 23 / 40 ] loss: 0.022
[ 24 / 40 ] loss: 0.140
[ 25 / 40 ] loss: 0.068
[ 26 / 40 ] loss: 0.081
[ 27 / 40 ] loss: 0.026
[ 28 / 40 ] loss: 0.076
[ 29 / 40 ] loss: 0.036
[ 30 / 40 ] loss: 0.105
[ 31 / 40 ] loss: 0.115
[ 32 / 40 ] loss: 0.028
[ 33 / 40 ] loss: 0.018
[ 34 / 40 ] loss: 0.211
[ 35 / 40 ] loss: 0.042
[ 36 / 40 ] loss: 0.143
[ 37 / 40 ] loss: 0.044
[ 38 / 40 ] loss: 0.028
[ 39 / 40 ] loss: 0.041
[ 40 / 40 ] loss: 0.210
0.07828798503614962
Accuracy: 0.738397 -- Precision: 0.750000 -- Recall: 0.784615 -- F1: 0.766917 -- AUC: 0.805572
========= epoch: 28 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.015
[ 2 / 40 ] loss: 0.049
[ 3 / 40 ] loss: 0.299
[ 4 / 40 ] loss: 0.027
[ 5 / 40 ] loss: 0.212
[ 6 / 40 ] loss: 0.187
[ 7 / 40 ] loss: 0.090
[ 8 / 40 ] loss: 0.011
[ 9 / 40 ] loss: 0.159
[ 10 / 40 ] loss: 0.169
[ 11 / 40 ] loss: 0.092
[ 12 / 40 ] loss: 0.026
[ 13 / 40 ] loss: 0.218
[ 14 / 40 ] loss: 0.029
[ 15 / 40 ] loss: 0.016
[ 16 / 40 ] loss: 0.094
[ 17 / 40 ] loss: 0.055
[ 18 / 40 ] loss: 0.114
[ 19 / 40 ] loss: 0.086
[ 20 / 40 ] loss: 0.046
[ 21 / 40 ] loss: 0.031
[ 22 / 40 ] loss: 0.098
[ 23 / 40 ] loss: 0.084
[ 24 / 40 ] loss: 0.033
[ 25 / 40 ] loss: 0.037
[ 26 / 40 ] loss: 0.054
[ 27 / 40 ] loss: 0.051
[ 28 / 40 ] loss: 0.013
[ 29 / 40 ] loss: 0.207
[ 30 / 40 ] loss: 0.192
[ 31 / 40 ] loss: 0.071
[ 32 / 40 ] loss: 0.023
[ 33 / 40 ] loss: 0.105
[ 34 / 40 ] loss: 0.386
[ 35 / 40 ] loss: 0.211
[ 36 / 40 ] loss: 0.013
[ 37 / 40 ] loss: 0.110
[ 38 / 40 ] loss: 0.036
[ 39 / 40 ] loss: 0.009
[ 40 / 40 ] loss: 0.090
0.09623205026146024
Accuracy: 0.738397 -- Precision: 0.729730 -- Recall: 0.830769 -- F1: 0.776978 -- AUC: 0.794033
========= epoch: 29 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.081
[ 2 / 40 ] loss: 0.031
[ 3 / 40 ] loss: 0.216
[ 4 / 40 ] loss: 0.079
[ 5 / 40 ] loss: 0.346
[ 6 / 40 ] loss: 0.028
[ 7 / 40 ] loss: 0.014
[ 8 / 40 ] loss: 0.032
[ 9 / 40 ] loss: 0.020
[ 10 / 40 ] loss: 0.164
[ 11 / 40 ] loss: 0.015
[ 12 / 40 ] loss: 0.016
[ 13 / 40 ] loss: 0.021
[ 14 / 40 ] loss: 0.296
[ 15 / 40 ] loss: 0.048
[ 16 / 40 ] loss: 0.064
[ 17 / 40 ] loss: 0.192
[ 18 / 40 ] loss: 0.023
[ 19 / 40 ] loss: 0.123
[ 20 / 40 ] loss: 0.080
[ 21 / 40 ] loss: 0.074
[ 22 / 40 ] loss: 0.093
[ 23 / 40 ] loss: 0.059
[ 24 / 40 ] loss: 0.045
[ 25 / 40 ] loss: 0.066
[ 26 / 40 ] loss: 0.036
[ 27 / 40 ] loss: 0.387
[ 28 / 40 ] loss: 0.023
[ 29 / 40 ] loss: 0.161
[ 30 / 40 ] loss: 0.039
[ 31 / 40 ] loss: 0.106
[ 32 / 40 ] loss: 0.089
[ 33 / 40 ] loss: 0.133
[ 34 / 40 ] loss: 0.239
[ 35 / 40 ] loss: 0.023
[ 36 / 40 ] loss: 0.031
[ 37 / 40 ] loss: 0.016
[ 38 / 40 ] loss: 0.098
[ 39 / 40 ] loss: 0.130
[ 40 / 40 ] loss: 0.008
0.09365328714484349
Accuracy: 0.767932 -- Precision: 0.781955 -- Recall: 0.800000 -- F1: 0.790875 -- AUC: 0.791157
========= epoch: 30 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.027
[ 2 / 40 ] loss: 0.015
[ 3 / 40 ] loss: 0.009
[ 4 / 40 ] loss: 0.019
[ 5 / 40 ] loss: 0.112
[ 6 / 40 ] loss: 0.027
[ 7 / 40 ] loss: 0.032
[ 8 / 40 ] loss: 0.022
[ 9 / 40 ] loss: 0.091
[ 10 / 40 ] loss: 0.011
[ 11 / 40 ] loss: 0.029
[ 12 / 40 ] loss: 0.008
[ 13 / 40 ] loss: 0.016
[ 14 / 40 ] loss: 0.024
[ 15 / 40 ] loss: 0.013
[ 16 / 40 ] loss: 0.026
[ 17 / 40 ] loss: 0.029
[ 18 / 40 ] loss: 0.019
[ 19 / 40 ] loss: 0.056
[ 20 / 40 ] loss: 0.011
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.134
[ 23 / 40 ] loss: 0.006
[ 24 / 40 ] loss: 0.018
[ 25 / 40 ] loss: 0.022
[ 26 / 40 ] loss: 0.009
[ 27 / 40 ] loss: 0.179
[ 28 / 40 ] loss: 0.025
[ 29 / 40 ] loss: 0.061
[ 30 / 40 ] loss: 0.422
[ 31 / 40 ] loss: 0.105
[ 32 / 40 ] loss: 0.158
[ 33 / 40 ] loss: 0.009
[ 34 / 40 ] loss: 0.021
[ 35 / 40 ] loss: 0.009
[ 36 / 40 ] loss: 0.232
[ 37 / 40 ] loss: 0.466
[ 38 / 40 ] loss: 0.019
[ 39 / 40 ] loss: 0.019
[ 40 / 40 ] loss: 0.297
0.07046640291810036
Accuracy: 0.746835 -- Precision: 0.750000 -- Recall: 0.807692 -- F1: 0.777778 -- AUC: 0.785334
========= epoch: 31 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.030
[ 2 / 40 ] loss: 0.115
[ 3 / 40 ] loss: 0.018
[ 4 / 40 ] loss: 0.052
[ 5 / 40 ] loss: 0.155
[ 6 / 40 ] loss: 0.240
[ 7 / 40 ] loss: 0.046
[ 8 / 40 ] loss: 0.011
[ 9 / 40 ] loss: 0.069
[ 10 / 40 ] loss: 0.014
[ 11 / 40 ] loss: 0.033
[ 12 / 40 ] loss: 0.031
[ 13 / 40 ] loss: 0.042
[ 14 / 40 ] loss: 0.108
[ 15 / 40 ] loss: 0.095
[ 16 / 40 ] loss: 0.024
[ 17 / 40 ] loss: 0.088
[ 18 / 40 ] loss: 0.250
[ 19 / 40 ] loss: 0.125
[ 20 / 40 ] loss: 0.048
[ 21 / 40 ] loss: 0.009
[ 22 / 40 ] loss: 0.074
[ 23 / 40 ] loss: 0.055
[ 24 / 40 ] loss: 0.229
[ 25 / 40 ] loss: 0.123
[ 26 / 40 ] loss: 0.009
[ 27 / 40 ] loss: 0.050
[ 28 / 40 ] loss: 0.130
[ 29 / 40 ] loss: 0.129
[ 30 / 40 ] loss: 0.323
[ 31 / 40 ] loss: 0.084
[ 32 / 40 ] loss: 0.235
[ 33 / 40 ] loss: 0.091
[ 34 / 40 ] loss: 0.086
[ 35 / 40 ] loss: 0.079
[ 36 / 40 ] loss: 0.044
[ 37 / 40 ] loss: 0.159
[ 38 / 40 ] loss: 0.073
[ 39 / 40 ] loss: 0.069
[ 40 / 40 ] loss: 0.113
0.09389449078589678
Accuracy: 0.729958 -- Precision: 0.739130 -- Recall: 0.784615 -- F1: 0.761194 -- AUC: 0.762329
========= epoch: 32 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.031
[ 2 / 40 ] loss: 0.190
[ 3 / 40 ] loss: 0.109
[ 4 / 40 ] loss: 0.039
[ 5 / 40 ] loss: 0.010
[ 6 / 40 ] loss: 0.030
[ 7 / 40 ] loss: 0.031
[ 8 / 40 ] loss: 0.022
[ 9 / 40 ] loss: 0.120
[ 10 / 40 ] loss: 0.078
[ 11 / 40 ] loss: 0.044
[ 12 / 40 ] loss: 0.014
[ 13 / 40 ] loss: 0.032
[ 14 / 40 ] loss: 0.025
[ 15 / 40 ] loss: 0.230
[ 16 / 40 ] loss: 0.035
[ 17 / 40 ] loss: 0.205
[ 18 / 40 ] loss: 0.046
[ 19 / 40 ] loss: 0.023
[ 20 / 40 ] loss: 0.021
[ 21 / 40 ] loss: 0.028
[ 22 / 40 ] loss: 0.094
[ 23 / 40 ] loss: 0.024
[ 24 / 40 ] loss: 0.083
[ 25 / 40 ] loss: 0.035
[ 26 / 40 ] loss: 0.179
[ 27 / 40 ] loss: 0.006
[ 28 / 40 ] loss: 0.171
[ 29 / 40 ] loss: 0.031
[ 30 / 40 ] loss: 0.184
[ 31 / 40 ] loss: 0.013
[ 32 / 40 ] loss: 0.008
[ 33 / 40 ] loss: 0.101
[ 34 / 40 ] loss: 0.237
[ 35 / 40 ] loss: 0.038
[ 36 / 40 ] loss: 0.075
[ 37 / 40 ] loss: 0.017
[ 38 / 40 ] loss: 0.019
[ 39 / 40 ] loss: 0.006
[ 40 / 40 ] loss: 0.027
0.06770187717629597
Accuracy: 0.746835 -- Precision: 0.761194 -- Recall: 0.784615 -- F1: 0.772727 -- AUC: 0.795327
========= epoch: 33 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.075
[ 2 / 40 ] loss: 0.050
[ 3 / 40 ] loss: 0.031
[ 4 / 40 ] loss: 0.047
[ 5 / 40 ] loss: 0.087
[ 6 / 40 ] loss: 0.006
[ 7 / 40 ] loss: 0.019
[ 8 / 40 ] loss: 0.022
[ 9 / 40 ] loss: 0.015
[ 10 / 40 ] loss: 0.009
[ 11 / 40 ] loss: 0.010
[ 12 / 40 ] loss: 0.021
[ 13 / 40 ] loss: 0.021
[ 14 / 40 ] loss: 0.007
[ 15 / 40 ] loss: 0.096
[ 16 / 40 ] loss: 0.110
[ 17 / 40 ] loss: 0.029
[ 18 / 40 ] loss: 0.042
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.024
[ 21 / 40 ] loss: 0.033
[ 22 / 40 ] loss: 0.006
[ 23 / 40 ] loss: 0.007
[ 24 / 40 ] loss: 0.058
[ 25 / 40 ] loss: 0.197
[ 26 / 40 ] loss: 0.097
[ 27 / 40 ] loss: 0.006
[ 28 / 40 ] loss: 0.105
[ 29 / 40 ] loss: 0.040
[ 30 / 40 ] loss: 0.054
[ 31 / 40 ] loss: 0.032
[ 32 / 40 ] loss: 0.024
[ 33 / 40 ] loss: 0.011
[ 34 / 40 ] loss: 0.108
[ 35 / 40 ] loss: 0.264
[ 36 / 40 ] loss: 0.095
[ 37 / 40 ] loss: 0.004
[ 38 / 40 ] loss: 0.019
[ 39 / 40 ] loss: 0.015
[ 40 / 40 ] loss: 0.006
0.04781048065051437
Accuracy: 0.767932 -- Precision: 0.777778 -- Recall: 0.807692 -- F1: 0.792453 -- AUC: 0.797628
========= epoch: 34 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.010
[ 2 / 40 ] loss: 0.021
[ 3 / 40 ] loss: 0.223
[ 4 / 40 ] loss: 0.057
[ 5 / 40 ] loss: 0.028
[ 6 / 40 ] loss: 0.005
[ 7 / 40 ] loss: 0.121
[ 8 / 40 ] loss: 0.011
[ 9 / 40 ] loss: 0.112
[ 10 / 40 ] loss: 0.034
[ 11 / 40 ] loss: 0.007
[ 12 / 40 ] loss: 0.045
[ 13 / 40 ] loss: 0.004
[ 14 / 40 ] loss: 0.023
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.008
[ 17 / 40 ] loss: 0.102
[ 18 / 40 ] loss: 0.098
[ 19 / 40 ] loss: 0.016
[ 20 / 40 ] loss: 0.026
[ 21 / 40 ] loss: 0.027
[ 22 / 40 ] loss: 0.023
[ 23 / 40 ] loss: 0.075
[ 24 / 40 ] loss: 0.007
[ 25 / 40 ] loss: 0.017
[ 26 / 40 ] loss: 0.020
[ 27 / 40 ] loss: 0.021
[ 28 / 40 ] loss: 0.020
[ 29 / 40 ] loss: 0.008
[ 30 / 40 ] loss: 0.014
[ 31 / 40 ] loss: 0.039
[ 32 / 40 ] loss: 0.015
[ 33 / 40 ] loss: 0.024
[ 34 / 40 ] loss: 0.011
[ 35 / 40 ] loss: 0.022
[ 36 / 40 ] loss: 0.016
[ 37 / 40 ] loss: 0.006
[ 38 / 40 ] loss: 0.405
[ 39 / 40 ] loss: 0.013
[ 40 / 40 ] loss: 0.050
0.04484440385131165
Accuracy: 0.755274 -- Precision: 0.768657 -- Recall: 0.792308 -- F1: 0.780303 -- AUC: 0.804170
========= epoch: 35 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.008
[ 2 / 40 ] loss: 0.013
[ 3 / 40 ] loss: 0.022
[ 4 / 40 ] loss: 0.011
[ 5 / 40 ] loss: 0.011
[ 6 / 40 ] loss: 0.003
[ 7 / 40 ] loss: 0.005
[ 8 / 40 ] loss: 0.021
[ 9 / 40 ] loss: 0.014
[ 10 / 40 ] loss: 0.006
[ 11 / 40 ] loss: 0.216
[ 12 / 40 ] loss: 0.009
[ 13 / 40 ] loss: 0.008
[ 14 / 40 ] loss: 0.017
[ 15 / 40 ] loss: 0.227
[ 16 / 40 ] loss: 0.011
[ 17 / 40 ] loss: 0.004
[ 18 / 40 ] loss: 0.007
[ 19 / 40 ] loss: 0.009
[ 20 / 40 ] loss: 0.011
[ 21 / 40 ] loss: 0.010
[ 22 / 40 ] loss: 0.113
[ 23 / 40 ] loss: 0.325
[ 24 / 40 ] loss: 0.006
[ 25 / 40 ] loss: 0.003
[ 26 / 40 ] loss: 0.146
[ 27 / 40 ] loss: 0.014
[ 28 / 40 ] loss: 0.101
[ 29 / 40 ] loss: 0.060
[ 30 / 40 ] loss: 0.024
[ 31 / 40 ] loss: 0.093
[ 32 / 40 ] loss: 0.021
[ 33 / 40 ] loss: 0.034
[ 34 / 40 ] loss: 0.144
[ 35 / 40 ] loss: 0.084
[ 36 / 40 ] loss: 0.378
[ 37 / 40 ] loss: 0.186
[ 38 / 40 ] loss: 0.042
[ 39 / 40 ] loss: 0.044
[ 40 / 40 ] loss: 0.022
0.06208974607870914
Accuracy: 0.776371 -- Precision: 0.773050 -- Recall: 0.838462 -- F1: 0.804428 -- AUC: 0.775485
========= epoch: 36 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.096
[ 2 / 40 ] loss: 0.030
[ 3 / 40 ] loss: 0.249
[ 4 / 40 ] loss: 0.018
[ 5 / 40 ] loss: 0.009
[ 6 / 40 ] loss: 0.099
[ 7 / 40 ] loss: 0.013
[ 8 / 40 ] loss: 0.013
[ 9 / 40 ] loss: 0.195
[ 10 / 40 ] loss: 0.011
[ 11 / 40 ] loss: 0.128
[ 12 / 40 ] loss: 0.011
[ 13 / 40 ] loss: 0.084
[ 14 / 40 ] loss: 0.011
[ 15 / 40 ] loss: 0.143
[ 16 / 40 ] loss: 0.014
[ 17 / 40 ] loss: 0.008
[ 18 / 40 ] loss: 0.017
[ 19 / 40 ] loss: 0.030
[ 20 / 40 ] loss: 0.014
[ 21 / 40 ] loss: 0.137
[ 22 / 40 ] loss: 0.066
[ 23 / 40 ] loss: 0.166
[ 24 / 40 ] loss: 0.033
[ 25 / 40 ] loss: 0.017
[ 26 / 40 ] loss: 0.019
[ 27 / 40 ] loss: 0.017
[ 28 / 40 ] loss: 0.140
[ 29 / 40 ] loss: 0.026
[ 30 / 40 ] loss: 0.063
[ 31 / 40 ] loss: 0.028
[ 32 / 40 ] loss: 0.063
[ 33 / 40 ] loss: 0.026
[ 34 / 40 ] loss: 0.476
[ 35 / 40 ] loss: 0.180
[ 36 / 40 ] loss: 0.192
[ 37 / 40 ] loss: 0.230
[ 38 / 40 ] loss: 0.108
[ 39 / 40 ] loss: 0.030
[ 40 / 40 ] loss: 0.239
0.0862269779550843
Accuracy: 0.776371 -- Precision: 0.818182 -- Recall: 0.761538 -- F1: 0.788845 -- AUC: 0.798203
========= epoch: 37 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.125
[ 2 / 40 ] loss: 0.093
[ 3 / 40 ] loss: 0.150
[ 4 / 40 ] loss: 0.113
[ 5 / 40 ] loss: 0.011
[ 6 / 40 ] loss: 0.253
[ 7 / 40 ] loss: 0.035
[ 8 / 40 ] loss: 0.040
[ 9 / 40 ] loss: 0.017
[ 10 / 40 ] loss: 0.036
[ 11 / 40 ] loss: 0.058
[ 12 / 40 ] loss: 0.041
[ 13 / 40 ] loss: 0.042
[ 14 / 40 ] loss: 0.256
[ 15 / 40 ] loss: 0.046
[ 16 / 40 ] loss: 0.098
[ 17 / 40 ] loss: 0.285
[ 18 / 40 ] loss: 0.062
[ 19 / 40 ] loss: 0.060
[ 20 / 40 ] loss: 0.143
[ 21 / 40 ] loss: 0.107
[ 22 / 40 ] loss: 0.076
[ 23 / 40 ] loss: 0.171
[ 24 / 40 ] loss: 0.019
[ 25 / 40 ] loss: 0.016
[ 26 / 40 ] loss: 0.017
[ 27 / 40 ] loss: 0.043
[ 28 / 40 ] loss: 0.014
[ 29 / 40 ] loss: 0.007
[ 30 / 40 ] loss: 0.232
[ 31 / 40 ] loss: 0.011
[ 32 / 40 ] loss: 0.008
[ 33 / 40 ] loss: 0.105
[ 34 / 40 ] loss: 0.044
[ 35 / 40 ] loss: 0.014
[ 36 / 40 ] loss: 0.009
[ 37 / 40 ] loss: 0.018
[ 38 / 40 ] loss: 0.120
[ 39 / 40 ] loss: 0.439
[ 40 / 40 ] loss: 0.167
0.09003502076957375
Accuracy: 0.746835 -- Precision: 0.746479 -- Recall: 0.815385 -- F1: 0.779412 -- AUC: 0.815061
========= epoch: 38 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.106
[ 2 / 40 ] loss: 0.032
[ 3 / 40 ] loss: 0.015
[ 4 / 40 ] loss: 0.032
[ 5 / 40 ] loss: 0.019
[ 6 / 40 ] loss: 0.014
[ 7 / 40 ] loss: 0.177
[ 8 / 40 ] loss: 0.015
[ 9 / 40 ] loss: 0.046
[ 10 / 40 ] loss: 0.239
[ 11 / 40 ] loss: 0.130
[ 12 / 40 ] loss: 0.046
[ 13 / 40 ] loss: 0.033
[ 14 / 40 ] loss: 0.092
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.008
[ 17 / 40 ] loss: 0.121
[ 18 / 40 ] loss: 0.020
[ 19 / 40 ] loss: 0.109
[ 20 / 40 ] loss: 0.029
[ 21 / 40 ] loss: 0.035
[ 22 / 40 ] loss: 0.059
[ 23 / 40 ] loss: 0.233
[ 24 / 40 ] loss: 0.014
[ 25 / 40 ] loss: 0.358
[ 26 / 40 ] loss: 0.167
[ 27 / 40 ] loss: 0.058
[ 28 / 40 ] loss: 0.011
[ 29 / 40 ] loss: 0.019
[ 30 / 40 ] loss: 0.007
[ 31 / 40 ] loss: 0.015
[ 32 / 40 ] loss: 0.055
[ 33 / 40 ] loss: 0.019
[ 34 / 40 ] loss: 0.053
[ 35 / 40 ] loss: 0.175
[ 36 / 40 ] loss: 0.040
[ 37 / 40 ] loss: 0.091
[ 38 / 40 ] loss: 0.163
[ 39 / 40 ] loss: 0.296
[ 40 / 40 ] loss: 0.015
0.07941931698005647
Accuracy: 0.763713 -- Precision: 0.764286 -- Recall: 0.823077 -- F1: 0.792593 -- AUC: 0.820669
========= epoch: 39 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.009
[ 2 / 40 ] loss: 0.004
[ 3 / 40 ] loss: 0.006
[ 4 / 40 ] loss: 0.020
[ 5 / 40 ] loss: 0.018
[ 6 / 40 ] loss: 0.045
[ 7 / 40 ] loss: 0.218
[ 8 / 40 ] loss: 0.419
[ 9 / 40 ] loss: 0.008
[ 10 / 40 ] loss: 0.010
[ 11 / 40 ] loss: 0.130
[ 12 / 40 ] loss: 0.027
[ 13 / 40 ] loss: 0.048
[ 14 / 40 ] loss: 0.040
[ 15 / 40 ] loss: 0.098
[ 16 / 40 ] loss: 0.012
[ 17 / 40 ] loss: 0.032
[ 18 / 40 ] loss: 0.087
[ 19 / 40 ] loss: 0.028
[ 20 / 40 ] loss: 0.030
[ 21 / 40 ] loss: 0.031
[ 22 / 40 ] loss: 0.019
[ 23 / 40 ] loss: 0.031
[ 24 / 40 ] loss: 0.048
[ 25 / 40 ] loss: 0.031
[ 26 / 40 ] loss: 0.033
[ 27 / 40 ] loss: 0.235
[ 28 / 40 ] loss: 0.020
[ 29 / 40 ] loss: 0.009
[ 30 / 40 ] loss: 0.057
[ 31 / 40 ] loss: 0.045
[ 32 / 40 ] loss: 0.006
[ 33 / 40 ] loss: 0.008
[ 34 / 40 ] loss: 0.110
[ 35 / 40 ] loss: 0.009
[ 36 / 40 ] loss: 0.109
[ 37 / 40 ] loss: 0.131
[ 38 / 40 ] loss: 0.020
[ 39 / 40 ] loss: 0.092
[ 40 / 40 ] loss: 0.029
0.05899183574365452
Accuracy: 0.776371 -- Precision: 0.773050 -- Recall: 0.838462 -- F1: 0.804428 -- AUC: 0.801510
========= epoch: 40 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.030
[ 2 / 40 ] loss: 0.006
[ 3 / 40 ] loss: 0.137
[ 4 / 40 ] loss: 0.030
[ 5 / 40 ] loss: 0.014
[ 6 / 40 ] loss: 0.006
[ 7 / 40 ] loss: 0.014
[ 8 / 40 ] loss: 0.012
[ 9 / 40 ] loss: 0.020
[ 10 / 40 ] loss: 0.060
[ 11 / 40 ] loss: 0.038
[ 12 / 40 ] loss: 0.019
[ 13 / 40 ] loss: 0.085
[ 14 / 40 ] loss: 0.011
[ 15 / 40 ] loss: 0.008
[ 16 / 40 ] loss: 0.011
[ 17 / 40 ] loss: 0.098
[ 18 / 40 ] loss: 0.012
[ 19 / 40 ] loss: 0.025
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.010
[ 22 / 40 ] loss: 0.006
[ 23 / 40 ] loss: 0.011
[ 24 / 40 ] loss: 0.021
[ 25 / 40 ] loss: 0.224
[ 26 / 40 ] loss: 0.006
[ 27 / 40 ] loss: 0.006
[ 28 / 40 ] loss: 0.282
[ 29 / 40 ] loss: 0.148
[ 30 / 40 ] loss: 0.004
[ 31 / 40 ] loss: 0.004
[ 32 / 40 ] loss: 0.006
[ 33 / 40 ] loss: 0.012
[ 34 / 40 ] loss: 0.017
[ 35 / 40 ] loss: 0.018
[ 36 / 40 ] loss: 0.046
[ 37 / 40 ] loss: 0.054
[ 38 / 40 ] loss: 0.177
[ 39 / 40 ] loss: 0.016
[ 40 / 40 ] loss: 0.004
0.042923351714853196
Accuracy: 0.776371 -- Precision: 0.789474 -- Recall: 0.807692 -- F1: 0.798479 -- AUC: 0.812940
========= epoch: 41 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.024
[ 2 / 40 ] loss: 0.012
[ 3 / 40 ] loss: 0.011
[ 4 / 40 ] loss: 0.007
[ 5 / 40 ] loss: 0.092
[ 6 / 40 ] loss: 0.005
[ 7 / 40 ] loss: 0.013
[ 8 / 40 ] loss: 0.019
[ 9 / 40 ] loss: 0.020
[ 10 / 40 ] loss: 0.036
[ 11 / 40 ] loss: 0.010
[ 12 / 40 ] loss: 0.092
[ 13 / 40 ] loss: 0.004
[ 14 / 40 ] loss: 0.011
[ 15 / 40 ] loss: 0.032
[ 16 / 40 ] loss: 0.012
[ 17 / 40 ] loss: 0.018
[ 18 / 40 ] loss: 0.013
[ 19 / 40 ] loss: 0.004
[ 20 / 40 ] loss: 0.010
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.086
[ 23 / 40 ] loss: 0.010
[ 24 / 40 ] loss: 0.081
[ 25 / 40 ] loss: 0.014
[ 26 / 40 ] loss: 0.011
[ 27 / 40 ] loss: 0.015
[ 28 / 40 ] loss: 0.012
[ 29 / 40 ] loss: 0.003
[ 30 / 40 ] loss: 0.004
[ 31 / 40 ] loss: 0.007
[ 32 / 40 ] loss: 0.162
[ 33 / 40 ] loss: 0.004
[ 34 / 40 ] loss: 0.061
[ 35 / 40 ] loss: 0.242
[ 36 / 40 ] loss: 0.034
[ 37 / 40 ] loss: 0.033
[ 38 / 40 ] loss: 0.013
[ 39 / 40 ] loss: 0.170
[ 40 / 40 ] loss: 0.004
0.03531581812421791
Accuracy: 0.772152 -- Precision: 0.771429 -- Recall: 0.830769 -- F1: 0.800000 -- AUC: 0.816355
========= epoch: 42 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.014
[ 2 / 40 ] loss: 0.231
[ 3 / 40 ] loss: 0.004
[ 4 / 40 ] loss: 0.097
[ 5 / 40 ] loss: 0.015
[ 6 / 40 ] loss: 0.006
[ 7 / 40 ] loss: 0.012
[ 8 / 40 ] loss: 0.020
[ 9 / 40 ] loss: 0.005
[ 10 / 40 ] loss: 0.019
[ 11 / 40 ] loss: 0.004
[ 12 / 40 ] loss: 0.045
[ 13 / 40 ] loss: 0.007
[ 14 / 40 ] loss: 0.020
[ 15 / 40 ] loss: 0.004
[ 16 / 40 ] loss: 0.004
[ 17 / 40 ] loss: 0.082
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.008
[ 20 / 40 ] loss: 0.004
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.113
[ 23 / 40 ] loss: 0.082
[ 24 / 40 ] loss: 0.013
[ 25 / 40 ] loss: 0.032
[ 26 / 40 ] loss: 0.014
[ 27 / 40 ] loss: 0.004
[ 28 / 40 ] loss: 0.030
[ 29 / 40 ] loss: 0.012
[ 30 / 40 ] loss: 0.026
[ 31 / 40 ] loss: 0.131
[ 32 / 40 ] loss: 0.005
[ 33 / 40 ] loss: 0.006
[ 34 / 40 ] loss: 0.029
[ 35 / 40 ] loss: 0.075
[ 36 / 40 ] loss: 0.088
[ 37 / 40 ] loss: 0.042
[ 38 / 40 ] loss: 0.014
[ 39 / 40 ] loss: 0.105
[ 40 / 40 ] loss: 0.004
0.036013316467870024
Accuracy: 0.776371 -- Precision: 0.823529 -- Recall: 0.753846 -- F1: 0.787149 -- AUC: 0.819482
========= epoch: 43 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.017
[ 2 / 40 ] loss: 0.076
[ 3 / 40 ] loss: 0.008
[ 4 / 40 ] loss: 0.017
[ 5 / 40 ] loss: 0.025
[ 6 / 40 ] loss: 0.245
[ 7 / 40 ] loss: 0.100
[ 8 / 40 ] loss: 0.012
[ 9 / 40 ] loss: 0.003
[ 10 / 40 ] loss: 0.003
[ 11 / 40 ] loss: 0.026
[ 12 / 40 ] loss: 0.003
[ 13 / 40 ] loss: 0.017
[ 14 / 40 ] loss: 0.011
[ 15 / 40 ] loss: 0.129
[ 16 / 40 ] loss: 0.300
[ 17 / 40 ] loss: 0.025
[ 18 / 40 ] loss: 0.029
[ 19 / 40 ] loss: 0.004
[ 20 / 40 ] loss: 0.007
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.033
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.003
[ 25 / 40 ] loss: 0.008
[ 26 / 40 ] loss: 0.020
[ 27 / 40 ] loss: 0.083
[ 28 / 40 ] loss: 0.076
[ 29 / 40 ] loss: 0.004
[ 30 / 40 ] loss: 0.006
[ 31 / 40 ] loss: 0.015
[ 32 / 40 ] loss: 0.071
[ 33 / 40 ] loss: 0.019
[ 34 / 40 ] loss: 0.005
[ 35 / 40 ] loss: 0.007
[ 36 / 40 ] loss: 0.025
[ 37 / 40 ] loss: 0.060
[ 38 / 40 ] loss: 0.013
[ 39 / 40 ] loss: 0.025
[ 40 / 40 ] loss: 0.355
0.047352397837676106
Accuracy: 0.755274 -- Precision: 0.736842 -- Recall: 0.861538 -- F1: 0.794326 -- AUC: 0.804601
========= epoch: 44 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.007
[ 2 / 40 ] loss: 0.088
[ 3 / 40 ] loss: 0.038
[ 4 / 40 ] loss: 0.043
[ 5 / 40 ] loss: 0.022
[ 6 / 40 ] loss: 0.018
[ 7 / 40 ] loss: 0.008
[ 8 / 40 ] loss: 0.061
[ 9 / 40 ] loss: 0.082
[ 10 / 40 ] loss: 0.011
[ 11 / 40 ] loss: 0.011
[ 12 / 40 ] loss: 0.020
[ 13 / 40 ] loss: 0.010
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.010
[ 16 / 40 ] loss: 0.063
[ 17 / 40 ] loss: 0.022
[ 18 / 40 ] loss: 0.011
[ 19 / 40 ] loss: 0.020
[ 20 / 40 ] loss: 0.085
[ 21 / 40 ] loss: 0.012
[ 22 / 40 ] loss: 0.010
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.010
[ 25 / 40 ] loss: 0.010
[ 26 / 40 ] loss: 0.003
[ 27 / 40 ] loss: 0.081
[ 28 / 40 ] loss: 0.051
[ 29 / 40 ] loss: 0.245
[ 30 / 40 ] loss: 0.003
[ 31 / 40 ] loss: 0.005
[ 32 / 40 ] loss: 0.013
[ 33 / 40 ] loss: 0.005
[ 34 / 40 ] loss: 0.099
[ 35 / 40 ] loss: 0.085
[ 36 / 40 ] loss: 0.016
[ 37 / 40 ] loss: 0.069
[ 38 / 40 ] loss: 0.100
[ 39 / 40 ] loss: 0.249
[ 40 / 40 ] loss: 0.002
0.04263237735140137
Accuracy: 0.767932 -- Precision: 0.769784 -- Recall: 0.823077 -- F1: 0.795539 -- AUC: 0.808375
========= epoch: 45 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.003
[ 2 / 40 ] loss: 0.079
[ 3 / 40 ] loss: 0.047
[ 4 / 40 ] loss: 0.075
[ 5 / 40 ] loss: 0.327
[ 6 / 40 ] loss: 0.027
[ 7 / 40 ] loss: 0.015
[ 8 / 40 ] loss: 0.003
[ 9 / 40 ] loss: 0.058
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.015
[ 12 / 40 ] loss: 0.177
[ 13 / 40 ] loss: 0.058
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.013
[ 16 / 40 ] loss: 0.031
[ 17 / 40 ] loss: 0.023
[ 18 / 40 ] loss: 0.021
[ 19 / 40 ] loss: 0.090
[ 20 / 40 ] loss: 0.015
[ 21 / 40 ] loss: 0.007
[ 22 / 40 ] loss: 0.202
[ 23 / 40 ] loss: 0.012
[ 24 / 40 ] loss: 0.006
[ 25 / 40 ] loss: 0.034
[ 26 / 40 ] loss: 0.118
[ 27 / 40 ] loss: 0.024
[ 28 / 40 ] loss: 0.004
[ 29 / 40 ] loss: 0.093
[ 30 / 40 ] loss: 0.003
[ 31 / 40 ] loss: 0.005
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.017
[ 34 / 40 ] loss: 0.071
[ 35 / 40 ] loss: 0.012
[ 36 / 40 ] loss: 0.092
[ 37 / 40 ] loss: 0.004
[ 38 / 40 ] loss: 0.013
[ 39 / 40 ] loss: 0.077
[ 40 / 40 ] loss: 0.026
0.04771535939653404
Accuracy: 0.767932 -- Precision: 0.773723 -- Recall: 0.815385 -- F1: 0.794007 -- AUC: 0.808627
========= epoch: 46 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.062
[ 2 / 40 ] loss: 0.026
[ 3 / 40 ] loss: 0.067
[ 4 / 40 ] loss: 0.026
[ 5 / 40 ] loss: 0.035
[ 6 / 40 ] loss: 0.063
[ 7 / 40 ] loss: 0.016
[ 8 / 40 ] loss: 0.030
[ 9 / 40 ] loss: 0.016
[ 10 / 40 ] loss: 0.016
[ 11 / 40 ] loss: 0.012
[ 12 / 40 ] loss: 0.154
[ 13 / 40 ] loss: 0.004
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.005
[ 16 / 40 ] loss: 0.003
[ 17 / 40 ] loss: 0.003
[ 18 / 40 ] loss: 0.087
[ 19 / 40 ] loss: 0.014
[ 20 / 40 ] loss: 0.033
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.014
[ 23 / 40 ] loss: 0.017
[ 24 / 40 ] loss: 0.002
[ 25 / 40 ] loss: 0.006
[ 26 / 40 ] loss: 0.026
[ 27 / 40 ] loss: 0.017
[ 28 / 40 ] loss: 0.003
[ 29 / 40 ] loss: 0.006
[ 30 / 40 ] loss: 0.032
[ 31 / 40 ] loss: 0.004
[ 32 / 40 ] loss: 0.016
[ 33 / 40 ] loss: 0.014
[ 34 / 40 ] loss: 0.317
[ 35 / 40 ] loss: 0.003
[ 36 / 40 ] loss: 0.012
[ 37 / 40 ] loss: 0.006
[ 38 / 40 ] loss: 0.068
[ 39 / 40 ] loss: 0.015
[ 40 / 40 ] loss: 0.738
0.04990090447245166
Accuracy: 0.772152 -- Precision: 0.771429 -- Recall: 0.830769 -- F1: 0.800000 -- AUC: 0.813156
========= epoch: 47 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.004
[ 2 / 40 ] loss: 0.004
[ 3 / 40 ] loss: 0.012
[ 4 / 40 ] loss: 0.008
[ 5 / 40 ] loss: 0.011
[ 6 / 40 ] loss: 0.023
[ 7 / 40 ] loss: 0.004
[ 8 / 40 ] loss: 0.011
[ 9 / 40 ] loss: 0.004
[ 10 / 40 ] loss: 0.011
[ 11 / 40 ] loss: 0.013
[ 12 / 40 ] loss: 0.052
[ 13 / 40 ] loss: 0.199
[ 14 / 40 ] loss: 0.016
[ 15 / 40 ] loss: 0.023
[ 16 / 40 ] loss: 0.017
[ 17 / 40 ] loss: 0.011
[ 18 / 40 ] loss: 0.004
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.024
[ 22 / 40 ] loss: 0.008
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.025
[ 25 / 40 ] loss: 0.136
[ 26 / 40 ] loss: 0.005
[ 27 / 40 ] loss: 0.004
[ 28 / 40 ] loss: 0.095
[ 29 / 40 ] loss: 0.092
[ 30 / 40 ] loss: 0.281
[ 31 / 40 ] loss: 0.051
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.013
[ 34 / 40 ] loss: 0.039
[ 35 / 40 ] loss: 0.091
[ 36 / 40 ] loss: 0.083
[ 37 / 40 ] loss: 0.022
[ 38 / 40 ] loss: 0.100
[ 39 / 40 ] loss: 0.017
[ 40 / 40 ] loss: 0.005
0.038422650896245615
Accuracy: 0.759494 -- Precision: 0.770370 -- Recall: 0.800000 -- F1: 0.784906 -- AUC: 0.768512
========= epoch: 48 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.038
[ 2 / 40 ] loss: 0.023
[ 3 / 40 ] loss: 0.017
[ 4 / 40 ] loss: 0.014
[ 5 / 40 ] loss: 0.024
[ 6 / 40 ] loss: 0.015
[ 7 / 40 ] loss: 0.017
[ 8 / 40 ] loss: 0.007
[ 9 / 40 ] loss: 0.076
[ 10 / 40 ] loss: 0.022
[ 11 / 40 ] loss: 0.015
[ 12 / 40 ] loss: 0.088
[ 13 / 40 ] loss: 0.066
[ 14 / 40 ] loss: 0.024
[ 15 / 40 ] loss: 0.028
[ 16 / 40 ] loss: 0.099
[ 17 / 40 ] loss: 0.020
[ 18 / 40 ] loss: 0.002
[ 19 / 40 ] loss: 0.229
[ 20 / 40 ] loss: 0.014
[ 21 / 40 ] loss: 0.021
[ 22 / 40 ] loss: 0.085
[ 23 / 40 ] loss: 0.003
[ 24 / 40 ] loss: 0.004
[ 25 / 40 ] loss: 0.012
[ 26 / 40 ] loss: 0.003
[ 27 / 40 ] loss: 0.074
[ 28 / 40 ] loss: 0.009
[ 29 / 40 ] loss: 0.011
[ 30 / 40 ] loss: 0.012
[ 31 / 40 ] loss: 0.011
[ 32 / 40 ] loss: 0.003
[ 33 / 40 ] loss: 0.004
[ 34 / 40 ] loss: 0.097
[ 35 / 40 ] loss: 0.101
[ 36 / 40 ] loss: 0.189
[ 37 / 40 ] loss: 0.004
[ 38 / 40 ] loss: 0.016
[ 39 / 40 ] loss: 0.004
[ 40 / 40 ] loss: 0.018
0.03799467280041426
Accuracy: 0.755274 -- Precision: 0.768657 -- Recall: 0.792308 -- F1: 0.780303 -- AUC: 0.805823
========= epoch: 49 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.003
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.010
[ 4 / 40 ] loss: 0.008
[ 5 / 40 ] loss: 0.050
[ 6 / 40 ] loss: 0.007
[ 7 / 40 ] loss: 0.022
[ 8 / 40 ] loss: 0.005
[ 9 / 40 ] loss: 0.003
[ 10 / 40 ] loss: 0.018
[ 11 / 40 ] loss: 0.015
[ 12 / 40 ] loss: 0.011
[ 13 / 40 ] loss: 0.197
[ 14 / 40 ] loss: 0.003
[ 15 / 40 ] loss: 0.093
[ 16 / 40 ] loss: 0.103
[ 17 / 40 ] loss: 0.098
[ 18 / 40 ] loss: 0.096
[ 19 / 40 ] loss: 0.066
[ 20 / 40 ] loss: 0.010
[ 21 / 40 ] loss: 0.013
[ 22 / 40 ] loss: 0.010
[ 23 / 40 ] loss: 0.141
[ 24 / 40 ] loss: 0.011
[ 25 / 40 ] loss: 0.013
[ 26 / 40 ] loss: 0.004
[ 27 / 40 ] loss: 0.003
[ 28 / 40 ] loss: 0.016
[ 29 / 40 ] loss: 0.037
[ 30 / 40 ] loss: 0.004
[ 31 / 40 ] loss: 0.075
[ 32 / 40 ] loss: 0.057
[ 33 / 40 ] loss: 0.004
[ 34 / 40 ] loss: 0.024
[ 35 / 40 ] loss: 0.009
[ 36 / 40 ] loss: 0.060
[ 37 / 40 ] loss: 0.010
[ 38 / 40 ] loss: 0.106
[ 39 / 40 ] loss: 0.002
[ 40 / 40 ] loss: 0.004
0.03560521222534589
Accuracy: 0.767932 -- Precision: 0.758621 -- Recall: 0.846154 -- F1: 0.800000 -- AUC: 0.801725
0.4675105485232067 0.48207214119065683 0.4753846153846154 0.47818288008340826 0.4851186196980589
[113, 82, 39, 57, 189, 58, 178, 218, 172, 152, 66, 188, 185, 181, 195, 51, 91, 146, 40, 215, 63, 72, 119, 184, 101, 194, 74, 23, 227, 201, 200, 60, 95, 233, 192, 123, 202, 237, 186, 31, 7, 18, 187, 171, 132, 204, 214, 125, 45, 16, 22, 124, 109, 34, 232, 41, 169, 167, 225, 234, 166, 55, 128, 61, 32, 68, 219, 2, 149, 76, 207, 35, 43, 209, 62, 158, 196, 73, 168, 11, 230, 115, 93, 129, 59, 220, 154, 121, 142, 208, 231, 116, 131, 42, 147, 37, 136, 87, 191, 112, 213, 173, 175, 150, 10, 139, 221, 75, 183, 15, 199, 24, 193, 143, 47, 176, 103, 38, 54, 81, 228, 56, 9, 8, 97, 96, 78, 27, 25, 85, 212, 50, 44, 110, 30, 71, 177, 14, 159, 84, 86, 46, 65, 100, 216, 26, 190, 67, 236, 134, 52, 117, 89, 222, 211, 210, 144, 137, 153, 94, 6, 140, 79, 223, 48, 12, 205, 120, 53, 17, 226, 19, 170, 36, 155, 174, 164, 3, 151, 69, 217, 180, 88, 13, 92, 99, 165, 163, 162, 64, 138, 28, 29, 98, 203, 130, 77, 104, 229, 107, 108, 21, 197, 156, 157, 114, 126, 127, 161, 179, 118, 224, 141, 105, 90, 70, 33, 182, 5, 106, 83, 133, 235, 20, 206, 135, 122, 145, 80, 198, 49, 148, 102, 111, 4, 160, 1, 156, 127, 91, 61, 15, 124, 44, 130, 216, 235, 170, 66, 53, 67, 12, 14, 154, 196, 126, 176, 168, 76, 54, 88, 129, 201, 166, 94, 161, 200, 158, 78, 199, 211, 81, 29, 73, 184, 237, 26, 179, 45, 32, 134, 116, 75, 164, 20, 56, 24, 84, 8, 132, 183, 59, 57, 58, 202, 1, 192, 46, 42, 121, 34, 206, 217, 48, 155, 207, 13, 223, 18, 185, 140, 23, 107, 37, 138, 52, 172, 30, 106, 22, 227, 149, 47, 82, 125, 112, 114, 95, 188, 175, 115, 69, 16, 70, 25, 225, 177, 79, 195, 74, 109, 146, 186, 232, 43, 137, 80, 77, 3, 27, 105, 83, 60, 190, 220, 101, 10, 113, 210, 40, 122, 193, 39, 141, 108, 98, 221, 49, 197, 171, 150, 87, 119, 50, 62, 139, 99, 167, 93, 21, 209, 55, 173, 65, 38, 194, 204, 2, 189, 41, 102, 152, 36, 213, 131, 203, 228, 162, 198, 9, 143, 218, 11, 157, 219, 144, 230, 234, 224, 111, 96, 5, 120, 85, 51, 71, 136, 180, 117, 169, 229, 7, 208, 222, 19, 214, 226, 178, 147, 103, 133, 165, 236, 31, 187, 4, 104, 72, 148, 68, 215, 63, 35, 151, 191, 182, 110, 174, 145, 64, 89, 123, 142, 17, 90, 100, 233, 6, 159, 135, 231, 160, 128, 28, 212, 97, 118, 163, 153, 92, 181, 205, 33, 86, 147, 229, 219, 41, 92, 34, 134, 45, 200, 225, 195, 85, 210, 173, 8, 136, 54, 207, 35, 95, 108, 230, 90, 149, 227, 53, 129, 56, 26, 181, 28, 148, 143, 197, 20, 228, 166, 86, 9, 25, 224, 128, 96, 3, 87, 170, 161, 97, 174, 47, 142, 124, 62, 160, 198, 183, 145, 205, 138, 126, 152, 27, 164, 6, 191, 75, 12, 139, 175, 55, 84, 21, 201, 153, 22, 235, 192, 18, 154, 119, 93, 60, 51, 67, 237, 33, 24, 44, 104, 232, 151, 186, 133, 63, 109, 68, 176, 52, 13, 32, 203, 30, 71, 19, 169, 162, 80, 77, 39, 46, 188, 231, 66, 144, 167, 215, 59, 172, 193, 211, 209, 1, 40, 4, 184, 132, 146, 36, 204, 218, 212, 165, 31, 206, 48, 117, 214, 69, 82, 15, 141, 116, 114, 49, 23, 118, 182, 178, 81, 106, 185, 2, 158, 123, 159, 196, 64, 43, 157, 233, 125, 131, 220, 111, 213, 38, 208, 107, 190, 177, 171, 199, 105, 103, 127, 5, 102, 101, 155, 121, 29, 10, 76, 236, 83, 226, 78, 58, 217, 11, 57, 88, 194, 98, 50, 113, 137, 61, 110, 17, 74, 120, 202, 221, 7, 79, 65, 89, 222, 112, 150, 91, 168, 100, 156, 180, 14, 16, 99, 70, 73, 130, 42, 135, 122, 140, 216, 72, 189, 234, 223, 94, 179, 37, 187, 115, 163]
[1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1]
[1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1]
训练集: 947
测试集: 236
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 0 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.700
[ 2 / 40 ] loss: 0.678
[ 3 / 40 ] loss: 0.686
[ 4 / 40 ] loss: 0.704
[ 5 / 40 ] loss: 0.681
[ 6 / 40 ] loss: 0.703
[ 7 / 40 ] loss: 0.682
[ 8 / 40 ] loss: 0.676
[ 9 / 40 ] loss: 0.690
[ 10 / 40 ] loss: 0.702
[ 11 / 40 ] loss: 0.666
[ 12 / 40 ] loss: 0.674
[ 13 / 40 ] loss: 0.737
[ 14 / 40 ] loss: 0.661
[ 15 / 40 ] loss: 0.670
[ 16 / 40 ] loss: 0.692
[ 17 / 40 ] loss: 0.658
[ 18 / 40 ] loss: 0.678
[ 19 / 40 ] loss: 0.621
[ 20 / 40 ] loss: 0.710
[ 21 / 40 ] loss: 0.663
[ 22 / 40 ] loss: 0.685
[ 23 / 40 ] loss: 0.674
[ 24 / 40 ] loss: 0.631
[ 25 / 40 ] loss: 0.672
[ 26 / 40 ] loss: 0.764
[ 27 / 40 ] loss: 0.742
[ 28 / 40 ] loss: 0.662
[ 29 / 40 ] loss: 0.670
[ 30 / 40 ] loss: 0.739
[ 31 / 40 ] loss: 0.694
[ 32 / 40 ] loss: 0.684
[ 33 / 40 ] loss: 0.680
[ 34 / 40 ] loss: 0.643
[ 35 / 40 ] loss: 0.676
[ 36 / 40 ] loss: 0.734
[ 37 / 40 ] loss: 0.723
[ 38 / 40 ] loss: 0.703
[ 39 / 40 ] loss: 0.691
[ 40 / 40 ] loss: 0.710
0.6877448335289955
Accuracy: 0.635593 -- Precision: 0.741573 -- Recall: 0.511628 -- F1: 0.605505 -- AUC: 0.734116
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[171, 141, 129, 44, 67, 158, 170, 229, 122, 211, 12, 210, 55, 76, 133, 176, 207, 14, 84, 81, 200, 125, 145, 19, 131, 16, 152, 151, 115, 74, 93, 213, 155, 138, 162, 96, 60, 2, 31, 118, 53, 1, 15, 161, 137, 182, 64, 34, 163, 186, 219, 208, 75, 37, 112, 32, 33, 52, 157, 230, 11, 40, 51, 175, 179, 136, 41, 126, 221, 18, 218, 142, 198, 205, 197, 98, 77, 56, 209, 109, 65, 226, 107, 113, 222, 135, 89, 220, 227, 99, 216, 57, 203, 5, 58, 117, 190, 20, 97, 231, 24, 29, 185, 49, 17, 88, 61, 196, 187, 195, 154, 134, 102, 212, 119, 6, 9, 191, 7, 94, 132, 50, 121, 172, 39, 225, 114, 224, 100, 25, 143, 83, 110, 202, 21, 27, 111, 144, 204, 124, 147, 87, 173, 193, 101, 156, 201, 188, 223, 166, 78, 120, 108, 159, 178, 23, 184, 38, 45, 8, 181, 183, 47, 228, 71, 91, 160, 194, 22, 168, 73, 68, 123, 82, 63, 104, 106, 189, 180, 62, 234, 233, 235, 150, 80, 105, 146, 54, 165, 4, 70, 206, 92, 26, 35, 215, 10, 86, 28, 90, 164, 139, 130, 36, 42, 148, 169, 46, 153, 85, 214, 59, 72, 79, 232, 116, 103, 30, 140, 127, 192, 69, 199, 149, 43, 66, 48, 95, 128, 167, 13, 177, 3, 217, 236, 174]
[0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]
[1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 1 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.691
[ 2 / 40 ] loss: 0.686
[ 3 / 40 ] loss: 0.664
[ 4 / 40 ] loss: 0.682
[ 5 / 40 ] loss: 0.688
[ 6 / 40 ] loss: 0.655
[ 7 / 40 ] loss: 0.706
[ 8 / 40 ] loss: 0.732
[ 9 / 40 ] loss: 0.714
[ 10 / 40 ] loss: 0.665
[ 11 / 40 ] loss: 0.678
[ 12 / 40 ] loss: 0.702
[ 13 / 40 ] loss: 0.667
[ 14 / 40 ] loss: 0.673
[ 15 / 40 ] loss: 0.677
[ 16 / 40 ] loss: 0.649
[ 17 / 40 ] loss: 0.684
[ 18 / 40 ] loss: 0.690
[ 19 / 40 ] loss: 0.690
[ 20 / 40 ] loss: 0.680
[ 21 / 40 ] loss: 0.698
[ 22 / 40 ] loss: 0.681
[ 23 / 40 ] loss: 0.698
[ 24 / 40 ] loss: 0.669
[ 25 / 40 ] loss: 0.644
[ 26 / 40 ] loss: 0.655
[ 27 / 40 ] loss: 0.667
[ 28 / 40 ] loss: 0.667
[ 29 / 40 ] loss: 0.620
[ 30 / 40 ] loss: 0.675
[ 31 / 40 ] loss: 0.707
[ 32 / 40 ] loss: 0.663
[ 33 / 40 ] loss: 0.657
[ 34 / 40 ] loss: 0.662
[ 35 / 40 ] loss: 0.599
[ 36 / 40 ] loss: 0.636
[ 37 / 40 ] loss: 0.612
[ 38 / 40 ] loss: 0.589
[ 39 / 40 ] loss: 0.620
[ 40 / 40 ] loss: 0.644
0.6683792784810066
Accuracy: 0.682203 -- Precision: 0.646739 -- Recall: 0.922481 -- F1: 0.760383 -- AUC: 0.776860
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[155, 134, 213, 47, 3, 126, 72, 29, 105, 97, 235, 68, 75, 71, 141, 173, 210, 48, 138, 74, 79, 202, 27, 42, 26, 84, 18, 214, 91, 132, 222, 11, 166, 179, 162, 171, 4, 63, 101, 92, 58, 39, 160, 227, 167, 195, 225, 157, 233, 44, 127, 143, 103, 83, 133, 70, 35, 196, 159, 24, 93, 165, 232, 229, 121, 144, 164, 223, 140, 216, 170, 10, 142, 106, 41, 189, 217, 76, 61, 88, 199, 100, 122, 172, 20, 149, 220, 145, 116, 139, 151, 218, 2, 109, 177, 234, 96, 94, 86, 25, 161, 6, 28, 1, 117, 90, 174, 32, 150, 33, 211, 154, 191, 30, 51, 215, 119, 131, 57, 187, 231, 113, 85, 77, 104, 147, 55, 230, 102, 120, 43, 89, 197, 128, 190, 148, 23, 38, 178, 192, 7, 16, 153, 107, 236, 5, 201, 65, 182, 37, 175, 114, 184, 19, 209, 224, 99, 180, 13, 146, 156, 219, 40, 62, 21, 158, 95, 152, 185, 198, 31, 59, 14, 53, 69, 188, 123, 169, 34, 226, 67, 8, 66, 186, 54, 52, 80, 108, 73, 206, 125, 78, 50, 193, 82, 194, 98, 115, 56, 110, 176, 135, 204, 207, 221, 163, 130, 36, 60, 124, 168, 81, 9, 15, 111, 203, 205, 228, 49, 136, 17, 200, 118, 212, 112, 12, 183, 46, 22, 181, 87, 64, 137, 208, 129, 45]
[0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]
[1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 2 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.659
[ 2 / 40 ] loss: 0.639
[ 3 / 40 ] loss: 0.698
[ 4 / 40 ] loss: 0.652
[ 5 / 40 ] loss: 0.575
[ 6 / 40 ] loss: 0.633
[ 7 / 40 ] loss: 0.618
[ 8 / 40 ] loss: 0.557
[ 9 / 40 ] loss: 0.617
[ 10 / 40 ] loss: 0.683
[ 11 / 40 ] loss: 0.537
[ 12 / 40 ] loss: 0.585
[ 13 / 40 ] loss: 0.649
[ 14 / 40 ] loss: 0.645
[ 15 / 40 ] loss: 0.498
[ 16 / 40 ] loss: 0.634
[ 17 / 40 ] loss: 0.699
[ 18 / 40 ] loss: 0.588
[ 19 / 40 ] loss: 0.641
[ 20 / 40 ] loss: 0.579
[ 21 / 40 ] loss: 0.599
[ 22 / 40 ] loss: 0.680
[ 23 / 40 ] loss: 0.621
[ 24 / 40 ] loss: 0.668
[ 25 / 40 ] loss: 0.678
[ 26 / 40 ] loss: 0.597
[ 27 / 40 ] loss: 0.645
[ 28 / 40 ] loss: 0.686
[ 29 / 40 ] loss: 0.664
[ 30 / 40 ] loss: 0.630
[ 31 / 40 ] loss: 0.657
[ 32 / 40 ] loss: 0.601
[ 33 / 40 ] loss: 0.569
[ 34 / 40 ] loss: 0.688
[ 35 / 40 ] loss: 0.528
[ 36 / 40 ] loss: 0.438
[ 37 / 40 ] loss: 0.625
[ 38 / 40 ] loss: 0.708
[ 39 / 40 ] loss: 0.608
[ 40 / 40 ] loss: 0.463
0.6184817880392075
Accuracy: 0.720339 -- Precision: 0.760331 -- Recall: 0.713178 -- F1: 0.736000 -- AUC: 0.749330
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[141, 64, 45, 235, 42, 102, 144, 190, 147, 117, 196, 68, 194, 31, 165, 103, 150, 205, 49, 212, 53, 6, 36, 66, 3, 233, 75, 22, 152, 177, 218, 172, 26, 40, 80, 11, 84, 133, 170, 5, 227, 81, 65, 34, 88, 228, 206, 179, 47, 199, 214, 9, 173, 54, 209, 186, 148, 58, 121, 25, 43, 108, 114, 82, 221, 59, 95, 122, 20, 154, 23, 123, 184, 182, 109, 17, 215, 164, 203, 57, 151, 124, 118, 98, 202, 50, 136, 229, 174, 27, 192, 85, 232, 76, 180, 178, 111, 101, 28, 224, 181, 230, 220, 46, 72, 225, 127, 52, 107, 129, 142, 138, 91, 158, 10, 41, 236, 226, 149, 175, 2, 126, 115, 213, 207, 211, 79, 193, 18, 216, 198, 200, 234, 39, 140, 160, 105, 8, 168, 19, 191, 74, 24, 1, 161, 110, 171, 73, 15, 14, 35, 143, 162, 223, 83, 156, 119, 48, 155, 77, 55, 159, 104, 99, 167, 139, 92, 189, 204, 135, 67, 87, 93, 128, 16, 130, 153, 12, 176, 112, 208, 195, 231, 201, 86, 187, 169, 37, 210, 106, 89, 30, 145, 183, 219, 197, 61, 188, 78, 29, 131, 38, 185, 116, 62, 90, 132, 63, 71, 70, 69, 166, 33, 97, 13, 44, 137, 163, 60, 51, 134, 4, 113, 94, 7, 120, 222, 96, 217, 125, 21, 32, 100, 146, 56, 157]
[0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1]
[1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 3 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.455
[ 2 / 40 ] loss: 0.483
[ 3 / 40 ] loss: 0.434
[ 4 / 40 ] loss: 0.620
[ 5 / 40 ] loss: 0.388
[ 6 / 40 ] loss: 0.630
[ 7 / 40 ] loss: 0.367
[ 8 / 40 ] loss: 0.574
[ 9 / 40 ] loss: 0.510
[ 10 / 40 ] loss: 0.550
[ 11 / 40 ] loss: 0.612
[ 12 / 40 ] loss: 0.629
[ 13 / 40 ] loss: 0.519
[ 14 / 40 ] loss: 0.425
[ 15 / 40 ] loss: 0.572
[ 16 / 40 ] loss: 1.006
[ 17 / 40 ] loss: 0.479
[ 18 / 40 ] loss: 0.635
[ 19 / 40 ] loss: 0.539
[ 20 / 40 ] loss: 0.730
[ 21 / 40 ] loss: 0.577
[ 22 / 40 ] loss: 0.750
[ 23 / 40 ] loss: 0.566
[ 24 / 40 ] loss: 0.576
[ 25 / 40 ] loss: 0.650
[ 26 / 40 ] loss: 0.590
[ 27 / 40 ] loss: 0.594
[ 28 / 40 ] loss: 0.497
[ 29 / 40 ] loss: 0.643
[ 30 / 40 ] loss: 0.644
[ 31 / 40 ] loss: 0.514
[ 32 / 40 ] loss: 0.397
[ 33 / 40 ] loss: 0.648
[ 34 / 40 ] loss: 0.503
[ 35 / 40 ] loss: 0.418
[ 36 / 40 ] loss: 0.503
[ 37 / 40 ] loss: 0.565
[ 38 / 40 ] loss: 0.443
[ 39 / 40 ] loss: 0.562
[ 40 / 40 ] loss: 0.743
0.5634545758366585
Accuracy: 0.703390 -- Precision: 0.692810 -- Recall: 0.821705 -- F1: 0.751773 -- AUC: 0.783598
========= epoch: 4 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.375
[ 2 / 40 ] loss: 0.437
[ 3 / 40 ] loss: 0.691
[ 4 / 40 ] loss: 0.494
[ 5 / 40 ] loss: 0.378
[ 6 / 40 ] loss: 0.638
[ 7 / 40 ] loss: 0.563
[ 8 / 40 ] loss: 0.485
[ 9 / 40 ] loss: 0.503
[ 10 / 40 ] loss: 0.425
[ 11 / 40 ] loss: 0.487
[ 12 / 40 ] loss: 0.469
[ 13 / 40 ] loss: 0.357
[ 14 / 40 ] loss: 0.449
[ 15 / 40 ] loss: 0.478
[ 16 / 40 ] loss: 0.478
[ 17 / 40 ] loss: 0.585
[ 18 / 40 ] loss: 0.584
[ 19 / 40 ] loss: 0.430
[ 20 / 40 ] loss: 0.494
[ 21 / 40 ] loss: 0.645
[ 22 / 40 ] loss: 0.622
[ 23 / 40 ] loss: 0.517
[ 24 / 40 ] loss: 0.544
[ 25 / 40 ] loss: 0.504
[ 26 / 40 ] loss: 0.480
[ 27 / 40 ] loss: 0.525
[ 28 / 40 ] loss: 0.453
[ 29 / 40 ] loss: 0.458
[ 30 / 40 ] loss: 0.588
[ 31 / 40 ] loss: 0.584
[ 32 / 40 ] loss: 0.548
[ 33 / 40 ] loss: 0.405
[ 34 / 40 ] loss: 0.407
[ 35 / 40 ] loss: 0.330
[ 36 / 40 ] loss: 0.434
[ 37 / 40 ] loss: 0.274
[ 38 / 40 ] loss: 0.635
[ 39 / 40 ] loss: 0.271
[ 40 / 40 ] loss: 0.657
0.4920834109187126
Accuracy: 0.733051 -- Precision: 0.770492 -- Recall: 0.728682 -- F1: 0.749004 -- AUC: 0.808447
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[121, 74, 180, 163, 19, 111, 148, 131, 137, 102, 34, 140, 75, 135, 127, 25, 45, 12, 119, 90, 96, 132, 68, 53, 92, 192, 94, 69, 60, 218, 46, 217, 117, 158, 118, 18, 83, 130, 58, 22, 184, 177, 183, 10, 47, 174, 31, 76, 114, 129, 101, 70, 88, 133, 6, 81, 179, 227, 199, 40, 20, 23, 170, 155, 216, 201, 136, 73, 124, 231, 222, 52, 154, 48, 235, 61, 169, 207, 55, 56, 150, 11, 213, 143, 181, 236, 146, 24, 57, 223, 32, 108, 104, 50, 42, 204, 1, 78, 110, 195, 206, 152, 89, 63, 230, 210, 100, 30, 103, 175, 220, 82, 157, 59, 145, 112, 87, 234, 66, 182, 232, 86, 8, 156, 212, 3, 15, 116, 67, 125, 35, 95, 144, 44, 37, 165, 97, 107, 193, 62, 115, 142, 54, 106, 134, 173, 159, 197, 166, 171, 77, 172, 186, 17, 21, 26, 4, 225, 226, 208, 38, 99, 229, 162, 219, 64, 13, 190, 51, 214, 161, 33, 28, 123, 36, 141, 233, 105, 84, 196, 167, 41, 228, 7, 43, 203, 147, 16, 168, 205, 138, 188, 91, 202, 176, 153, 98, 79, 85, 72, 9, 198, 80, 221, 187, 39, 109, 215, 65, 128, 194, 14, 126, 160, 224, 120, 139, 200, 27, 93, 113, 71, 211, 189, 149, 164, 185, 191, 2, 151, 29, 49, 178, 122, 209, 5]
[1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0]
[1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 5 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.436
[ 2 / 40 ] loss: 0.525
[ 3 / 40 ] loss: 0.484
[ 4 / 40 ] loss: 0.506
[ 5 / 40 ] loss: 0.403
[ 6 / 40 ] loss: 0.408
[ 7 / 40 ] loss: 0.292
[ 8 / 40 ] loss: 0.358
[ 9 / 40 ] loss: 0.327
[ 10 / 40 ] loss: 0.451
[ 11 / 40 ] loss: 0.427
[ 12 / 40 ] loss: 0.436
[ 13 / 40 ] loss: 0.426
[ 14 / 40 ] loss: 0.326
[ 15 / 40 ] loss: 0.510
[ 16 / 40 ] loss: 0.543
[ 17 / 40 ] loss: 0.481
[ 18 / 40 ] loss: 0.553
[ 19 / 40 ] loss: 0.382
[ 20 / 40 ] loss: 0.447
[ 21 / 40 ] loss: 0.271
[ 22 / 40 ] loss: 0.433
[ 23 / 40 ] loss: 0.569
[ 24 / 40 ] loss: 0.504
[ 25 / 40 ] loss: 0.330
[ 26 / 40 ] loss: 0.542
[ 27 / 40 ] loss: 0.427
[ 28 / 40 ] loss: 0.396
[ 29 / 40 ] loss: 0.586
[ 30 / 40 ] loss: 0.469
[ 31 / 40 ] loss: 0.382
[ 32 / 40 ] loss: 0.360
[ 33 / 40 ] loss: 0.250
[ 34 / 40 ] loss: 0.365
[ 35 / 40 ] loss: 0.455
[ 36 / 40 ] loss: 0.322
[ 37 / 40 ] loss: 0.413
[ 38 / 40 ] loss: 0.441
[ 39 / 40 ] loss: 0.229
[ 40 / 40 ] loss: 0.244
0.41780121475458143
Accuracy: 0.745763 -- Precision: 0.771654 -- Recall: 0.759690 -- F1: 0.765625 -- AUC: 0.836920
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[199, 161, 39, 44, 76, 218, 27, 141, 114, 97, 61, 225, 72, 31, 73, 163, 81, 22, 195, 236, 111, 202, 228, 25, 156, 137, 54, 146, 6, 125, 3, 17, 178, 123, 133, 215, 90, 96, 47, 162, 157, 70, 170, 128, 84, 122, 28, 224, 46, 206, 52, 230, 165, 158, 232, 93, 60, 16, 100, 174, 56, 159, 235, 35, 134, 124, 233, 10, 14, 62, 45, 101, 7, 149, 187, 4, 110, 129, 172, 77, 33, 20, 179, 116, 135, 186, 155, 75, 138, 176, 205, 106, 57, 55, 142, 200, 99, 40, 48, 145, 208, 126, 169, 197, 34, 63, 89, 196, 58, 203, 210, 167, 91, 214, 121, 92, 30, 109, 211, 87, 26, 32, 164, 67, 105, 69, 119, 36, 144, 219, 131, 160, 143, 29, 153, 80, 53, 104, 51, 103, 18, 118, 136, 152, 43, 11, 113, 154, 49, 24, 222, 191, 82, 71, 194, 229, 38, 8, 15, 88, 85, 212, 175, 132, 59, 192, 168, 112, 234, 13, 107, 151, 12, 177, 216, 5, 79, 21, 37, 190, 98, 68, 139, 184, 2, 108, 66, 207, 50, 198, 201, 223, 1, 115, 183, 220, 19, 64, 74, 231, 204, 127, 94, 185, 193, 95, 140, 227, 217, 117, 148, 173, 102, 189, 130, 226, 86, 65, 147, 182, 180, 78, 150, 171, 188, 9, 23, 120, 181, 42, 221, 166, 41, 83, 213, 209]
[1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]
[1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 6 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.359
[ 2 / 40 ] loss: 0.185
[ 3 / 40 ] loss: 0.468
[ 4 / 40 ] loss: 0.290
[ 5 / 40 ] loss: 0.386
[ 6 / 40 ] loss: 0.613
[ 7 / 40 ] loss: 0.362
[ 8 / 40 ] loss: 0.491
[ 9 / 40 ] loss: 0.436
[ 10 / 40 ] loss: 0.360
[ 11 / 40 ] loss: 0.521
[ 12 / 40 ] loss: 0.136
[ 13 / 40 ] loss: 0.285
[ 14 / 40 ] loss: 0.212
[ 15 / 40 ] loss: 0.187
[ 16 / 40 ] loss: 0.315
[ 17 / 40 ] loss: 0.259
[ 18 / 40 ] loss: 0.231
[ 19 / 40 ] loss: 0.267
[ 20 / 40 ] loss: 0.211
[ 21 / 40 ] loss: 0.273
[ 22 / 40 ] loss: 0.457
[ 23 / 40 ] loss: 0.324
[ 24 / 40 ] loss: 0.408
[ 25 / 40 ] loss: 0.311
[ 26 / 40 ] loss: 0.545
[ 27 / 40 ] loss: 0.366
[ 28 / 40 ] loss: 0.410
[ 29 / 40 ] loss: 0.286
[ 30 / 40 ] loss: 0.220
[ 31 / 40 ] loss: 0.643
[ 32 / 40 ] loss: 0.291
[ 33 / 40 ] loss: 0.251
[ 34 / 40 ] loss: 0.176
[ 35 / 40 ] loss: 0.463
[ 36 / 40 ] loss: 0.395
[ 37 / 40 ] loss: 0.462
[ 38 / 40 ] loss: 0.540
[ 39 / 40 ] loss: 0.341
[ 40 / 40 ] loss: 0.425
0.35401006899774073
Accuracy: 0.783898 -- Precision: 0.782609 -- Recall: 0.837209 -- F1: 0.808989 -- AUC: 0.860030
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[167, 201, 215, 156, 58, 81, 65, 172, 6, 226, 78, 197, 61, 18, 225, 98, 89, 102, 133, 50, 234, 103, 121, 200, 138, 86, 59, 47, 174, 176, 90, 187, 183, 162, 147, 94, 179, 95, 168, 85, 235, 119, 212, 88, 166, 52, 72, 97, 56, 29, 17, 49, 115, 13, 41, 63, 184, 27, 96, 100, 54, 31, 137, 148, 55, 30, 230, 154, 210, 87, 64, 186, 112, 51, 202, 9, 146, 93, 135, 12, 105, 223, 91, 160, 37, 163, 23, 84, 182, 128, 57, 124, 218, 169, 113, 205, 67, 140, 66, 80, 110, 32, 123, 150, 164, 92, 8, 136, 217, 11, 111, 196, 188, 109, 71, 45, 60, 83, 40, 145, 39, 127, 3, 69, 185, 199, 22, 120, 68, 142, 1, 193, 79, 233, 214, 107, 25, 227, 153, 157, 132, 70, 21, 149, 209, 104, 108, 192, 195, 152, 101, 118, 232, 74, 158, 82, 10, 42, 35, 62, 134, 224, 231, 203, 33, 141, 180, 53, 190, 194, 114, 36, 131, 198, 178, 165, 75, 221, 73, 206, 125, 219, 213, 7, 5, 222, 191, 15, 20, 44, 139, 159, 177, 4, 116, 204, 106, 14, 207, 16, 76, 236, 24, 151, 181, 173, 117, 130, 220, 99, 208, 38, 126, 171, 122, 77, 143, 26, 28, 129, 2, 216, 155, 48, 228, 170, 19, 34, 189, 46, 211, 144, 43, 229, 175, 161]
[1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1]
[1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 7 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.179
[ 2 / 40 ] loss: 0.359
[ 3 / 40 ] loss: 0.246
[ 4 / 40 ] loss: 0.347
[ 5 / 40 ] loss: 0.417
[ 6 / 40 ] loss: 0.338
[ 7 / 40 ] loss: 0.176
[ 8 / 40 ] loss: 0.263
[ 9 / 40 ] loss: 0.249
[ 10 / 40 ] loss: 0.369
[ 11 / 40 ] loss: 0.152
[ 12 / 40 ] loss: 0.269
[ 13 / 40 ] loss: 0.350
[ 14 / 40 ] loss: 0.197
[ 15 / 40 ] loss: 0.248
[ 16 / 40 ] loss: 0.118
[ 17 / 40 ] loss: 0.135
[ 18 / 40 ] loss: 0.186
[ 19 / 40 ] loss: 0.465
[ 20 / 40 ] loss: 0.134
[ 21 / 40 ] loss: 0.276
[ 22 / 40 ] loss: 0.675
[ 23 / 40 ] loss: 0.512
[ 24 / 40 ] loss: 0.596
[ 25 / 40 ] loss: 0.265
[ 26 / 40 ] loss: 0.157
[ 27 / 40 ] loss: 0.369
[ 28 / 40 ] loss: 0.483
[ 29 / 40 ] loss: 0.609
[ 30 / 40 ] loss: 0.231
[ 31 / 40 ] loss: 0.483
[ 32 / 40 ] loss: 0.301
[ 33 / 40 ] loss: 0.365
[ 34 / 40 ] loss: 0.167
[ 35 / 40 ] loss: 0.482
[ 36 / 40 ] loss: 0.160
[ 37 / 40 ] loss: 0.322
[ 38 / 40 ] loss: 0.391
[ 39 / 40 ] loss: 0.292
[ 40 / 40 ] loss: 0.421
0.31886130310595034
Accuracy: 0.677966 -- Precision: 0.646409 -- Recall: 0.906977 -- F1: 0.754839 -- AUC: 0.819242
========= epoch: 8 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.580
[ 2 / 40 ] loss: 0.332
[ 3 / 40 ] loss: 0.178
[ 4 / 40 ] loss: 0.380
[ 5 / 40 ] loss: 0.231
[ 6 / 40 ] loss: 0.210
[ 7 / 40 ] loss: 0.253
[ 8 / 40 ] loss: 0.283
[ 9 / 40 ] loss: 0.297
[ 10 / 40 ] loss: 0.194
[ 11 / 40 ] loss: 0.216
[ 12 / 40 ] loss: 0.387
[ 13 / 40 ] loss: 0.178
[ 14 / 40 ] loss: 0.369
[ 15 / 40 ] loss: 0.120
[ 16 / 40 ] loss: 0.228
[ 17 / 40 ] loss: 0.245
[ 18 / 40 ] loss: 0.121
[ 19 / 40 ] loss: 0.272
[ 20 / 40 ] loss: 0.249
[ 21 / 40 ] loss: 0.223
[ 22 / 40 ] loss: 0.306
[ 23 / 40 ] loss: 0.364
[ 24 / 40 ] loss: 0.324
[ 25 / 40 ] loss: 0.192
[ 26 / 40 ] loss: 0.513
[ 27 / 40 ] loss: 0.265
[ 28 / 40 ] loss: 0.207
[ 29 / 40 ] loss: 0.059
[ 30 / 40 ] loss: 0.296
[ 31 / 40 ] loss: 0.435
[ 32 / 40 ] loss: 0.785
[ 33 / 40 ] loss: 0.374
[ 34 / 40 ] loss: 0.223
[ 35 / 40 ] loss: 0.486
[ 36 / 40 ] loss: 0.288
[ 37 / 40 ] loss: 0.177
[ 38 / 40 ] loss: 0.209
[ 39 / 40 ] loss: 0.248
[ 40 / 40 ] loss: 0.258
0.2888884484767914
Accuracy: 0.762712 -- Precision: 0.854369 -- Recall: 0.682171 -- F1: 0.758621 -- AUC: 0.844382
========= epoch: 9 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.332
[ 2 / 40 ] loss: 0.345
[ 3 / 40 ] loss: 0.130
[ 4 / 40 ] loss: 0.312
[ 5 / 40 ] loss: 0.311
[ 6 / 40 ] loss: 0.077
[ 7 / 40 ] loss: 0.360
[ 8 / 40 ] loss: 0.141
[ 9 / 40 ] loss: 0.520
[ 10 / 40 ] loss: 0.162
[ 11 / 40 ] loss: 0.216
[ 12 / 40 ] loss: 0.443
[ 13 / 40 ] loss: 0.547
[ 14 / 40 ] loss: 0.195
[ 15 / 40 ] loss: 0.160
[ 16 / 40 ] loss: 0.239
[ 17 / 40 ] loss: 0.233
[ 18 / 40 ] loss: 0.647
[ 19 / 40 ] loss: 0.092
[ 20 / 40 ] loss: 0.182
[ 21 / 40 ] loss: 0.120
[ 22 / 40 ] loss: 0.121
[ 23 / 40 ] loss: 0.295
[ 24 / 40 ] loss: 0.128
[ 25 / 40 ] loss: 0.376
[ 26 / 40 ] loss: 0.296
[ 27 / 40 ] loss: 0.304
[ 28 / 40 ] loss: 0.088
[ 29 / 40 ] loss: 0.329
[ 30 / 40 ] loss: 0.280
[ 31 / 40 ] loss: 0.236
[ 32 / 40 ] loss: 0.127
[ 33 / 40 ] loss: 0.421
[ 34 / 40 ] loss: 0.352
[ 35 / 40 ] loss: 0.141
[ 36 / 40 ] loss: 0.085
[ 37 / 40 ] loss: 0.417
[ 38 / 40 ] loss: 0.311
[ 39 / 40 ] loss: 0.206
[ 40 / 40 ] loss: 0.340
0.2653967125341296
Accuracy: 0.741525 -- Precision: 0.746377 -- Recall: 0.798450 -- F1: 0.771536 -- AUC: 0.846845
========= epoch: 10 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.075
[ 2 / 40 ] loss: 0.316
[ 3 / 40 ] loss: 0.203
[ 4 / 40 ] loss: 0.114
[ 5 / 40 ] loss: 0.237
[ 6 / 40 ] loss: 0.243
[ 7 / 40 ] loss: 0.069
[ 8 / 40 ] loss: 0.374
[ 9 / 40 ] loss: 0.180
[ 10 / 40 ] loss: 0.226
[ 11 / 40 ] loss: 0.121
[ 12 / 40 ] loss: 0.176
[ 13 / 40 ] loss: 0.227
[ 14 / 40 ] loss: 0.332
[ 15 / 40 ] loss: 0.301
[ 16 / 40 ] loss: 0.060
[ 17 / 40 ] loss: 0.091
[ 18 / 40 ] loss: 0.239
[ 19 / 40 ] loss: 0.191
[ 20 / 40 ] loss: 0.301
[ 21 / 40 ] loss: 0.058
[ 22 / 40 ] loss: 0.140
[ 23 / 40 ] loss: 0.292
[ 24 / 40 ] loss: 0.177
[ 25 / 40 ] loss: 0.087
[ 26 / 40 ] loss: 0.312
[ 27 / 40 ] loss: 0.514
[ 28 / 40 ] loss: 0.241
[ 29 / 40 ] loss: 0.074
[ 30 / 40 ] loss: 0.242
[ 31 / 40 ] loss: 0.128
[ 32 / 40 ] loss: 0.189
[ 33 / 40 ] loss: 0.134
[ 34 / 40 ] loss: 0.103
[ 35 / 40 ] loss: 0.057
[ 36 / 40 ] loss: 0.250
[ 37 / 40 ] loss: 0.336
[ 38 / 40 ] loss: 0.563
[ 39 / 40 ] loss: 0.425
[ 40 / 40 ] loss: 0.046
0.21116559188812972
Accuracy: 0.783898 -- Precision: 0.825000 -- Recall: 0.767442 -- F1: 0.795181 -- AUC: 0.864812
========= epoch: 11 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.282
[ 2 / 40 ] loss: 0.084
[ 3 / 40 ] loss: 0.239
[ 4 / 40 ] loss: 0.190
[ 5 / 40 ] loss: 0.134
[ 6 / 40 ] loss: 0.420
[ 7 / 40 ] loss: 0.087
[ 8 / 40 ] loss: 0.078
[ 9 / 40 ] loss: 0.106
[ 10 / 40 ] loss: 0.050
[ 11 / 40 ] loss: 0.048
[ 12 / 40 ] loss: 0.103
[ 13 / 40 ] loss: 0.383
[ 14 / 40 ] loss: 0.203
[ 15 / 40 ] loss: 0.497
[ 16 / 40 ] loss: 0.076
[ 17 / 40 ] loss: 0.339
[ 18 / 40 ] loss: 0.153
[ 19 / 40 ] loss: 0.502
[ 20 / 40 ] loss: 0.069
[ 21 / 40 ] loss: 0.222
[ 22 / 40 ] loss: 0.079
[ 23 / 40 ] loss: 0.191
[ 24 / 40 ] loss: 0.082
[ 25 / 40 ] loss: 0.127
[ 26 / 40 ] loss: 0.170
[ 27 / 40 ] loss: 0.063
[ 28 / 40 ] loss: 0.051
[ 29 / 40 ] loss: 0.104
[ 30 / 40 ] loss: 0.188
[ 31 / 40 ] loss: 0.089
[ 32 / 40 ] loss: 0.295
[ 33 / 40 ] loss: 0.196
[ 34 / 40 ] loss: 0.140
[ 35 / 40 ] loss: 0.049
[ 36 / 40 ] loss: 0.645
[ 37 / 40 ] loss: 0.256
[ 38 / 40 ] loss: 0.090
[ 39 / 40 ] loss: 0.229
[ 40 / 40 ] loss: 0.122
0.18569195112213494
Accuracy: 0.792373 -- Precision: 0.777778 -- Recall: 0.868217 -- F1: 0.820513 -- AUC: 0.844816
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[150, 217, 209, 149, 197, 208, 85, 24, 108, 229, 30, 62, 212, 215, 6, 213, 226, 18, 53, 9, 110, 173, 29, 195, 223, 164, 3, 96, 55, 207, 174, 105, 14, 11, 211, 145, 58, 143, 4, 99, 161, 135, 59, 171, 88, 77, 42, 31, 124, 163, 203, 177, 121, 180, 83, 168, 152, 146, 27, 194, 219, 61, 139, 1, 125, 157, 52, 72, 165, 158, 112, 218, 138, 97, 233, 178, 205, 117, 28, 69, 45, 193, 154, 144, 175, 127, 227, 35, 100, 153, 56, 65, 232, 236, 136, 201, 220, 43, 22, 159, 75, 142, 41, 115, 199, 183, 147, 114, 130, 79, 235, 90, 66, 222, 23, 71, 221, 101, 148, 74, 94, 103, 196, 104, 206, 123, 91, 189, 132, 116, 216, 167, 111, 64, 134, 109, 113, 188, 63, 160, 102, 16, 7, 176, 186, 17, 50, 179, 73, 13, 49, 156, 166, 60, 119, 40, 182, 33, 204, 81, 10, 192, 20, 234, 86, 2, 78, 80, 133, 95, 47, 5, 162, 191, 84, 231, 170, 224, 98, 89, 92, 106, 39, 38, 70, 155, 202, 37, 46, 190, 172, 48, 131, 128, 214, 181, 200, 185, 225, 54, 126, 210, 34, 118, 198, 44, 228, 169, 93, 230, 129, 51, 26, 141, 76, 107, 15, 25, 21, 140, 82, 57, 120, 32, 68, 87, 36, 12, 187, 19, 137, 184, 151, 8, 122, 67]
[1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0]
[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 12 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.039
[ 2 / 40 ] loss: 0.151
[ 3 / 40 ] loss: 0.069
[ 4 / 40 ] loss: 0.220
[ 5 / 40 ] loss: 0.174
[ 6 / 40 ] loss: 0.075
[ 7 / 40 ] loss: 0.189
[ 8 / 40 ] loss: 0.061
[ 9 / 40 ] loss: 0.089
[ 10 / 40 ] loss: 0.339
[ 11 / 40 ] loss: 0.037
[ 12 / 40 ] loss: 0.094
[ 13 / 40 ] loss: 0.203
[ 14 / 40 ] loss: 0.280
[ 15 / 40 ] loss: 0.074
[ 16 / 40 ] loss: 0.194
[ 17 / 40 ] loss: 0.539
[ 18 / 40 ] loss: 0.170
[ 19 / 40 ] loss: 0.105
[ 20 / 40 ] loss: 0.030
[ 21 / 40 ] loss: 0.328
[ 22 / 40 ] loss: 0.232
[ 23 / 40 ] loss: 0.091
[ 24 / 40 ] loss: 0.091
[ 25 / 40 ] loss: 0.077
[ 26 / 40 ] loss: 0.307
[ 27 / 40 ] loss: 0.171
[ 28 / 40 ] loss: 0.532
[ 29 / 40 ] loss: 0.145
[ 30 / 40 ] loss: 0.200
[ 31 / 40 ] loss: 0.306
[ 32 / 40 ] loss: 0.085
[ 33 / 40 ] loss: 0.289
[ 34 / 40 ] loss: 0.377
[ 35 / 40 ] loss: 0.393
[ 36 / 40 ] loss: 0.280
[ 37 / 40 ] loss: 0.054
[ 38 / 40 ] loss: 0.213
[ 39 / 40 ] loss: 0.067
[ 40 / 40 ] loss: 0.141
0.18774168328382074
Accuracy: 0.771186 -- Precision: 0.820513 -- Recall: 0.744186 -- F1: 0.780488 -- AUC: 0.824603
========= epoch: 13 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.077
[ 2 / 40 ] loss: 0.055
[ 3 / 40 ] loss: 0.041
[ 4 / 40 ] loss: 0.315
[ 5 / 40 ] loss: 0.356
[ 6 / 40 ] loss: 0.244
[ 7 / 40 ] loss: 0.622
[ 8 / 40 ] loss: 0.249
[ 9 / 40 ] loss: 0.146
[ 10 / 40 ] loss: 0.123
[ 11 / 40 ] loss: 0.172
[ 12 / 40 ] loss: 0.231
[ 13 / 40 ] loss: 0.388
[ 14 / 40 ] loss: 0.209
[ 15 / 40 ] loss: 0.163
[ 16 / 40 ] loss: 0.355
[ 17 / 40 ] loss: 0.206
[ 18 / 40 ] loss: 0.142
[ 19 / 40 ] loss: 0.137
[ 20 / 40 ] loss: 0.105
[ 21 / 40 ] loss: 0.150
[ 22 / 40 ] loss: 0.231
[ 23 / 40 ] loss: 0.299
[ 24 / 40 ] loss: 0.132
[ 25 / 40 ] loss: 0.082
[ 26 / 40 ] loss: 0.174
[ 27 / 40 ] loss: 0.051
[ 28 / 40 ] loss: 0.045
[ 29 / 40 ] loss: 0.032
[ 30 / 40 ] loss: 0.183
[ 31 / 40 ] loss: 0.274
[ 32 / 40 ] loss: 0.056
[ 33 / 40 ] loss: 0.113
[ 34 / 40 ] loss: 0.144
[ 35 / 40 ] loss: 0.266
[ 36 / 40 ] loss: 0.360
[ 37 / 40 ] loss: 0.537
[ 38 / 40 ] loss: 0.189
[ 39 / 40 ] loss: 0.077
[ 40 / 40 ] loss: 0.047
0.19448241023346782
Accuracy: 0.783898 -- Precision: 0.782609 -- Recall: 0.837209 -- F1: 0.808989 -- AUC: 0.839817
========= epoch: 14 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.252
[ 2 / 40 ] loss: 0.219
[ 3 / 40 ] loss: 0.158
[ 4 / 40 ] loss: 0.048
[ 5 / 40 ] loss: 0.203
[ 6 / 40 ] loss: 0.062
[ 7 / 40 ] loss: 0.178
[ 8 / 40 ] loss: 0.323
[ 9 / 40 ] loss: 0.107
[ 10 / 40 ] loss: 0.107
[ 11 / 40 ] loss: 0.124
[ 12 / 40 ] loss: 0.130
[ 13 / 40 ] loss: 0.209
[ 14 / 40 ] loss: 0.095
[ 15 / 40 ] loss: 0.285
[ 16 / 40 ] loss: 0.345
[ 17 / 40 ] loss: 0.035
[ 18 / 40 ] loss: 0.095
[ 19 / 40 ] loss: 0.053
[ 20 / 40 ] loss: 0.053
[ 21 / 40 ] loss: 0.126
[ 22 / 40 ] loss: 0.092
[ 23 / 40 ] loss: 0.045
[ 24 / 40 ] loss: 0.273
[ 25 / 40 ] loss: 0.314
[ 26 / 40 ] loss: 0.256
[ 27 / 40 ] loss: 0.359
[ 28 / 40 ] loss: 0.078
[ 29 / 40 ] loss: 0.059
[ 30 / 40 ] loss: 0.156
[ 31 / 40 ] loss: 0.129
[ 32 / 40 ] loss: 0.182
[ 33 / 40 ] loss: 0.268
[ 34 / 40 ] loss: 0.353
[ 35 / 40 ] loss: 0.099
[ 36 / 40 ] loss: 0.199
[ 37 / 40 ] loss: 0.063
[ 38 / 40 ] loss: 0.077
[ 39 / 40 ] loss: 0.080
[ 40 / 40 ] loss: 0.030
0.15800411035306752
Accuracy: 0.775424 -- Precision: 0.792308 -- Recall: 0.798450 -- F1: 0.795367 -- AUC: 0.851409
========= epoch: 15 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.052
[ 2 / 40 ] loss: 0.192
[ 3 / 40 ] loss: 0.178
[ 4 / 40 ] loss: 0.374
[ 5 / 40 ] loss: 0.068
[ 6 / 40 ] loss: 0.029
[ 7 / 40 ] loss: 0.024
[ 8 / 40 ] loss: 0.428
[ 9 / 40 ] loss: 0.075
[ 10 / 40 ] loss: 0.115
[ 11 / 40 ] loss: 0.067
[ 12 / 40 ] loss: 0.130
[ 13 / 40 ] loss: 0.200
[ 14 / 40 ] loss: 0.028
[ 15 / 40 ] loss: 0.056
[ 16 / 40 ] loss: 0.139
[ 17 / 40 ] loss: 0.230
[ 18 / 40 ] loss: 0.244
[ 19 / 40 ] loss: 0.241
[ 20 / 40 ] loss: 0.058
[ 21 / 40 ] loss: 0.125
[ 22 / 40 ] loss: 0.028
[ 23 / 40 ] loss: 0.208
[ 24 / 40 ] loss: 0.194
[ 25 / 40 ] loss: 0.040
[ 26 / 40 ] loss: 0.078
[ 27 / 40 ] loss: 0.284
[ 28 / 40 ] loss: 0.045
[ 29 / 40 ] loss: 0.124
[ 30 / 40 ] loss: 0.023
[ 31 / 40 ] loss: 0.242
[ 32 / 40 ] loss: 0.281
[ 33 / 40 ] loss: 0.074
[ 34 / 40 ] loss: 0.137
[ 35 / 40 ] loss: 0.043
[ 36 / 40 ] loss: 0.112
[ 37 / 40 ] loss: 0.137
[ 38 / 40 ] loss: 0.047
[ 39 / 40 ] loss: 0.274
[ 40 / 40 ] loss: 0.052
0.13695650645531715
Accuracy: 0.762712 -- Precision: 0.755245 -- Recall: 0.837209 -- F1: 0.794118 -- AUC: 0.830979
========= epoch: 16 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.048
[ 2 / 40 ] loss: 0.069
[ 3 / 40 ] loss: 0.063
[ 4 / 40 ] loss: 0.401
[ 5 / 40 ] loss: 0.148
[ 6 / 40 ] loss: 0.117
[ 7 / 40 ] loss: 0.036
[ 8 / 40 ] loss: 0.170
[ 9 / 40 ] loss: 0.091
[ 10 / 40 ] loss: 0.055
[ 11 / 40 ] loss: 0.061
[ 12 / 40 ] loss: 0.097
[ 13 / 40 ] loss: 0.108
[ 14 / 40 ] loss: 0.028
[ 15 / 40 ] loss: 0.354
[ 16 / 40 ] loss: 0.198
[ 17 / 40 ] loss: 0.033
[ 18 / 40 ] loss: 0.214
[ 19 / 40 ] loss: 0.051
[ 20 / 40 ] loss: 0.035
[ 21 / 40 ] loss: 0.068
[ 22 / 40 ] loss: 0.052
[ 23 / 40 ] loss: 0.242
[ 24 / 40 ] loss: 0.022
[ 25 / 40 ] loss: 0.092
[ 26 / 40 ] loss: 0.109
[ 27 / 40 ] loss: 0.019
[ 28 / 40 ] loss: 0.400
[ 29 / 40 ] loss: 0.117
[ 30 / 40 ] loss: 0.099
[ 31 / 40 ] loss: 0.112
[ 32 / 40 ] loss: 0.234
[ 33 / 40 ] loss: 0.071
[ 34 / 40 ] loss: 0.068
[ 35 / 40 ] loss: 0.228
[ 36 / 40 ] loss: 0.042
[ 37 / 40 ] loss: 0.167
[ 38 / 40 ] loss: 0.071
[ 39 / 40 ] loss: 0.019
[ 40 / 40 ] loss: 0.093
0.11762943249195815
Accuracy: 0.758475 -- Precision: 0.740000 -- Recall: 0.860465 -- F1: 0.795699 -- AUC: 0.833152
========= epoch: 17 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.097
[ 2 / 40 ] loss: 0.103
[ 3 / 40 ] loss: 0.016
[ 4 / 40 ] loss: 0.108
[ 5 / 40 ] loss: 0.396
[ 6 / 40 ] loss: 0.184
[ 7 / 40 ] loss: 0.026
[ 8 / 40 ] loss: 0.080
[ 9 / 40 ] loss: 0.054
[ 10 / 40 ] loss: 0.129
[ 11 / 40 ] loss: 0.077
[ 12 / 40 ] loss: 0.025
[ 13 / 40 ] loss: 0.026
[ 14 / 40 ] loss: 0.437
[ 15 / 40 ] loss: 0.215
[ 16 / 40 ] loss: 0.078
[ 17 / 40 ] loss: 0.073
[ 18 / 40 ] loss: 0.185
[ 19 / 40 ] loss: 0.196
[ 20 / 40 ] loss: 0.036
[ 21 / 40 ] loss: 0.063
[ 22 / 40 ] loss: 0.081
[ 23 / 40 ] loss: 0.212
[ 24 / 40 ] loss: 0.061
[ 25 / 40 ] loss: 0.039
[ 26 / 40 ] loss: 0.077
[ 27 / 40 ] loss: 0.023
[ 28 / 40 ] loss: 0.293
[ 29 / 40 ] loss: 0.068
[ 30 / 40 ] loss: 0.179
[ 31 / 40 ] loss: 0.167
[ 32 / 40 ] loss: 0.393
[ 33 / 40 ] loss: 0.014
[ 34 / 40 ] loss: 0.182
[ 35 / 40 ] loss: 0.353
[ 36 / 40 ] loss: 0.131
[ 37 / 40 ] loss: 0.017
[ 38 / 40 ] loss: 0.265
[ 39 / 40 ] loss: 0.096
[ 40 / 40 ] loss: 0.369
0.1406298924703151
Accuracy: 0.775424 -- Precision: 0.787879 -- Recall: 0.806202 -- F1: 0.796935 -- AUC: 0.838803
========= epoch: 18 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.215
[ 2 / 40 ] loss: 0.035
[ 3 / 40 ] loss: 0.040
[ 4 / 40 ] loss: 0.042
[ 5 / 40 ] loss: 0.048
[ 6 / 40 ] loss: 0.091
[ 7 / 40 ] loss: 0.138
[ 8 / 40 ] loss: 0.139
[ 9 / 40 ] loss: 0.042
[ 10 / 40 ] loss: 0.027
[ 11 / 40 ] loss: 0.093
[ 12 / 40 ] loss: 0.255
[ 13 / 40 ] loss: 0.047
[ 14 / 40 ] loss: 0.192
[ 15 / 40 ] loss: 0.110
[ 16 / 40 ] loss: 0.248
[ 17 / 40 ] loss: 0.189
[ 18 / 40 ] loss: 0.219
[ 19 / 40 ] loss: 0.180
[ 20 / 40 ] loss: 0.119
[ 21 / 40 ] loss: 0.041
[ 22 / 40 ] loss: 0.189
[ 23 / 40 ] loss: 0.095
[ 24 / 40 ] loss: 0.078
[ 25 / 40 ] loss: 0.205
[ 26 / 40 ] loss: 0.099
[ 27 / 40 ] loss: 0.106
[ 28 / 40 ] loss: 0.105
[ 29 / 40 ] loss: 0.118
[ 30 / 40 ] loss: 0.469
[ 31 / 40 ] loss: 0.056
[ 32 / 40 ] loss: 0.145
[ 33 / 40 ] loss: 0.415
[ 34 / 40 ] loss: 0.058
[ 35 / 40 ] loss: 0.221
[ 36 / 40 ] loss: 0.265
[ 37 / 40 ] loss: 0.163
[ 38 / 40 ] loss: 0.067
[ 39 / 40 ] loss: 0.042
[ 40 / 40 ] loss: 0.027
0.13582687033340335
Accuracy: 0.779661 -- Precision: 0.789474 -- Recall: 0.813953 -- F1: 0.801527 -- AUC: 0.861842
========= epoch: 19 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.077
[ 2 / 40 ] loss: 0.052
[ 3 / 40 ] loss: 0.016
[ 4 / 40 ] loss: 0.064
[ 5 / 40 ] loss: 0.050
[ 6 / 40 ] loss: 0.068
[ 7 / 40 ] loss: 0.243
[ 8 / 40 ] loss: 0.031
[ 9 / 40 ] loss: 0.018
[ 10 / 40 ] loss: 0.422
[ 11 / 40 ] loss: 0.022
[ 12 / 40 ] loss: 0.026
[ 13 / 40 ] loss: 0.026
[ 14 / 40 ] loss: 0.027
[ 15 / 40 ] loss: 0.015
[ 16 / 40 ] loss: 0.228
[ 17 / 40 ] loss: 0.017
[ 18 / 40 ] loss: 0.230
[ 19 / 40 ] loss: 0.097
[ 20 / 40 ] loss: 0.222
[ 21 / 40 ] loss: 0.411
[ 22 / 40 ] loss: 0.233
[ 23 / 40 ] loss: 0.089
[ 24 / 40 ] loss: 0.076
[ 25 / 40 ] loss: 0.149
[ 26 / 40 ] loss: 0.198
[ 27 / 40 ] loss: 0.190
[ 28 / 40 ] loss: 0.362
[ 29 / 40 ] loss: 0.460
[ 30 / 40 ] loss: 0.084
[ 31 / 40 ] loss: 0.187
[ 32 / 40 ] loss: 0.086
[ 33 / 40 ] loss: 0.187
[ 34 / 40 ] loss: 0.047
[ 35 / 40 ] loss: 0.240
[ 36 / 40 ] loss: 0.257
[ 37 / 40 ] loss: 0.349
[ 38 / 40 ] loss: 0.093
[ 39 / 40 ] loss: 0.226
[ 40 / 40 ] loss: 0.145
0.15041766269132495
Accuracy: 0.783898 -- Precision: 0.825000 -- Recall: 0.767442 -- F1: 0.795181 -- AUC: 0.827284
========= epoch: 20 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.188
[ 2 / 40 ] loss: 0.350
[ 3 / 40 ] loss: 0.041
[ 4 / 40 ] loss: 0.103
[ 5 / 40 ] loss: 0.046
[ 6 / 40 ] loss: 0.167
[ 7 / 40 ] loss: 0.074
[ 8 / 40 ] loss: 0.022
[ 9 / 40 ] loss: 0.028
[ 10 / 40 ] loss: 0.396
[ 11 / 40 ] loss: 0.143
[ 12 / 40 ] loss: 0.236
[ 13 / 40 ] loss: 0.180
[ 14 / 40 ] loss: 0.132
[ 15 / 40 ] loss: 0.173
[ 16 / 40 ] loss: 0.105
[ 17 / 40 ] loss: 0.242
[ 18 / 40 ] loss: 0.170
[ 19 / 40 ] loss: 0.195
[ 20 / 40 ] loss: 0.230
[ 21 / 40 ] loss: 0.060
[ 22 / 40 ] loss: 0.048
[ 23 / 40 ] loss: 0.026
[ 24 / 40 ] loss: 0.076
[ 25 / 40 ] loss: 0.320
[ 26 / 40 ] loss: 0.032
[ 27 / 40 ] loss: 0.194
[ 28 / 40 ] loss: 0.121
[ 29 / 40 ] loss: 0.316
[ 30 / 40 ] loss: 0.094
[ 31 / 40 ] loss: 0.378
[ 32 / 40 ] loss: 0.043
[ 33 / 40 ] loss: 0.059
[ 34 / 40 ] loss: 0.119
[ 35 / 40 ] loss: 0.082
[ 36 / 40 ] loss: 0.060
[ 37 / 40 ] loss: 0.127
[ 38 / 40 ] loss: 0.067
[ 39 / 40 ] loss: 0.170
[ 40 / 40 ] loss: 0.121
0.14343443200923503
Accuracy: 0.771186 -- Precision: 0.804878 -- Recall: 0.767442 -- F1: 0.785714 -- AUC: 0.811056
========= epoch: 21 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.019
[ 2 / 40 ] loss: 0.108
[ 3 / 40 ] loss: 0.098
[ 4 / 40 ] loss: 0.081
[ 5 / 40 ] loss: 0.055
[ 6 / 40 ] loss: 0.034
[ 7 / 40 ] loss: 0.091
[ 8 / 40 ] loss: 0.041
[ 9 / 40 ] loss: 0.231
[ 10 / 40 ] loss: 0.091
[ 11 / 40 ] loss: 0.261
[ 12 / 40 ] loss: 0.029
[ 13 / 40 ] loss: 0.091
[ 14 / 40 ] loss: 0.202
[ 15 / 40 ] loss: 0.216
[ 16 / 40 ] loss: 0.226
[ 17 / 40 ] loss: 0.015
[ 18 / 40 ] loss: 0.205
[ 19 / 40 ] loss: 0.060
[ 20 / 40 ] loss: 0.021
[ 21 / 40 ] loss: 0.121
[ 22 / 40 ] loss: 0.205
[ 23 / 40 ] loss: 0.014
[ 24 / 40 ] loss: 0.069
[ 25 / 40 ] loss: 0.185
[ 26 / 40 ] loss: 0.202
[ 27 / 40 ] loss: 0.067
[ 28 / 40 ] loss: 0.018
[ 29 / 40 ] loss: 0.046
[ 30 / 40 ] loss: 0.185
[ 31 / 40 ] loss: 0.234
[ 32 / 40 ] loss: 0.187
[ 33 / 40 ] loss: 0.088
[ 34 / 40 ] loss: 0.090
[ 35 / 40 ] loss: 0.238
[ 36 / 40 ] loss: 0.282
[ 37 / 40 ] loss: 0.105
[ 38 / 40 ] loss: 0.061
[ 39 / 40 ] loss: 0.108
[ 40 / 40 ] loss: 0.103
0.11954294464085251
Accuracy: 0.779661 -- Precision: 0.808000 -- Recall: 0.782946 -- F1: 0.795276 -- AUC: 0.842208
========= epoch: 22 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.049
[ 2 / 40 ] loss: 0.025
[ 3 / 40 ] loss: 0.069
[ 4 / 40 ] loss: 0.068
[ 5 / 40 ] loss: 0.143
[ 6 / 40 ] loss: 0.016
[ 7 / 40 ] loss: 0.109
[ 8 / 40 ] loss: 0.095
[ 9 / 40 ] loss: 0.028
[ 10 / 40 ] loss: 0.149
[ 11 / 40 ] loss: 0.019
[ 12 / 40 ] loss: 0.069
[ 13 / 40 ] loss: 0.042
[ 14 / 40 ] loss: 0.098
[ 15 / 40 ] loss: 0.042
[ 16 / 40 ] loss: 0.052
[ 17 / 40 ] loss: 0.055
[ 18 / 40 ] loss: 0.251
[ 19 / 40 ] loss: 0.066
[ 20 / 40 ] loss: 0.012
[ 21 / 40 ] loss: 0.107
[ 22 / 40 ] loss: 0.239
[ 23 / 40 ] loss: 0.173
[ 24 / 40 ] loss: 0.040
[ 25 / 40 ] loss: 0.262
[ 26 / 40 ] loss: 0.087
[ 27 / 40 ] loss: 0.187
[ 28 / 40 ] loss: 0.016
[ 29 / 40 ] loss: 0.022
[ 30 / 40 ] loss: 0.072
[ 31 / 40 ] loss: 0.242
[ 32 / 40 ] loss: 0.294
[ 33 / 40 ] loss: 0.229
[ 34 / 40 ] loss: 0.155
[ 35 / 40 ] loss: 0.033
[ 36 / 40 ] loss: 0.018
[ 37 / 40 ] loss: 0.042
[ 38 / 40 ] loss: 0.109
[ 39 / 40 ] loss: 0.019
[ 40 / 40 ] loss: 0.032
0.0959049343597144
Accuracy: 0.771186 -- Precision: 0.795276 -- Recall: 0.782946 -- F1: 0.789062 -- AUC: 0.815185
========= epoch: 23 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.058
[ 2 / 40 ] loss: 0.213
[ 3 / 40 ] loss: 0.176
[ 4 / 40 ] loss: 0.199
[ 5 / 40 ] loss: 0.041
[ 6 / 40 ] loss: 0.206
[ 7 / 40 ] loss: 0.035
[ 8 / 40 ] loss: 0.024
[ 9 / 40 ] loss: 0.044
[ 10 / 40 ] loss: 0.308
[ 11 / 40 ] loss: 0.122
[ 12 / 40 ] loss: 0.104
[ 13 / 40 ] loss: 0.071
[ 14 / 40 ] loss: 0.115
[ 15 / 40 ] loss: 0.062
[ 16 / 40 ] loss: 0.047
[ 17 / 40 ] loss: 0.046
[ 18 / 40 ] loss: 0.066
[ 19 / 40 ] loss: 0.064
[ 20 / 40 ] loss: 0.013
[ 21 / 40 ] loss: 0.014
[ 22 / 40 ] loss: 0.076
[ 23 / 40 ] loss: 0.180
[ 24 / 40 ] loss: 0.127
[ 25 / 40 ] loss: 0.064
[ 26 / 40 ] loss: 0.085
[ 27 / 40 ] loss: 0.019
[ 28 / 40 ] loss: 0.209
[ 29 / 40 ] loss: 0.066
[ 30 / 40 ] loss: 0.157
[ 31 / 40 ] loss: 0.200
[ 32 / 40 ] loss: 0.012
[ 33 / 40 ] loss: 0.043
[ 34 / 40 ] loss: 0.014
[ 35 / 40 ] loss: 0.039
[ 36 / 40 ] loss: 0.197
[ 37 / 40 ] loss: 0.015
[ 38 / 40 ] loss: 0.018
[ 39 / 40 ] loss: 0.017
[ 40 / 40 ] loss: 0.015
0.08943807017058134
Accuracy: 0.766949 -- Precision: 0.793651 -- Recall: 0.775194 -- F1: 0.784314 -- AUC: 0.787220
========= epoch: 24 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.013
[ 2 / 40 ] loss: 0.037
[ 3 / 40 ] loss: 0.047
[ 4 / 40 ] loss: 0.027
[ 5 / 40 ] loss: 0.044
[ 6 / 40 ] loss: 0.010
[ 7 / 40 ] loss: 0.031
[ 8 / 40 ] loss: 0.111
[ 9 / 40 ] loss: 0.120
[ 10 / 40 ] loss: 0.033
[ 11 / 40 ] loss: 0.013
[ 12 / 40 ] loss: 0.052
[ 13 / 40 ] loss: 0.207
[ 14 / 40 ] loss: 0.113
[ 15 / 40 ] loss: 0.052
[ 16 / 40 ] loss: 0.011
[ 17 / 40 ] loss: 0.032
[ 18 / 40 ] loss: 0.142
[ 19 / 40 ] loss: 0.045
[ 20 / 40 ] loss: 0.015
[ 21 / 40 ] loss: 0.053
[ 22 / 40 ] loss: 0.012
[ 23 / 40 ] loss: 0.237
[ 24 / 40 ] loss: 0.011
[ 25 / 40 ] loss: 0.205
[ 26 / 40 ] loss: 0.080
[ 27 / 40 ] loss: 0.024
[ 28 / 40 ] loss: 0.219
[ 29 / 40 ] loss: 0.035
[ 30 / 40 ] loss: 0.021
[ 31 / 40 ] loss: 0.065
[ 32 / 40 ] loss: 0.200
[ 33 / 40 ] loss: 0.009
[ 34 / 40 ] loss: 0.030
[ 35 / 40 ] loss: 0.043
[ 36 / 40 ] loss: 0.082
[ 37 / 40 ] loss: 0.257
[ 38 / 40 ] loss: 0.008
[ 39 / 40 ] loss: 0.016
[ 40 / 40 ] loss: 0.415
0.07942381943576038
Accuracy: 0.771186 -- Precision: 0.790698 -- Recall: 0.790698 -- F1: 0.790698 -- AUC: 0.804173
========= epoch: 25 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.080
[ 2 / 40 ] loss: 0.008
[ 3 / 40 ] loss: 0.010
[ 4 / 40 ] loss: 0.191
[ 5 / 40 ] loss: 0.011
[ 6 / 40 ] loss: 0.010
[ 7 / 40 ] loss: 0.276
[ 8 / 40 ] loss: 0.184
[ 9 / 40 ] loss: 0.270
[ 10 / 40 ] loss: 0.087
[ 11 / 40 ] loss: 0.012
[ 12 / 40 ] loss: 0.024
[ 13 / 40 ] loss: 0.013
[ 14 / 40 ] loss: 0.014
[ 15 / 40 ] loss: 0.013
[ 16 / 40 ] loss: 0.012
[ 17 / 40 ] loss: 0.235
[ 18 / 40 ] loss: 0.016
[ 19 / 40 ] loss: 0.043
[ 20 / 40 ] loss: 0.139
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.045
[ 23 / 40 ] loss: 0.016
[ 24 / 40 ] loss: 0.206
[ 25 / 40 ] loss: 0.040
[ 26 / 40 ] loss: 0.026
[ 27 / 40 ] loss: 0.102
[ 28 / 40 ] loss: 0.200
[ 29 / 40 ] loss: 0.067
[ 30 / 40 ] loss: 0.014
[ 31 / 40 ] loss: 0.019
[ 32 / 40 ] loss: 0.010
[ 33 / 40 ] loss: 0.012
[ 34 / 40 ] loss: 0.027
[ 35 / 40 ] loss: 0.013
[ 36 / 40 ] loss: 0.012
[ 37 / 40 ] loss: 0.092
[ 38 / 40 ] loss: 0.020
[ 39 / 40 ] loss: 0.012
[ 40 / 40 ] loss: 0.236
0.07062202063389122
Accuracy: 0.775424 -- Precision: 0.783582 -- Recall: 0.813953 -- F1: 0.798479 -- AUC: 0.819460
========= epoch: 26 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.039
[ 2 / 40 ] loss: 0.011
[ 3 / 40 ] loss: 0.044
[ 4 / 40 ] loss: 0.024
[ 5 / 40 ] loss: 0.204
[ 6 / 40 ] loss: 0.012
[ 7 / 40 ] loss: 0.205
[ 8 / 40 ] loss: 0.008
[ 9 / 40 ] loss: 0.010
[ 10 / 40 ] loss: 0.059
[ 11 / 40 ] loss: 0.008
[ 12 / 40 ] loss: 0.040
[ 13 / 40 ] loss: 0.014
[ 14 / 40 ] loss: 0.014
[ 15 / 40 ] loss: 0.052
[ 16 / 40 ] loss: 0.216
[ 17 / 40 ] loss: 0.021
[ 18 / 40 ] loss: 0.012
[ 19 / 40 ] loss: 0.043
[ 20 / 40 ] loss: 0.008
[ 21 / 40 ] loss: 0.011
[ 22 / 40 ] loss: 0.008
[ 23 / 40 ] loss: 0.187
[ 24 / 40 ] loss: 0.159
[ 25 / 40 ] loss: 0.013
[ 26 / 40 ] loss: 0.006
[ 27 / 40 ] loss: 0.011
[ 28 / 40 ] loss: 0.057
[ 29 / 40 ] loss: 0.016
[ 30 / 40 ] loss: 0.131
[ 31 / 40 ] loss: 0.209
[ 32 / 40 ] loss: 0.044
[ 33 / 40 ] loss: 0.011
[ 34 / 40 ] loss: 0.085
[ 35 / 40 ] loss: 0.010
[ 36 / 40 ] loss: 0.011
[ 37 / 40 ] loss: 0.093
[ 38 / 40 ] loss: 0.071
[ 39 / 40 ] loss: 0.184
[ 40 / 40 ] loss: 0.009
0.05932437190786004
Accuracy: 0.792373 -- Precision: 0.794118 -- Recall: 0.837209 -- F1: 0.815094 -- AUC: 0.819387
========= epoch: 27 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.037
[ 2 / 40 ] loss: 0.019
[ 3 / 40 ] loss: 0.044
[ 4 / 40 ] loss: 0.020
[ 5 / 40 ] loss: 0.052
[ 6 / 40 ] loss: 0.037
[ 7 / 40 ] loss: 0.008
[ 8 / 40 ] loss: 0.036
[ 9 / 40 ] loss: 0.016
[ 10 / 40 ] loss: 0.047
[ 11 / 40 ] loss: 0.008
[ 12 / 40 ] loss: 0.010
[ 13 / 40 ] loss: 0.052
[ 14 / 40 ] loss: 0.016
[ 15 / 40 ] loss: 0.056
[ 16 / 40 ] loss: 0.060
[ 17 / 40 ] loss: 0.036
[ 18 / 40 ] loss: 0.068
[ 19 / 40 ] loss: 0.008
[ 20 / 40 ] loss: 0.008
[ 21 / 40 ] loss: 0.194
[ 22 / 40 ] loss: 0.197
[ 23 / 40 ] loss: 0.074
[ 24 / 40 ] loss: 0.023
[ 25 / 40 ] loss: 0.011
[ 26 / 40 ] loss: 0.236
[ 27 / 40 ] loss: 0.106
[ 28 / 40 ] loss: 0.084
[ 29 / 40 ] loss: 0.008
[ 30 / 40 ] loss: 0.096
[ 31 / 40 ] loss: 0.007
[ 32 / 40 ] loss: 0.125
[ 33 / 40 ] loss: 0.009
[ 34 / 40 ] loss: 0.172
[ 35 / 40 ] loss: 0.010
[ 36 / 40 ] loss: 0.009
[ 37 / 40 ] loss: 0.044
[ 38 / 40 ] loss: 0.025
[ 39 / 40 ] loss: 0.185
[ 40 / 40 ] loss: 0.401
0.06642838848056272
Accuracy: 0.771186 -- Precision: 0.804878 -- Recall: 0.767442 -- F1: 0.785714 -- AUC: 0.814424
========= epoch: 28 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.045
[ 2 / 40 ] loss: 0.033
[ 3 / 40 ] loss: 0.009
[ 4 / 40 ] loss: 0.008
[ 5 / 40 ] loss: 0.011
[ 6 / 40 ] loss: 0.012
[ 7 / 40 ] loss: 0.033
[ 8 / 40 ] loss: 0.027
[ 9 / 40 ] loss: 0.170
[ 10 / 40 ] loss: 0.025
[ 11 / 40 ] loss: 0.030
[ 12 / 40 ] loss: 0.038
[ 13 / 40 ] loss: 0.015
[ 14 / 40 ] loss: 0.025
[ 15 / 40 ] loss: 0.058
[ 16 / 40 ] loss: 0.057
[ 17 / 40 ] loss: 0.076
[ 18 / 40 ] loss: 0.053
[ 19 / 40 ] loss: 0.015
[ 20 / 40 ] loss: 0.044
[ 21 / 40 ] loss: 0.009
[ 22 / 40 ] loss: 0.021
[ 23 / 40 ] loss: 0.031
[ 24 / 40 ] loss: 0.192
[ 25 / 40 ] loss: 0.107
[ 26 / 40 ] loss: 0.017
[ 27 / 40 ] loss: 0.050
[ 28 / 40 ] loss: 0.011
[ 29 / 40 ] loss: 0.191
[ 30 / 40 ] loss: 0.009
[ 31 / 40 ] loss: 0.023
[ 32 / 40 ] loss: 0.190
[ 33 / 40 ] loss: 0.304
[ 34 / 40 ] loss: 0.007
[ 35 / 40 ] loss: 0.009
[ 36 / 40 ] loss: 0.138
[ 37 / 40 ] loss: 0.034
[ 38 / 40 ] loss: 0.052
[ 39 / 40 ] loss: 0.226
[ 40 / 40 ] loss: 0.011
0.06036306746536866
Accuracy: 0.758475 -- Precision: 0.795082 -- Recall: 0.751938 -- F1: 0.772908 -- AUC: 0.809969
========= epoch: 29 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.195
[ 2 / 40 ] loss: 0.011
[ 3 / 40 ] loss: 0.030
[ 4 / 40 ] loss: 0.051
[ 5 / 40 ] loss: 0.089
[ 6 / 40 ] loss: 0.040
[ 7 / 40 ] loss: 0.089
[ 8 / 40 ] loss: 0.033
[ 9 / 40 ] loss: 0.010
[ 10 / 40 ] loss: 0.015
[ 11 / 40 ] loss: 0.090
[ 12 / 40 ] loss: 0.011
[ 13 / 40 ] loss: 0.044
[ 14 / 40 ] loss: 0.028
[ 15 / 40 ] loss: 0.061
[ 16 / 40 ] loss: 0.219
[ 17 / 40 ] loss: 0.014
[ 18 / 40 ] loss: 0.008
[ 19 / 40 ] loss: 0.037
[ 20 / 40 ] loss: 0.009
[ 21 / 40 ] loss: 0.010
[ 22 / 40 ] loss: 0.095
[ 23 / 40 ] loss: 0.008
[ 24 / 40 ] loss: 0.212
[ 25 / 40 ] loss: 0.007
[ 26 / 40 ] loss: 0.008
[ 27 / 40 ] loss: 0.191
[ 28 / 40 ] loss: 0.008
[ 29 / 40 ] loss: 0.005
[ 30 / 40 ] loss: 0.025
[ 31 / 40 ] loss: 0.008
[ 32 / 40 ] loss: 0.064
[ 33 / 40 ] loss: 0.037
[ 34 / 40 ] loss: 0.026
[ 35 / 40 ] loss: 0.249
[ 36 / 40 ] loss: 0.025
[ 37 / 40 ] loss: 0.006
[ 38 / 40 ] loss: 0.039
[ 39 / 40 ] loss: 0.007
[ 40 / 40 ] loss: 0.134
0.05625934926792979
Accuracy: 0.783898 -- Precision: 0.795455 -- Recall: 0.813953 -- F1: 0.804598 -- AUC: 0.823662
========= epoch: 30 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.007
[ 2 / 40 ] loss: 0.007
[ 3 / 40 ] loss: 0.046
[ 4 / 40 ] loss: 0.008
[ 5 / 40 ] loss: 0.079
[ 6 / 40 ] loss: 0.048
[ 7 / 40 ] loss: 0.034
[ 8 / 40 ] loss: 0.013
[ 9 / 40 ] loss: 0.176
[ 10 / 40 ] loss: 0.030
[ 11 / 40 ] loss: 0.364
[ 12 / 40 ] loss: 0.178
[ 13 / 40 ] loss: 0.020
[ 14 / 40 ] loss: 0.180
[ 15 / 40 ] loss: 0.045
[ 16 / 40 ] loss: 0.051
[ 17 / 40 ] loss: 0.216
[ 18 / 40 ] loss: 0.071
[ 19 / 40 ] loss: 0.525
[ 20 / 40 ] loss: 1.388
[ 21 / 40 ] loss: 1.239
[ 22 / 40 ] loss: 0.197
[ 23 / 40 ] loss: 0.776
[ 24 / 40 ] loss: 0.236
[ 25 / 40 ] loss: 0.484
[ 26 / 40 ] loss: 0.349
[ 27 / 40 ] loss: 0.338
[ 28 / 40 ] loss: 0.295
[ 29 / 40 ] loss: 0.403
[ 30 / 40 ] loss: 0.212
[ 31 / 40 ] loss: 0.172
[ 32 / 40 ] loss: 0.169
[ 33 / 40 ] loss: 0.142
[ 34 / 40 ] loss: 0.169
[ 35 / 40 ] loss: 0.198
[ 36 / 40 ] loss: 0.134
[ 37 / 40 ] loss: 0.020
[ 38 / 40 ] loss: 0.359
[ 39 / 40 ] loss: 0.295
[ 40 / 40 ] loss: 0.344
0.25042296532774344
Accuracy: 0.800847 -- Precision: 0.815385 -- Recall: 0.821705 -- F1: 0.818533 -- AUC: 0.845106
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[199, 113, 231, 51, 109, 187, 232, 98, 208, 163, 198, 105, 13, 45, 116, 128, 190, 3, 117, 92, 82, 209, 115, 218, 58, 143, 213, 188, 183, 130, 123, 55, 147, 205, 118, 154, 204, 67, 77, 216, 185, 156, 68, 146, 2, 110, 155, 132, 167, 40, 181, 223, 235, 236, 150, 191, 162, 49, 229, 65, 189, 101, 157, 212, 148, 220, 122, 7, 86, 12, 186, 170, 182, 230, 194, 57, 94, 184, 211, 19, 164, 106, 151, 121, 95, 207, 90, 100, 25, 14, 62, 50, 174, 16, 165, 73, 54, 41, 166, 234, 168, 176, 124, 9, 26, 203, 15, 206, 159, 120, 233, 224, 66, 192, 63, 17, 80, 85, 145, 175, 36, 141, 71, 125, 43, 52, 20, 46, 129, 81, 178, 22, 201, 72, 152, 134, 219, 87, 112, 227, 99, 138, 48, 38, 75, 221, 172, 177, 4, 210, 64, 215, 88, 47, 173, 21, 217, 74, 200, 37, 93, 1, 89, 214, 228, 222, 135, 61, 70, 226, 127, 29, 31, 179, 39, 53, 69, 136, 11, 153, 171, 102, 27, 79, 225, 202, 139, 197, 195, 103, 142, 35, 6, 28, 59, 83, 97, 18, 193, 30, 96, 23, 180, 111, 160, 119, 114, 42, 149, 44, 8, 10, 32, 137, 158, 76, 133, 169, 104, 91, 34, 78, 33, 24, 56, 161, 140, 5, 144, 131, 84, 107, 60, 196, 126, 108]
[1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]
[1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 31 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.034
[ 2 / 40 ] loss: 0.082
[ 3 / 40 ] loss: 0.577
[ 4 / 40 ] loss: 0.119
[ 5 / 40 ] loss: 0.047
[ 6 / 40 ] loss: 0.100
[ 7 / 40 ] loss: 0.205
[ 8 / 40 ] loss: 0.373
[ 9 / 40 ] loss: 0.308
[ 10 / 40 ] loss: 0.137
[ 11 / 40 ] loss: 0.168
[ 12 / 40 ] loss: 0.106
[ 13 / 40 ] loss: 0.036
[ 14 / 40 ] loss: 0.131
[ 15 / 40 ] loss: 0.106
[ 16 / 40 ] loss: 0.146
[ 17 / 40 ] loss: 0.117
[ 18 / 40 ] loss: 0.121
[ 19 / 40 ] loss: 0.042
[ 20 / 40 ] loss: 0.033
[ 21 / 40 ] loss: 0.030
[ 22 / 40 ] loss: 0.091
[ 23 / 40 ] loss: 0.260
[ 24 / 40 ] loss: 0.168
[ 25 / 40 ] loss: 0.167
[ 26 / 40 ] loss: 0.145
[ 27 / 40 ] loss: 0.097
[ 28 / 40 ] loss: 0.071
[ 29 / 40 ] loss: 0.040
[ 30 / 40 ] loss: 0.021
[ 31 / 40 ] loss: 0.046
[ 32 / 40 ] loss: 0.038
[ 33 / 40 ] loss: 0.058
[ 34 / 40 ] loss: 0.028
[ 35 / 40 ] loss: 0.032
[ 36 / 40 ] loss: 0.042
[ 37 / 40 ] loss: 0.154
[ 38 / 40 ] loss: 0.030
[ 39 / 40 ] loss: 0.061
[ 40 / 40 ] loss: 0.040
0.11521783964708447
Accuracy: 0.788136 -- Precision: 0.796992 -- Recall: 0.821705 -- F1: 0.809160 -- AUC: 0.828805
========= epoch: 32 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.034
[ 2 / 40 ] loss: 0.021
[ 3 / 40 ] loss: 0.213
[ 4 / 40 ] loss: 0.062
[ 5 / 40 ] loss: 0.014
[ 6 / 40 ] loss: 0.012
[ 7 / 40 ] loss: 0.013
[ 8 / 40 ] loss: 0.016
[ 9 / 40 ] loss: 0.107
[ 10 / 40 ] loss: 0.038
[ 11 / 40 ] loss: 0.015
[ 12 / 40 ] loss: 0.012
[ 13 / 40 ] loss: 0.010
[ 14 / 40 ] loss: 0.015
[ 15 / 40 ] loss: 0.012
[ 16 / 40 ] loss: 0.310
[ 17 / 40 ] loss: 0.127
[ 18 / 40 ] loss: 0.038
[ 19 / 40 ] loss: 0.027
[ 20 / 40 ] loss: 0.096
[ 21 / 40 ] loss: 0.014
[ 22 / 40 ] loss: 0.042
[ 23 / 40 ] loss: 0.010
[ 24 / 40 ] loss: 0.064
[ 25 / 40 ] loss: 0.206
[ 26 / 40 ] loss: 0.152
[ 27 / 40 ] loss: 0.154
[ 28 / 40 ] loss: 0.019
[ 29 / 40 ] loss: 0.020
[ 30 / 40 ] loss: 0.213
[ 31 / 40 ] loss: 0.205
[ 32 / 40 ] loss: 0.222
[ 33 / 40 ] loss: 0.018
[ 34 / 40 ] loss: 0.058
[ 35 / 40 ] loss: 0.045
[ 36 / 40 ] loss: 0.149
[ 37 / 40 ] loss: 0.128
[ 38 / 40 ] loss: 0.011
[ 39 / 40 ] loss: 0.026
[ 40 / 40 ] loss: 0.016
0.07411554176360369
Accuracy: 0.796610 -- Precision: 0.800000 -- Recall: 0.837209 -- F1: 0.818182 -- AUC: 0.837209
========= epoch: 33 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.051
[ 2 / 40 ] loss: 0.067
[ 3 / 40 ] loss: 0.011
[ 4 / 40 ] loss: 0.086
[ 5 / 40 ] loss: 0.111
[ 6 / 40 ] loss: 0.095
[ 7 / 40 ] loss: 0.011
[ 8 / 40 ] loss: 0.063
[ 9 / 40 ] loss: 0.028
[ 10 / 40 ] loss: 0.014
[ 11 / 40 ] loss: 0.104
[ 12 / 40 ] loss: 0.009
[ 13 / 40 ] loss: 0.020
[ 14 / 40 ] loss: 0.015
[ 15 / 40 ] loss: 0.024
[ 16 / 40 ] loss: 0.189
[ 17 / 40 ] loss: 0.009
[ 18 / 40 ] loss: 0.008
[ 19 / 40 ] loss: 0.064
[ 20 / 40 ] loss: 0.011
[ 21 / 40 ] loss: 0.009
[ 22 / 40 ] loss: 0.075
[ 23 / 40 ] loss: 0.076
[ 24 / 40 ] loss: 0.044
[ 25 / 40 ] loss: 0.017
[ 26 / 40 ] loss: 0.034
[ 27 / 40 ] loss: 0.008
[ 28 / 40 ] loss: 0.013
[ 29 / 40 ] loss: 0.173
[ 30 / 40 ] loss: 0.183
[ 31 / 40 ] loss: 0.038
[ 32 / 40 ] loss: 0.015
[ 33 / 40 ] loss: 0.042
[ 34 / 40 ] loss: 0.009
[ 35 / 40 ] loss: 0.234
[ 36 / 40 ] loss: 0.198
[ 37 / 40 ] loss: 0.201
[ 38 / 40 ] loss: 0.141
[ 39 / 40 ] loss: 0.043
[ 40 / 40 ] loss: 0.038
0.06455166230443865
Accuracy: 0.788136 -- Precision: 0.811024 -- Recall: 0.798450 -- F1: 0.804688 -- AUC: 0.827067
========= epoch: 34 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.024
[ 2 / 40 ] loss: 0.193
[ 3 / 40 ] loss: 0.065
[ 4 / 40 ] loss: 0.026
[ 5 / 40 ] loss: 0.022
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.046
[ 8 / 40 ] loss: 0.053
[ 9 / 40 ] loss: 0.046
[ 10 / 40 ] loss: 0.010
[ 11 / 40 ] loss: 0.186
[ 12 / 40 ] loss: 0.016
[ 13 / 40 ] loss: 0.072
[ 14 / 40 ] loss: 0.109
[ 15 / 40 ] loss: 0.011
[ 16 / 40 ] loss: 0.025
[ 17 / 40 ] loss: 0.012
[ 18 / 40 ] loss: 0.009
[ 19 / 40 ] loss: 0.044
[ 20 / 40 ] loss: 0.075
[ 21 / 40 ] loss: 0.032
[ 22 / 40 ] loss: 0.010
[ 23 / 40 ] loss: 0.246
[ 24 / 40 ] loss: 0.011
[ 25 / 40 ] loss: 0.025
[ 26 / 40 ] loss: 0.010
[ 27 / 40 ] loss: 0.077
[ 28 / 40 ] loss: 0.174
[ 29 / 40 ] loss: 0.008
[ 30 / 40 ] loss: 0.082
[ 31 / 40 ] loss: 0.073
[ 32 / 40 ] loss: 0.012
[ 33 / 40 ] loss: 0.012
[ 34 / 40 ] loss: 0.020
[ 35 / 40 ] loss: 0.015
[ 36 / 40 ] loss: 0.081
[ 37 / 40 ] loss: 0.040
[ 38 / 40 ] loss: 0.145
[ 39 / 40 ] loss: 0.105
[ 40 / 40 ] loss: 0.005
0.05583024714142084
Accuracy: 0.783898 -- Precision: 0.782609 -- Recall: 0.837209 -- F1: 0.808989 -- AUC: 0.845468
========= epoch: 35 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.014
[ 2 / 40 ] loss: 0.249
[ 3 / 40 ] loss: 0.034
[ 4 / 40 ] loss: 0.010
[ 5 / 40 ] loss: 0.213
[ 6 / 40 ] loss: 0.077
[ 7 / 40 ] loss: 0.007
[ 8 / 40 ] loss: 0.011
[ 9 / 40 ] loss: 0.063
[ 10 / 40 ] loss: 0.028
[ 11 / 40 ] loss: 0.017
[ 12 / 40 ] loss: 0.068
[ 13 / 40 ] loss: 0.026
[ 14 / 40 ] loss: 0.079
[ 15 / 40 ] loss: 0.192
[ 16 / 40 ] loss: 0.066
[ 17 / 40 ] loss: 0.035
[ 18 / 40 ] loss: 0.032
[ 19 / 40 ] loss: 0.035
[ 20 / 40 ] loss: 0.007
[ 21 / 40 ] loss: 0.101
[ 22 / 40 ] loss: 0.129
[ 23 / 40 ] loss: 0.009
[ 24 / 40 ] loss: 0.007
[ 25 / 40 ] loss: 0.029
[ 26 / 40 ] loss: 0.007
[ 27 / 40 ] loss: 0.068
[ 28 / 40 ] loss: 0.026
[ 29 / 40 ] loss: 0.013
[ 30 / 40 ] loss: 0.043
[ 31 / 40 ] loss: 0.020
[ 32 / 40 ] loss: 0.043
[ 33 / 40 ] loss: 0.037
[ 34 / 40 ] loss: 0.054
[ 35 / 40 ] loss: 0.007
[ 36 / 40 ] loss: 0.024
[ 37 / 40 ] loss: 0.007
[ 38 / 40 ] loss: 0.045
[ 39 / 40 ] loss: 0.007
[ 40 / 40 ] loss: 0.006
0.04862974830903113
Accuracy: 0.796610 -- Precision: 0.809160 -- Recall: 0.821705 -- F1: 0.815385 -- AUC: 0.833152
========= epoch: 36 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.227
[ 2 / 40 ] loss: 0.005
[ 3 / 40 ] loss: 0.034
[ 4 / 40 ] loss: 0.007
[ 5 / 40 ] loss: 0.191
[ 6 / 40 ] loss: 0.186
[ 7 / 40 ] loss: 0.005
[ 8 / 40 ] loss: 0.006
[ 9 / 40 ] loss: 0.090
[ 10 / 40 ] loss: 0.020
[ 11 / 40 ] loss: 0.035
[ 12 / 40 ] loss: 0.054
[ 13 / 40 ] loss: 0.009
[ 14 / 40 ] loss: 0.117
[ 15 / 40 ] loss: 0.029
[ 16 / 40 ] loss: 0.033
[ 17 / 40 ] loss: 0.006
[ 18 / 40 ] loss: 0.025
[ 19 / 40 ] loss: 0.027
[ 20 / 40 ] loss: 0.007
[ 21 / 40 ] loss: 0.008
[ 22 / 40 ] loss: 0.029
[ 23 / 40 ] loss: 0.008
[ 24 / 40 ] loss: 0.030
[ 25 / 40 ] loss: 0.080
[ 26 / 40 ] loss: 0.043
[ 27 / 40 ] loss: 0.033
[ 28 / 40 ] loss: 0.015
[ 29 / 40 ] loss: 0.009
[ 30 / 40 ] loss: 0.243
[ 31 / 40 ] loss: 0.009
[ 32 / 40 ] loss: 0.099
[ 33 / 40 ] loss: 0.005
[ 34 / 40 ] loss: 0.023
[ 35 / 40 ] loss: 0.054
[ 36 / 40 ] loss: 0.050
[ 37 / 40 ] loss: 0.064
[ 38 / 40 ] loss: 0.008
[ 39 / 40 ] loss: 0.005
[ 40 / 40 ] loss: 0.237
0.05412514646304771
Accuracy: 0.775424 -- Precision: 0.775362 -- Recall: 0.829457 -- F1: 0.801498 -- AUC: 0.832464
========= epoch: 37 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.029
[ 2 / 40 ] loss: 0.007
[ 3 / 40 ] loss: 0.054
[ 4 / 40 ] loss: 0.019
[ 5 / 40 ] loss: 0.035
[ 6 / 40 ] loss: 0.007
[ 7 / 40 ] loss: 0.033
[ 8 / 40 ] loss: 0.009
[ 9 / 40 ] loss: 0.040
[ 10 / 40 ] loss: 0.058
[ 11 / 40 ] loss: 0.038
[ 12 / 40 ] loss: 0.046
[ 13 / 40 ] loss: 0.023
[ 14 / 40 ] loss: 0.007
[ 15 / 40 ] loss: 0.028
[ 16 / 40 ] loss: 0.086
[ 17 / 40 ] loss: 0.009
[ 18 / 40 ] loss: 0.040
[ 19 / 40 ] loss: 0.013
[ 20 / 40 ] loss: 0.026
[ 21 / 40 ] loss: 0.009
[ 22 / 40 ] loss: 0.033
[ 23 / 40 ] loss: 0.021
[ 24 / 40 ] loss: 0.240
[ 25 / 40 ] loss: 0.012
[ 26 / 40 ] loss: 0.083
[ 27 / 40 ] loss: 0.005
[ 28 / 40 ] loss: 0.010
[ 29 / 40 ] loss: 0.159
[ 30 / 40 ] loss: 0.040
[ 31 / 40 ] loss: 0.214
[ 32 / 40 ] loss: 0.064
[ 33 / 40 ] loss: 0.006
[ 34 / 40 ] loss: 0.098
[ 35 / 40 ] loss: 0.005
[ 36 / 40 ] loss: 0.006
[ 37 / 40 ] loss: 0.041
[ 38 / 40 ] loss: 0.020
[ 39 / 40 ] loss: 0.217
[ 40 / 40 ] loss: 0.124
0.05032688599312678
Accuracy: 0.783898 -- Precision: 0.791045 -- Recall: 0.821705 -- F1: 0.806084 -- AUC: 0.842426
========= epoch: 38 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.027
[ 2 / 40 ] loss: 0.050
[ 3 / 40 ] loss: 0.004
[ 4 / 40 ] loss: 0.078
[ 5 / 40 ] loss: 0.044
[ 6 / 40 ] loss: 0.010
[ 7 / 40 ] loss: 0.010
[ 8 / 40 ] loss: 0.078
[ 9 / 40 ] loss: 0.169
[ 10 / 40 ] loss: 0.099
[ 11 / 40 ] loss: 0.085
[ 12 / 40 ] loss: 0.008
[ 13 / 40 ] loss: 0.031
[ 14 / 40 ] loss: 0.012
[ 15 / 40 ] loss: 0.028
[ 16 / 40 ] loss: 0.022
[ 17 / 40 ] loss: 0.033
[ 18 / 40 ] loss: 0.074
[ 19 / 40 ] loss: 0.063
[ 20 / 40 ] loss: 0.006
[ 21 / 40 ] loss: 0.264
[ 22 / 40 ] loss: 0.024
[ 23 / 40 ] loss: 0.012
[ 24 / 40 ] loss: 0.173
[ 25 / 40 ] loss: 0.023
[ 26 / 40 ] loss: 0.246
[ 27 / 40 ] loss: 0.011
[ 28 / 40 ] loss: 0.007
[ 29 / 40 ] loss: 0.089
[ 30 / 40 ] loss: 0.180
[ 31 / 40 ] loss: 0.085
[ 32 / 40 ] loss: 0.057
[ 33 / 40 ] loss: 0.007
[ 34 / 40 ] loss: 0.015
[ 35 / 40 ] loss: 0.071
[ 36 / 40 ] loss: 0.042
[ 37 / 40 ] loss: 0.008
[ 38 / 40 ] loss: 0.169
[ 39 / 40 ] loss: 0.030
[ 40 / 40 ] loss: 0.014
0.06149752718629316
Accuracy: 0.796610 -- Precision: 0.787234 -- Recall: 0.860465 -- F1: 0.822222 -- AUC: 0.835108
========= epoch: 39 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.079
[ 2 / 40 ] loss: 0.038
[ 3 / 40 ] loss: 0.045
[ 4 / 40 ] loss: 0.007
[ 5 / 40 ] loss: 0.006
[ 6 / 40 ] loss: 0.041
[ 7 / 40 ] loss: 0.035
[ 8 / 40 ] loss: 0.254
[ 9 / 40 ] loss: 0.030
[ 10 / 40 ] loss: 0.033
[ 11 / 40 ] loss: 0.007
[ 12 / 40 ] loss: 0.026
[ 13 / 40 ] loss: 0.024
[ 14 / 40 ] loss: 0.006
[ 15 / 40 ] loss: 0.051
[ 16 / 40 ] loss: 0.061
[ 17 / 40 ] loss: 0.008
[ 18 / 40 ] loss: 0.330
[ 19 / 40 ] loss: 0.038
[ 20 / 40 ] loss: 0.068
[ 21 / 40 ] loss: 0.036
[ 22 / 40 ] loss: 0.051
[ 23 / 40 ] loss: 0.060
[ 24 / 40 ] loss: 0.053
[ 25 / 40 ] loss: 0.009
[ 26 / 40 ] loss: 0.058
[ 27 / 40 ] loss: 0.038
[ 28 / 40 ] loss: 0.009
[ 29 / 40 ] loss: 0.067
[ 30 / 40 ] loss: 0.052
[ 31 / 40 ] loss: 0.145
[ 32 / 40 ] loss: 0.177
[ 33 / 40 ] loss: 0.010
[ 34 / 40 ] loss: 0.030
[ 35 / 40 ] loss: 0.191
[ 36 / 40 ] loss: 0.040
[ 37 / 40 ] loss: 0.068
[ 38 / 40 ] loss: 0.025
[ 39 / 40 ] loss: 0.012
[ 40 / 40 ] loss: 0.005
0.058087711967527864
Accuracy: 0.792373 -- Precision: 0.789855 -- Recall: 0.844961 -- F1: 0.816479 -- AUC: 0.840578
========= epoch: 40 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.005
[ 2 / 40 ] loss: 0.056
[ 3 / 40 ] loss: 0.034
[ 4 / 40 ] loss: 0.114
[ 5 / 40 ] loss: 0.033
[ 6 / 40 ] loss: 0.041
[ 7 / 40 ] loss: 0.026
[ 8 / 40 ] loss: 0.011
[ 9 / 40 ] loss: 0.037
[ 10 / 40 ] loss: 0.013
[ 11 / 40 ] loss: 0.302
[ 12 / 40 ] loss: 0.009
[ 13 / 40 ] loss: 0.045
[ 14 / 40 ] loss: 0.107
[ 15 / 40 ] loss: 0.029
[ 16 / 40 ] loss: 0.009
[ 17 / 40 ] loss: 0.074
[ 18 / 40 ] loss: 0.217
[ 19 / 40 ] loss: 0.265
[ 20 / 40 ] loss: 0.096
[ 21 / 40 ] loss: 0.171
[ 22 / 40 ] loss: 0.146
[ 23 / 40 ] loss: 0.072
[ 24 / 40 ] loss: 0.040
[ 25 / 40 ] loss: 0.141
[ 26 / 40 ] loss: 0.031
[ 27 / 40 ] loss: 0.075
[ 28 / 40 ] loss: 0.018
[ 29 / 40 ] loss: 0.094
[ 30 / 40 ] loss: 0.191
[ 31 / 40 ] loss: 0.021
[ 32 / 40 ] loss: 0.050
[ 33 / 40 ] loss: 0.014
[ 34 / 40 ] loss: 0.091
[ 35 / 40 ] loss: 0.012
[ 36 / 40 ] loss: 0.027
[ 37 / 40 ] loss: 0.011
[ 38 / 40 ] loss: 0.064
[ 39 / 40 ] loss: 0.069
[ 40 / 40 ] loss: 0.110
0.07427120170323179
Accuracy: 0.788136 -- Precision: 0.831933 -- Recall: 0.767442 -- F1: 0.798387 -- AUC: 0.833080
========= epoch: 41 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.006
[ 2 / 40 ] loss: 0.029
[ 3 / 40 ] loss: 0.005
[ 4 / 40 ] loss: 0.044
[ 5 / 40 ] loss: 0.225
[ 6 / 40 ] loss: 0.021
[ 7 / 40 ] loss: 0.030
[ 8 / 40 ] loss: 0.018
[ 9 / 40 ] loss: 0.009
[ 10 / 40 ] loss: 0.010
[ 11 / 40 ] loss: 0.009
[ 12 / 40 ] loss: 0.031
[ 13 / 40 ] loss: 0.115
[ 14 / 40 ] loss: 0.080
[ 15 / 40 ] loss: 0.024
[ 16 / 40 ] loss: 0.048
[ 17 / 40 ] loss: 0.133
[ 18 / 40 ] loss: 0.071
[ 19 / 40 ] loss: 0.010
[ 20 / 40 ] loss: 0.008
[ 21 / 40 ] loss: 0.052
[ 22 / 40 ] loss: 0.079
[ 23 / 40 ] loss: 0.088
[ 24 / 40 ] loss: 0.075
[ 25 / 40 ] loss: 0.167
[ 26 / 40 ] loss: 0.005
[ 27 / 40 ] loss: 0.110
[ 28 / 40 ] loss: 0.033
[ 29 / 40 ] loss: 0.007
[ 30 / 40 ] loss: 0.088
[ 31 / 40 ] loss: 0.008
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.006
[ 34 / 40 ] loss: 0.008
[ 35 / 40 ] loss: 0.006
[ 36 / 40 ] loss: 0.206
[ 37 / 40 ] loss: 0.011
[ 38 / 40 ] loss: 0.029
[ 39 / 40 ] loss: 0.126
[ 40 / 40 ] loss: 0.005
0.05099813641281799
Accuracy: 0.788136 -- Precision: 0.831933 -- Recall: 0.767442 -- F1: 0.798387 -- AUC: 0.823191
========= epoch: 42 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.005
[ 2 / 40 ] loss: 0.031
[ 3 / 40 ] loss: 0.037
[ 4 / 40 ] loss: 0.028
[ 5 / 40 ] loss: 0.014
[ 6 / 40 ] loss: 0.026
[ 7 / 40 ] loss: 0.025
[ 8 / 40 ] loss: 0.018
[ 9 / 40 ] loss: 0.058
[ 10 / 40 ] loss: 0.006
[ 11 / 40 ] loss: 0.005
[ 12 / 40 ] loss: 0.065
[ 13 / 40 ] loss: 0.014
[ 14 / 40 ] loss: 0.044
[ 15 / 40 ] loss: 0.078
[ 16 / 40 ] loss: 0.016
[ 17 / 40 ] loss: 0.017
[ 18 / 40 ] loss: 0.380
[ 19 / 40 ] loss: 0.007
[ 20 / 40 ] loss: 0.024
[ 21 / 40 ] loss: 0.199
[ 22 / 40 ] loss: 0.004
[ 23 / 40 ] loss: 0.067
[ 24 / 40 ] loss: 0.458
[ 25 / 40 ] loss: 0.040
[ 26 / 40 ] loss: 0.228
[ 27 / 40 ] loss: 0.008
[ 28 / 40 ] loss: 0.026
[ 29 / 40 ] loss: 0.020
[ 30 / 40 ] loss: 0.100
[ 31 / 40 ] loss: 0.083
[ 32 / 40 ] loss: 0.115
[ 33 / 40 ] loss: 0.114
[ 34 / 40 ] loss: 0.143
[ 35 / 40 ] loss: 0.068
[ 36 / 40 ] loss: 0.183
[ 37 / 40 ] loss: 0.041
[ 38 / 40 ] loss: 0.194
[ 39 / 40 ] loss: 0.021
[ 40 / 40 ] loss: 0.320
0.08324376137461513
Accuracy: 0.754237 -- Precision: 0.732026 -- Recall: 0.868217 -- F1: 0.794326 -- AUC: 0.825871
========= epoch: 43 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.158
[ 2 / 40 ] loss: 0.046
[ 3 / 40 ] loss: 0.011
[ 4 / 40 ] loss: 0.098
[ 5 / 40 ] loss: 0.183
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.055
[ 8 / 40 ] loss: 0.117
[ 9 / 40 ] loss: 0.006
[ 10 / 40 ] loss: 0.024
[ 11 / 40 ] loss: 0.073
[ 12 / 40 ] loss: 0.017
[ 13 / 40 ] loss: 0.091
[ 14 / 40 ] loss: 0.012
[ 15 / 40 ] loss: 0.139
[ 16 / 40 ] loss: 0.670
[ 17 / 40 ] loss: 0.009
[ 18 / 40 ] loss: 0.088
[ 19 / 40 ] loss: 0.026
[ 20 / 40 ] loss: 0.059
[ 21 / 40 ] loss: 0.087
[ 22 / 40 ] loss: 0.010
[ 23 / 40 ] loss: 0.016
[ 24 / 40 ] loss: 0.056
[ 25 / 40 ] loss: 0.063
[ 26 / 40 ] loss: 0.033
[ 27 / 40 ] loss: 0.009
[ 28 / 40 ] loss: 0.030
[ 29 / 40 ] loss: 0.020
[ 30 / 40 ] loss: 0.258
[ 31 / 40 ] loss: 0.186
[ 32 / 40 ] loss: 0.054
[ 33 / 40 ] loss: 0.064
[ 34 / 40 ] loss: 0.011
[ 35 / 40 ] loss: 0.012
[ 36 / 40 ] loss: 0.049
[ 37 / 40 ] loss: 0.035
[ 38 / 40 ] loss: 0.021
[ 39 / 40 ] loss: 0.044
[ 40 / 40 ] loss: 0.008
0.07387969642877579
Accuracy: 0.771186 -- Precision: 0.790698 -- Recall: 0.790698 -- F1: 0.790698 -- AUC: 0.821488
========= epoch: 44 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.007
[ 2 / 40 ] loss: 0.076
[ 3 / 40 ] loss: 0.037
[ 4 / 40 ] loss: 0.074
[ 5 / 40 ] loss: 0.252
[ 6 / 40 ] loss: 0.006
[ 7 / 40 ] loss: 0.205
[ 8 / 40 ] loss: 0.008
[ 9 / 40 ] loss: 0.012
[ 10 / 40 ] loss: 0.005
[ 11 / 40 ] loss: 0.014
[ 12 / 40 ] loss: 0.090
[ 13 / 40 ] loss: 0.045
[ 14 / 40 ] loss: 0.014
[ 15 / 40 ] loss: 0.066
[ 16 / 40 ] loss: 0.013
[ 17 / 40 ] loss: 0.231
[ 18 / 40 ] loss: 0.272
[ 19 / 40 ] loss: 0.017
[ 20 / 40 ] loss: 0.131
[ 21 / 40 ] loss: 0.099
[ 22 / 40 ] loss: 0.116
[ 23 / 40 ] loss: 0.018
[ 24 / 40 ] loss: 0.016
[ 25 / 40 ] loss: 0.087
[ 26 / 40 ] loss: 0.057
[ 27 / 40 ] loss: 0.023
[ 28 / 40 ] loss: 0.055
[ 29 / 40 ] loss: 0.077
[ 30 / 40 ] loss: 0.024
[ 31 / 40 ] loss: 0.021
[ 32 / 40 ] loss: 0.255
[ 33 / 40 ] loss: 0.051
[ 34 / 40 ] loss: 0.013
[ 35 / 40 ] loss: 0.105
[ 36 / 40 ] loss: 0.175
[ 37 / 40 ] loss: 0.047
[ 38 / 40 ] loss: 0.006
[ 39 / 40 ] loss: 0.024
[ 40 / 40 ] loss: 0.004
0.07114627554547041
Accuracy: 0.745763 -- Precision: 0.759398 -- Recall: 0.782946 -- F1: 0.770992 -- AUC: 0.811961
========= epoch: 45 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.015
[ 2 / 40 ] loss: 0.198
[ 3 / 40 ] loss: 0.015
[ 4 / 40 ] loss: 0.020
[ 5 / 40 ] loss: 0.010
[ 6 / 40 ] loss: 0.011
[ 7 / 40 ] loss: 0.024
[ 8 / 40 ] loss: 0.055
[ 9 / 40 ] loss: 0.022
[ 10 / 40 ] loss: 0.043
[ 11 / 40 ] loss: 0.026
[ 12 / 40 ] loss: 0.073
[ 13 / 40 ] loss: 0.005
[ 14 / 40 ] loss: 0.108
[ 15 / 40 ] loss: 0.006
[ 16 / 40 ] loss: 0.010
[ 17 / 40 ] loss: 0.180
[ 18 / 40 ] loss: 0.027
[ 19 / 40 ] loss: 0.193
[ 20 / 40 ] loss: 0.032
[ 21 / 40 ] loss: 0.007
[ 22 / 40 ] loss: 0.081
[ 23 / 40 ] loss: 0.010
[ 24 / 40 ] loss: 0.014
[ 25 / 40 ] loss: 0.034
[ 26 / 40 ] loss: 0.111
[ 27 / 40 ] loss: 0.013
[ 28 / 40 ] loss: 0.015
[ 29 / 40 ] loss: 0.006
[ 30 / 40 ] loss: 0.008
[ 31 / 40 ] loss: 0.011
[ 32 / 40 ] loss: 0.007
[ 33 / 40 ] loss: 0.033
[ 34 / 40 ] loss: 0.005
[ 35 / 40 ] loss: 0.030
[ 36 / 40 ] loss: 0.015
[ 37 / 40 ] loss: 0.044
[ 38 / 40 ] loss: 0.081
[ 39 / 40 ] loss: 0.146
[ 40 / 40 ] loss: 0.280
0.05063506566220895
Accuracy: 0.762712 -- Precision: 0.770370 -- Recall: 0.806202 -- F1: 0.787879 -- AUC: 0.840397
========= epoch: 46 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.059
[ 2 / 40 ] loss: 0.008
[ 3 / 40 ] loss: 0.111
[ 4 / 40 ] loss: 0.018
[ 5 / 40 ] loss: 0.039
[ 6 / 40 ] loss: 0.004
[ 7 / 40 ] loss: 0.040
[ 8 / 40 ] loss: 0.010
[ 9 / 40 ] loss: 0.023
[ 10 / 40 ] loss: 0.011
[ 11 / 40 ] loss: 0.004
[ 12 / 40 ] loss: 0.005
[ 13 / 40 ] loss: 0.010
[ 14 / 40 ] loss: 0.022
[ 15 / 40 ] loss: 0.008
[ 16 / 40 ] loss: 0.109
[ 17 / 40 ] loss: 0.009
[ 18 / 40 ] loss: 0.004
[ 19 / 40 ] loss: 0.006
[ 20 / 40 ] loss: 0.118
[ 21 / 40 ] loss: 0.019
[ 22 / 40 ] loss: 0.083
[ 23 / 40 ] loss: 0.080
[ 24 / 40 ] loss: 0.015
[ 25 / 40 ] loss: 0.082
[ 26 / 40 ] loss: 0.017
[ 27 / 40 ] loss: 0.072
[ 28 / 40 ] loss: 0.007
[ 29 / 40 ] loss: 0.248
[ 30 / 40 ] loss: 0.026
[ 31 / 40 ] loss: 0.326
[ 32 / 40 ] loss: 0.008
[ 33 / 40 ] loss: 0.014
[ 34 / 40 ] loss: 0.008
[ 35 / 40 ] loss: 0.115
[ 36 / 40 ] loss: 0.023
[ 37 / 40 ] loss: 0.101
[ 38 / 40 ] loss: 0.053
[ 39 / 40 ] loss: 0.034
[ 40 / 40 ] loss: 0.013
0.04901885916478932
Accuracy: 0.775424 -- Precision: 0.806452 -- Recall: 0.775194 -- F1: 0.790514 -- AUC: 0.809534
========= epoch: 47 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.016
[ 2 / 40 ] loss: 0.017
[ 3 / 40 ] loss: 0.009
[ 4 / 40 ] loss: 0.010
[ 5 / 40 ] loss: 0.081
[ 6 / 40 ] loss: 0.026
[ 7 / 40 ] loss: 0.006
[ 8 / 40 ] loss: 0.087
[ 9 / 40 ] loss: 0.007
[ 10 / 40 ] loss: 0.023
[ 11 / 40 ] loss: 0.006
[ 12 / 40 ] loss: 0.006
[ 13 / 40 ] loss: 0.005
[ 14 / 40 ] loss: 0.030
[ 15 / 40 ] loss: 0.006
[ 16 / 40 ] loss: 0.012
[ 17 / 40 ] loss: 0.008
[ 18 / 40 ] loss: 0.005
[ 19 / 40 ] loss: 0.102
[ 20 / 40 ] loss: 0.057
[ 21 / 40 ] loss: 0.010
[ 22 / 40 ] loss: 0.252
[ 23 / 40 ] loss: 0.009
[ 24 / 40 ] loss: 0.007
[ 25 / 40 ] loss: 0.201
[ 26 / 40 ] loss: 0.012
[ 27 / 40 ] loss: 0.102
[ 28 / 40 ] loss: 0.014
[ 29 / 40 ] loss: 0.023
[ 30 / 40 ] loss: 0.011
[ 31 / 40 ] loss: 0.008
[ 32 / 40 ] loss: 0.011
[ 33 / 40 ] loss: 0.005
[ 34 / 40 ] loss: 0.147
[ 35 / 40 ] loss: 0.077
[ 36 / 40 ] loss: 0.005
[ 37 / 40 ] loss: 0.005
[ 38 / 40 ] loss: 0.012
[ 39 / 40 ] loss: 0.023
[ 40 / 40 ] loss: 0.003
0.03640527472598478
Accuracy: 0.766949 -- Precision: 0.793651 -- Recall: 0.775194 -- F1: 0.784314 -- AUC: 0.809752
========= epoch: 48 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.069
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.061
[ 4 / 40 ] loss: 0.014
[ 5 / 40 ] loss: 0.004
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.003
[ 8 / 40 ] loss: 0.042
[ 9 / 40 ] loss: 0.002
[ 10 / 40 ] loss: 0.038
[ 11 / 40 ] loss: 0.106
[ 12 / 40 ] loss: 0.043
[ 13 / 40 ] loss: 0.503
[ 14 / 40 ] loss: 0.003
[ 15 / 40 ] loss: 0.237
[ 16 / 40 ] loss: 0.069
[ 17 / 40 ] loss: 0.004
[ 18 / 40 ] loss: 0.006
[ 19 / 40 ] loss: 0.009
[ 20 / 40 ] loss: 0.022
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.024
[ 23 / 40 ] loss: 0.020
[ 24 / 40 ] loss: 0.007
[ 25 / 40 ] loss: 0.044
[ 26 / 40 ] loss: 0.005
[ 27 / 40 ] loss: 0.019
[ 28 / 40 ] loss: 0.077
[ 29 / 40 ] loss: 0.005
[ 30 / 40 ] loss: 0.005
[ 31 / 40 ] loss: 0.005
[ 32 / 40 ] loss: 0.024
[ 33 / 40 ] loss: 0.014
[ 34 / 40 ] loss: 0.047
[ 35 / 40 ] loss: 0.005
[ 36 / 40 ] loss: 0.004
[ 37 / 40 ] loss: 0.005
[ 38 / 40 ] loss: 0.006
[ 39 / 40 ] loss: 0.003
[ 40 / 40 ] loss: 0.003
0.03929043491953053
Accuracy: 0.766949 -- Precision: 0.776119 -- Recall: 0.806202 -- F1: 0.790875 -- AUC: 0.817503
========= epoch: 49 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.003
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.015
[ 4 / 40 ] loss: 0.005
[ 5 / 40 ] loss: 0.004
[ 6 / 40 ] loss: 0.055
[ 7 / 40 ] loss: 0.005
[ 8 / 40 ] loss: 0.155
[ 9 / 40 ] loss: 0.033
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.018
[ 12 / 40 ] loss: 0.005
[ 13 / 40 ] loss: 0.046
[ 14 / 40 ] loss: 0.006
[ 15 / 40 ] loss: 0.006
[ 16 / 40 ] loss: 0.009
[ 17 / 40 ] loss: 0.003
[ 18 / 40 ] loss: 0.010
[ 19 / 40 ] loss: 0.010
[ 20 / 40 ] loss: 0.094
[ 21 / 40 ] loss: 0.007
[ 22 / 40 ] loss: 0.034
[ 23 / 40 ] loss: 0.004
[ 24 / 40 ] loss: 0.005
[ 25 / 40 ] loss: 0.007
[ 26 / 40 ] loss: 0.068
[ 27 / 40 ] loss: 0.017
[ 28 / 40 ] loss: 0.006
[ 29 / 40 ] loss: 0.003
[ 30 / 40 ] loss: 0.059
[ 31 / 40 ] loss: 0.003
[ 32 / 40 ] loss: 0.145
[ 33 / 40 ] loss: 0.076
[ 34 / 40 ] loss: 0.007
[ 35 / 40 ] loss: 0.020
[ 36 / 40 ] loss: 0.003
[ 37 / 40 ] loss: 0.054
[ 38 / 40 ] loss: 0.003
[ 39 / 40 ] loss: 0.017
[ 40 / 40 ] loss: 0.074
0.027495304611511527
Accuracy: 0.762712 -- Precision: 0.782946 -- Recall: 0.782946 -- F1: 0.782946 -- AUC: 0.813881
0.6276800400486304 0.6451490642675799 0.6397257006559333 0.641889443789972 0.654139846967493
[113, 82, 39, 57, 189, 58, 178, 218, 172, 152, 66, 188, 185, 181, 195, 51, 91, 146, 40, 215, 63, 72, 119, 184, 101, 194, 74, 23, 227, 201, 200, 60, 95, 233, 192, 123, 202, 237, 186, 31, 7, 18, 187, 171, 132, 204, 214, 125, 45, 16, 22, 124, 109, 34, 232, 41, 169, 167, 225, 234, 166, 55, 128, 61, 32, 68, 219, 2, 149, 76, 207, 35, 43, 209, 62, 158, 196, 73, 168, 11, 230, 115, 93, 129, 59, 220, 154, 121, 142, 208, 231, 116, 131, 42, 147, 37, 136, 87, 191, 112, 213, 173, 175, 150, 10, 139, 221, 75, 183, 15, 199, 24, 193, 143, 47, 176, 103, 38, 54, 81, 228, 56, 9, 8, 97, 96, 78, 27, 25, 85, 212, 50, 44, 110, 30, 71, 177, 14, 159, 84, 86, 46, 65, 100, 216, 26, 190, 67, 236, 134, 52, 117, 89, 222, 211, 210, 144, 137, 153, 94, 6, 140, 79, 223, 48, 12, 205, 120, 53, 17, 226, 19, 170, 36, 155, 174, 164, 3, 151, 69, 217, 180, 88, 13, 92, 99, 165, 163, 162, 64, 138, 28, 29, 98, 203, 130, 77, 104, 229, 107, 108, 21, 197, 156, 157, 114, 126, 127, 161, 179, 118, 224, 141, 105, 90, 70, 33, 182, 5, 106, 83, 133, 235, 20, 206, 135, 122, 145, 80, 198, 49, 148, 102, 111, 4, 160, 1, 156, 127, 91, 61, 15, 124, 44, 130, 216, 235, 170, 66, 53, 67, 12, 14, 154, 196, 126, 176, 168, 76, 54, 88, 129, 201, 166, 94, 161, 200, 158, 78, 199, 211, 81, 29, 73, 184, 237, 26, 179, 45, 32, 134, 116, 75, 164, 20, 56, 24, 84, 8, 132, 183, 59, 57, 58, 202, 1, 192, 46, 42, 121, 34, 206, 217, 48, 155, 207, 13, 223, 18, 185, 140, 23, 107, 37, 138, 52, 172, 30, 106, 22, 227, 149, 47, 82, 125, 112, 114, 95, 188, 175, 115, 69, 16, 70, 25, 225, 177, 79, 195, 74, 109, 146, 186, 232, 43, 137, 80, 77, 3, 27, 105, 83, 60, 190, 220, 101, 10, 113, 210, 40, 122, 193, 39, 141, 108, 98, 221, 49, 197, 171, 150, 87, 119, 50, 62, 139, 99, 167, 93, 21, 209, 55, 173, 65, 38, 194, 204, 2, 189, 41, 102, 152, 36, 213, 131, 203, 228, 162, 198, 9, 143, 218, 11, 157, 219, 144, 230, 234, 224, 111, 96, 5, 120, 85, 51, 71, 136, 180, 117, 169, 229, 7, 208, 222, 19, 214, 226, 178, 147, 103, 133, 165, 236, 31, 187, 4, 104, 72, 148, 68, 215, 63, 35, 151, 191, 182, 110, 174, 145, 64, 89, 123, 142, 17, 90, 100, 233, 6, 159, 135, 231, 160, 128, 28, 212, 97, 118, 163, 153, 92, 181, 205, 33, 86, 147, 229, 219, 41, 92, 34, 134, 45, 200, 225, 195, 85, 210, 173, 8, 136, 54, 207, 35, 95, 108, 230, 90, 149, 227, 53, 129, 56, 26, 181, 28, 148, 143, 197, 20, 228, 166, 86, 9, 25, 224, 128, 96, 3, 87, 170, 161, 97, 174, 47, 142, 124, 62, 160, 198, 183, 145, 205, 138, 126, 152, 27, 164, 6, 191, 75, 12, 139, 175, 55, 84, 21, 201, 153, 22, 235, 192, 18, 154, 119, 93, 60, 51, 67, 237, 33, 24, 44, 104, 232, 151, 186, 133, 63, 109, 68, 176, 52, 13, 32, 203, 30, 71, 19, 169, 162, 80, 77, 39, 46, 188, 231, 66, 144, 167, 215, 59, 172, 193, 211, 209, 1, 40, 4, 184, 132, 146, 36, 204, 218, 212, 165, 31, 206, 48, 117, 214, 69, 82, 15, 141, 116, 114, 49, 23, 118, 182, 178, 81, 106, 185, 2, 158, 123, 159, 196, 64, 43, 157, 233, 125, 131, 220, 111, 213, 38, 208, 107, 190, 177, 171, 199, 105, 103, 127, 5, 102, 101, 155, 121, 29, 10, 76, 236, 83, 226, 78, 58, 217, 11, 57, 88, 194, 98, 50, 113, 137, 61, 110, 17, 74, 120, 202, 221, 7, 79, 65, 89, 222, 112, 150, 91, 168, 100, 156, 180, 14, 16, 99, 70, 73, 130, 42, 135, 122, 140, 216, 72, 189, 234, 223, 94, 179, 37, 187, 115, 163, 199, 113, 231, 51, 109, 187, 232, 98, 208, 163, 198, 105, 13, 45, 116, 128, 190, 3, 117, 92, 82, 209, 115, 218, 58, 143, 213, 188, 183, 130, 123, 55, 147, 205, 118, 154, 204, 67, 77, 216, 185, 156, 68, 146, 2, 110, 155, 132, 167, 40, 181, 223, 235, 236, 150, 191, 162, 49, 229, 65, 189, 101, 157, 212, 148, 220, 122, 7, 86, 12, 186, 170, 182, 230, 194, 57, 94, 184, 211, 19, 164, 106, 151, 121, 95, 207, 90, 100, 25, 14, 62, 50, 174, 16, 165, 73, 54, 41, 166, 234, 168, 176, 124, 9, 26, 203, 15, 206, 159, 120, 233, 224, 66, 192, 63, 17, 80, 85, 145, 175, 36, 141, 71, 125, 43, 52, 20, 46, 129, 81, 178, 22, 201, 72, 152, 134, 219, 87, 112, 227, 99, 138, 48, 38, 75, 221, 172, 177, 4, 210, 64, 215, 88, 47, 173, 21, 217, 74, 200, 37, 93, 1, 89, 214, 228, 222, 135, 61, 70, 226, 127, 29, 31, 179, 39, 53, 69, 136, 11, 153, 171, 102, 27, 79, 225, 202, 139, 197, 195, 103, 142, 35, 6, 28, 59, 83, 97, 18, 193, 30, 96, 23, 180, 111, 160, 119, 114, 42, 149, 44, 8, 10, 32, 137, 158, 76, 133, 169, 104, 91, 34, 78, 33, 24, 56, 161, 140, 5, 144, 131, 84, 107, 60, 196, 126, 108]
[1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]
[1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]
训练集: 947
测试集: 236
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
-------------- start training --------------- 

========= epoch: 0 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.700
[ 2 / 40 ] loss: 0.679
[ 3 / 40 ] loss: 0.669
[ 4 / 40 ] loss: 0.669
[ 5 / 40 ] loss: 0.681
[ 6 / 40 ] loss: 0.757
[ 7 / 40 ] loss: 0.634
[ 8 / 40 ] loss: 0.701
[ 9 / 40 ] loss: 0.656
[ 10 / 40 ] loss: 0.719
[ 11 / 40 ] loss: 0.680
[ 12 / 40 ] loss: 0.664
[ 13 / 40 ] loss: 0.665
[ 14 / 40 ] loss: 0.653
[ 15 / 40 ] loss: 0.701
[ 16 / 40 ] loss: 0.608
[ 17 / 40 ] loss: 0.712
[ 18 / 40 ] loss: 0.681
[ 19 / 40 ] loss: 0.662
[ 20 / 40 ] loss: 0.695
[ 21 / 40 ] loss: 0.668
[ 22 / 40 ] loss: 0.679
[ 23 / 40 ] loss: 0.694
[ 24 / 40 ] loss: 0.692
[ 25 / 40 ] loss: 0.700
[ 26 / 40 ] loss: 0.704
[ 27 / 40 ] loss: 0.651
[ 28 / 40 ] loss: 0.690
[ 29 / 40 ] loss: 0.707
[ 30 / 40 ] loss: 0.616
[ 31 / 40 ] loss: 0.659
[ 32 / 40 ] loss: 0.635
[ 33 / 40 ] loss: 0.645
[ 34 / 40 ] loss: 0.627
[ 35 / 40 ] loss: 0.652
[ 36 / 40 ] loss: 0.744
[ 37 / 40 ] loss: 0.662
[ 38 / 40 ] loss: 0.679
[ 39 / 40 ] loss: 0.653
[ 40 / 40 ] loss: 0.766
0.6777131974697113
Accuracy: 0.618644 -- Precision: 0.590698 -- Recall: 0.984496 -- F1: 0.738372 -- AUC: 0.724915
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[215, 90, 157, 178, 169, 206, 25, 18, 5, 204, 111, 165, 100, 187, 104, 75, 225, 160, 224, 199, 76, 186, 2, 51, 116, 14, 20, 69, 24, 130, 96, 124, 150, 230, 87, 137, 182, 168, 48, 203, 172, 11, 175, 171, 67, 22, 163, 44, 88, 74, 16, 85, 42, 65, 84, 45, 73, 220, 56, 126, 148, 207, 211, 219, 213, 66, 231, 122, 7, 78, 83, 61, 101, 173, 121, 72, 125, 153, 112, 155, 129, 32, 185, 29, 233, 113, 118, 77, 142, 193, 135, 19, 139, 28, 145, 26, 98, 114, 33, 50, 236, 106, 127, 86, 115, 40, 151, 103, 138, 108, 91, 93, 190, 1, 3, 156, 120, 55, 9, 194, 8, 196, 39, 13, 95, 176, 97, 41, 68, 147, 46, 131, 177, 140, 167, 63, 195, 146, 133, 110, 181, 27, 52, 34, 79, 92, 31, 10, 201, 143, 149, 218, 49, 202, 128, 57, 232, 152, 154, 192, 226, 119, 228, 15, 214, 191, 164, 117, 81, 162, 229, 82, 47, 37, 105, 227, 209, 188, 70, 159, 94, 208, 30, 170, 234, 158, 174, 136, 223, 43, 134, 217, 107, 212, 144, 36, 197, 21, 23, 180, 198, 210, 141, 102, 59, 17, 4, 216, 166, 161, 221, 80, 183, 60, 179, 235, 222, 99, 109, 123, 205, 200, 54, 12, 35, 184, 71, 62, 64, 53, 89, 132, 58, 38, 6, 189]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1]
[1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 1 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.708
[ 2 / 40 ] loss: 0.691
[ 3 / 40 ] loss: 0.646
[ 4 / 40 ] loss: 0.624
[ 5 / 40 ] loss: 0.673
[ 6 / 40 ] loss: 0.641
[ 7 / 40 ] loss: 0.625
[ 8 / 40 ] loss: 0.682
[ 9 / 40 ] loss: 0.570
[ 10 / 40 ] loss: 0.615
[ 11 / 40 ] loss: 0.544
[ 12 / 40 ] loss: 0.676
[ 13 / 40 ] loss: 0.569
[ 14 / 40 ] loss: 0.479
[ 15 / 40 ] loss: 0.744
[ 16 / 40 ] loss: 0.682
[ 17 / 40 ] loss: 0.686
[ 18 / 40 ] loss: 0.589
[ 19 / 40 ] loss: 0.630
[ 20 / 40 ] loss: 0.627
[ 21 / 40 ] loss: 0.603
[ 22 / 40 ] loss: 0.637
[ 23 / 40 ] loss: 0.655
[ 24 / 40 ] loss: 0.591
[ 25 / 40 ] loss: 0.566
[ 26 / 40 ] loss: 0.614
[ 27 / 40 ] loss: 0.668
[ 28 / 40 ] loss: 0.598
[ 29 / 40 ] loss: 0.587
[ 30 / 40 ] loss: 0.533
[ 31 / 40 ] loss: 0.605
[ 32 / 40 ] loss: 0.581
[ 33 / 40 ] loss: 0.594
[ 34 / 40 ] loss: 0.544
[ 35 / 40 ] loss: 0.597
[ 36 / 40 ] loss: 0.512
[ 37 / 40 ] loss: 0.550
[ 38 / 40 ] loss: 0.646
[ 39 / 40 ] loss: 0.530
[ 40 / 40 ] loss: 0.690
0.6150832012295723
Accuracy: 0.690678 -- Precision: 0.684211 -- Recall: 0.806202 -- F1: 0.740214 -- AUC: 0.745563
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[161, 120, 209, 74, 155, 16, 174, 47, 49, 38, 18, 168, 10, 115, 236, 148, 204, 58, 41, 137, 123, 195, 139, 101, 138, 229, 81, 57, 185, 8, 152, 54, 75, 112, 73, 30, 13, 28, 207, 125, 15, 167, 164, 105, 189, 14, 194, 181, 211, 128, 67, 61, 165, 166, 62, 180, 85, 126, 191, 82, 52, 179, 156, 95, 141, 103, 119, 64, 154, 129, 77, 190, 42, 215, 217, 153, 59, 231, 142, 20, 83, 37, 187, 163, 108, 199, 91, 182, 48, 109, 124, 159, 232, 227, 40, 176, 149, 27, 206, 107, 234, 134, 171, 196, 192, 46, 53, 118, 32, 50, 143, 36, 66, 79, 31, 78, 197, 205, 104, 106, 144, 114, 110, 44, 5, 84, 21, 93, 63, 235, 132, 3, 222, 19, 4, 169, 127, 17, 2, 35, 214, 130, 80, 230, 94, 223, 29, 226, 6, 55, 7, 136, 158, 9, 71, 133, 111, 86, 69, 213, 131, 202, 116, 203, 193, 178, 26, 135, 173, 210, 33, 162, 228, 98, 146, 186, 70, 11, 96, 60, 188, 218, 170, 172, 97, 1, 224, 102, 90, 39, 198, 145, 113, 100, 160, 183, 184, 117, 25, 34, 212, 200, 221, 24, 225, 22, 219, 89, 88, 72, 51, 99, 220, 122, 92, 150, 43, 177, 87, 76, 147, 201, 233, 12, 56, 65, 23, 157, 45, 140, 68, 175, 151, 208, 121, 216]
[0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1]
[1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 2 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.507
[ 2 / 40 ] loss: 0.500
[ 3 / 40 ] loss: 0.583
[ 4 / 40 ] loss: 0.593
[ 5 / 40 ] loss: 0.545
[ 6 / 40 ] loss: 0.540
[ 7 / 40 ] loss: 0.521
[ 8 / 40 ] loss: 0.433
[ 9 / 40 ] loss: 0.512
[ 10 / 40 ] loss: 0.563
[ 11 / 40 ] loss: 0.772
[ 12 / 40 ] loss: 0.683
[ 13 / 40 ] loss: 0.354
[ 14 / 40 ] loss: 0.557
[ 15 / 40 ] loss: 0.503
[ 16 / 40 ] loss: 0.506
[ 17 / 40 ] loss: 0.776
[ 18 / 40 ] loss: 0.665
[ 19 / 40 ] loss: 0.453
[ 20 / 40 ] loss: 0.397
[ 21 / 40 ] loss: 0.481
[ 22 / 40 ] loss: 0.621
[ 23 / 40 ] loss: 0.757
[ 24 / 40 ] loss: 0.499
[ 25 / 40 ] loss: 0.662
[ 26 / 40 ] loss: 0.382
[ 27 / 40 ] loss: 0.626
[ 28 / 40 ] loss: 0.603
[ 29 / 40 ] loss: 0.544
[ 30 / 40 ] loss: 0.698
[ 31 / 40 ] loss: 0.522
[ 32 / 40 ] loss: 0.569
[ 33 / 40 ] loss: 0.547
[ 34 / 40 ] loss: 0.580
[ 35 / 40 ] loss: 0.546
[ 36 / 40 ] loss: 0.544
[ 37 / 40 ] loss: 0.500
[ 38 / 40 ] loss: 0.749
[ 39 / 40 ] loss: 0.726
[ 40 / 40 ] loss: 0.888
0.5751938544213772
Accuracy: 0.694915 -- Precision: 0.702128 -- Recall: 0.767442 -- F1: 0.733333 -- AUC: 0.742665
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[112, 136, 74, 217, 166, 132, 35, 187, 81, 190, 65, 224, 171, 59, 63, 17, 30, 223, 216, 177, 158, 230, 86, 120, 48, 29, 176, 115, 53, 208, 142, 28, 138, 128, 80, 82, 156, 163, 73, 109, 84, 218, 121, 114, 93, 141, 149, 226, 164, 13, 228, 194, 22, 143, 68, 146, 186, 72, 134, 60, 98, 33, 113, 183, 111, 126, 1, 52, 195, 100, 206, 118, 91, 19, 165, 5, 104, 71, 11, 236, 41, 40, 180, 70, 169, 55, 152, 9, 204, 39, 14, 31, 122, 77, 4, 215, 172, 191, 157, 76, 101, 179, 119, 32, 227, 133, 189, 51, 219, 117, 125, 116, 192, 198, 129, 233, 131, 197, 167, 205, 89, 36, 75, 2, 103, 42, 135, 124, 234, 200, 229, 174, 235, 7, 94, 43, 137, 6, 66, 87, 10, 210, 3, 196, 92, 175, 139, 37, 85, 18, 97, 64, 181, 159, 34, 127, 188, 232, 107, 168, 38, 78, 222, 69, 151, 88, 231, 54, 207, 44, 95, 170, 225, 213, 140, 16, 99, 57, 160, 79, 15, 110, 56, 108, 220, 90, 155, 145, 47, 178, 83, 154, 8, 12, 193, 212, 202, 161, 21, 20, 24, 144, 67, 27, 130, 147, 214, 26, 25, 123, 105, 150, 221, 153, 173, 203, 211, 201, 58, 185, 182, 46, 61, 102, 209, 23, 45, 62, 162, 106, 184, 49, 199, 50, 96, 148]
[1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]
[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 3 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.523
[ 2 / 40 ] loss: 0.533
[ 3 / 40 ] loss: 0.570
[ 4 / 40 ] loss: 0.630
[ 5 / 40 ] loss: 0.559
[ 6 / 40 ] loss: 0.613
[ 7 / 40 ] loss: 0.598
[ 8 / 40 ] loss: 0.649
[ 9 / 40 ] loss: 0.418
[ 10 / 40 ] loss: 0.539
[ 11 / 40 ] loss: 0.515
[ 12 / 40 ] loss: 0.526
[ 13 / 40 ] loss: 0.606
[ 14 / 40 ] loss: 0.578
[ 15 / 40 ] loss: 0.540
[ 16 / 40 ] loss: 0.577
[ 17 / 40 ] loss: 0.383
[ 18 / 40 ] loss: 0.577
[ 19 / 40 ] loss: 0.670
[ 20 / 40 ] loss: 0.435
[ 21 / 40 ] loss: 0.538
[ 22 / 40 ] loss: 0.391
[ 23 / 40 ] loss: 0.438
[ 24 / 40 ] loss: 0.745
[ 25 / 40 ] loss: 0.379
[ 26 / 40 ] loss: 0.436
[ 27 / 40 ] loss: 0.334
[ 28 / 40 ] loss: 0.463
[ 29 / 40 ] loss: 0.405
[ 30 / 40 ] loss: 0.345
[ 31 / 40 ] loss: 0.587
[ 32 / 40 ] loss: 0.477
[ 33 / 40 ] loss: 0.534
[ 34 / 40 ] loss: 0.642
[ 35 / 40 ] loss: 0.683
[ 36 / 40 ] loss: 0.734
[ 37 / 40 ] loss: 0.516
[ 38 / 40 ] loss: 0.604
[ 39 / 40 ] loss: 0.511
[ 40 / 40 ] loss: 0.518
0.5329632423818111
Accuracy: 0.716102 -- Precision: 0.724638 -- Recall: 0.775194 -- F1: 0.749064 -- AUC: 0.779396
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[97, 206, 54, 75, 198, 77, 129, 137, 16, 171, 120, 228, 187, 18, 155, 4, 22, 217, 166, 21, 136, 117, 90, 227, 53, 178, 144, 200, 88, 32, 153, 124, 98, 17, 202, 168, 110, 86, 62, 145, 209, 95, 225, 113, 65, 83, 55, 158, 208, 226, 143, 47, 201, 28, 205, 7, 134, 150, 81, 42, 149, 233, 214, 154, 52, 135, 151, 197, 123, 235, 148, 71, 11, 109, 15, 185, 87, 2, 41, 3, 93, 72, 27, 122, 6, 102, 30, 186, 161, 165, 140, 212, 146, 215, 189, 223, 85, 44, 108, 184, 115, 163, 67, 220, 57, 91, 69, 59, 78, 128, 125, 230, 79, 229, 207, 19, 48, 39, 224, 196, 36, 40, 236, 132, 221, 231, 13, 29, 20, 162, 26, 188, 147, 218, 182, 131, 31, 33, 181, 66, 51, 14, 176, 63, 203, 114, 111, 56, 34, 45, 121, 70, 210, 12, 133, 173, 119, 112, 141, 80, 190, 38, 152, 92, 96, 195, 10, 177, 180, 157, 164, 25, 104, 159, 76, 23, 183, 60, 169, 46, 127, 194, 216, 118, 100, 192, 106, 43, 175, 64, 156, 199, 82, 107, 172, 37, 211, 89, 74, 73, 8, 24, 142, 1, 58, 232, 126, 49, 61, 234, 99, 191, 179, 105, 204, 68, 193, 35, 5, 94, 167, 9, 170, 222, 103, 138, 130, 139, 213, 101, 160, 219, 116, 50, 174, 84]
[0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]
[0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 4 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.503
[ 2 / 40 ] loss: 0.395
[ 3 / 40 ] loss: 0.472
[ 4 / 40 ] loss: 0.297
[ 5 / 40 ] loss: 0.401
[ 6 / 40 ] loss: 0.655
[ 7 / 40 ] loss: 0.333
[ 8 / 40 ] loss: 0.542
[ 9 / 40 ] loss: 0.410
[ 10 / 40 ] loss: 0.394
[ 11 / 40 ] loss: 0.316
[ 12 / 40 ] loss: 0.377
[ 13 / 40 ] loss: 0.431
[ 14 / 40 ] loss: 0.420
[ 15 / 40 ] loss: 0.675
[ 16 / 40 ] loss: 0.507
[ 17 / 40 ] loss: 0.590
[ 18 / 40 ] loss: 0.466
[ 19 / 40 ] loss: 0.511
[ 20 / 40 ] loss: 0.341
[ 21 / 40 ] loss: 0.259
[ 22 / 40 ] loss: 0.235
[ 23 / 40 ] loss: 0.439
[ 24 / 40 ] loss: 0.429
[ 25 / 40 ] loss: 0.391
[ 26 / 40 ] loss: 0.616
[ 27 / 40 ] loss: 0.454
[ 28 / 40 ] loss: 0.265
[ 29 / 40 ] loss: 0.482
[ 30 / 40 ] loss: 0.431
[ 31 / 40 ] loss: 0.475
[ 32 / 40 ] loss: 0.598
[ 33 / 40 ] loss: 0.375
[ 34 / 40 ] loss: 0.338
[ 35 / 40 ] loss: 0.315
[ 36 / 40 ] loss: 0.421
[ 37 / 40 ] loss: 0.306
[ 38 / 40 ] loss: 0.837
[ 39 / 40 ] loss: 0.600
[ 40 / 40 ] loss: 0.558
0.4465153716504574
Accuracy: 0.758475 -- Precision: 0.800000 -- Recall: 0.744186 -- F1: 0.771084 -- AUC: 0.809679
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[183, 185, 47, 71, 107, 3, 199, 157, 217, 205, 129, 42, 164, 34, 173, 4, 211, 101, 171, 29, 20, 15, 25, 228, 223, 26, 36, 12, 87, 126, 202, 35, 218, 90, 141, 212, 127, 21, 159, 194, 232, 96, 68, 219, 174, 32, 139, 44, 8, 43, 213, 123, 88, 66, 104, 64, 153, 63, 2, 196, 94, 118, 169, 70, 170, 222, 14, 1, 209, 206, 208, 117, 61, 11, 180, 77, 231, 40, 179, 80, 9, 193, 160, 72, 166, 85, 132, 56, 99, 100, 18, 37, 105, 200, 113, 137, 74, 221, 115, 62, 82, 167, 86, 165, 93, 216, 79, 135, 17, 144, 156, 46, 6, 109, 121, 112, 184, 55, 220, 5, 13, 7, 91, 178, 161, 198, 201, 234, 54, 76, 120, 57, 163, 236, 28, 190, 168, 195, 148, 102, 143, 175, 19, 48, 197, 226, 203, 75, 147, 124, 30, 186, 45, 229, 103, 52, 146, 27, 49, 150, 97, 33, 128, 106, 210, 155, 187, 83, 10, 69, 23, 119, 110, 149, 130, 116, 224, 214, 172, 111, 41, 73, 138, 31, 58, 131, 182, 176, 108, 207, 53, 95, 98, 134, 204, 215, 233, 133, 59, 24, 162, 230, 152, 154, 89, 84, 78, 151, 22, 65, 158, 122, 38, 192, 50, 188, 225, 191, 114, 177, 136, 227, 125, 92, 140, 181, 39, 16, 235, 145, 189, 60, 81, 67, 51, 142]
[0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]
[1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 5 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.249
[ 2 / 40 ] loss: 0.504
[ 3 / 40 ] loss: 0.275
[ 4 / 40 ] loss: 0.499
[ 5 / 40 ] loss: 0.445
[ 6 / 40 ] loss: 0.236
[ 7 / 40 ] loss: 0.411
[ 8 / 40 ] loss: 0.360
[ 9 / 40 ] loss: 0.383
[ 10 / 40 ] loss: 0.334
[ 11 / 40 ] loss: 0.352
[ 12 / 40 ] loss: 0.263
[ 13 / 40 ] loss: 0.281
[ 14 / 40 ] loss: 0.461
[ 15 / 40 ] loss: 0.330
[ 16 / 40 ] loss: 0.317
[ 17 / 40 ] loss: 0.369
[ 18 / 40 ] loss: 0.385
[ 19 / 40 ] loss: 0.356
[ 20 / 40 ] loss: 0.326
[ 21 / 40 ] loss: 0.568
[ 22 / 40 ] loss: 0.282
[ 23 / 40 ] loss: 0.242
[ 24 / 40 ] loss: 0.376
[ 25 / 40 ] loss: 0.308
[ 26 / 40 ] loss: 0.482
[ 27 / 40 ] loss: 0.258
[ 28 / 40 ] loss: 0.343
[ 29 / 40 ] loss: 0.390
[ 30 / 40 ] loss: 0.272
[ 31 / 40 ] loss: 0.618
[ 32 / 40 ] loss: 0.389
[ 33 / 40 ] loss: 0.633
[ 34 / 40 ] loss: 0.328
[ 35 / 40 ] loss: 0.356
[ 36 / 40 ] loss: 0.499
[ 37 / 40 ] loss: 0.669
[ 38 / 40 ] loss: 0.490
[ 39 / 40 ] loss: 0.348
[ 40 / 40 ] loss: 0.119
0.3776481909677386
Accuracy: 0.728814 -- Precision: 0.724138 -- Recall: 0.813953 -- F1: 0.766423 -- AUC: 0.775049
========= epoch: 6 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.474
[ 2 / 40 ] loss: 0.360
[ 3 / 40 ] loss: 0.284
[ 4 / 40 ] loss: 0.355
[ 5 / 40 ] loss: 0.262
[ 6 / 40 ] loss: 0.448
[ 7 / 40 ] loss: 0.365
[ 8 / 40 ] loss: 0.457
[ 9 / 40 ] loss: 0.759
[ 10 / 40 ] loss: 0.368
[ 11 / 40 ] loss: 0.418
[ 12 / 40 ] loss: 0.652
[ 13 / 40 ] loss: 0.305
[ 14 / 40 ] loss: 0.231
[ 15 / 40 ] loss: 0.411
[ 16 / 40 ] loss: 0.475
[ 17 / 40 ] loss: 0.363
[ 18 / 40 ] loss: 0.295
[ 19 / 40 ] loss: 0.334
[ 20 / 40 ] loss: 0.455
[ 21 / 40 ] loss: 0.288
[ 22 / 40 ] loss: 0.542
[ 23 / 40 ] loss: 0.385
[ 24 / 40 ] loss: 0.213
[ 25 / 40 ] loss: 0.227
[ 26 / 40 ] loss: 0.281
[ 27 / 40 ] loss: 0.325
[ 28 / 40 ] loss: 0.397
[ 29 / 40 ] loss: 0.394
[ 30 / 40 ] loss: 0.241
[ 31 / 40 ] loss: 0.345
[ 32 / 40 ] loss: 0.413
[ 33 / 40 ] loss: 0.318
[ 34 / 40 ] loss: 0.207
[ 35 / 40 ] loss: 0.267
[ 36 / 40 ] loss: 0.125
[ 37 / 40 ] loss: 0.136
[ 38 / 40 ] loss: 0.193
[ 39 / 40 ] loss: 0.422
[ 40 / 40 ] loss: 0.196
0.3496193550527096
Accuracy: 0.741525 -- Precision: 0.739437 -- Recall: 0.813953 -- F1: 0.774908 -- AUC: 0.781569
========= epoch: 7 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.223
[ 2 / 40 ] loss: 0.322
[ 3 / 40 ] loss: 0.312
[ 4 / 40 ] loss: 0.576
[ 5 / 40 ] loss: 0.447
[ 6 / 40 ] loss: 0.264
[ 7 / 40 ] loss: 0.209
[ 8 / 40 ] loss: 0.224
[ 9 / 40 ] loss: 0.181
[ 10 / 40 ] loss: 0.299
[ 11 / 40 ] loss: 0.287
[ 12 / 40 ] loss: 0.415
[ 13 / 40 ] loss: 0.365
[ 14 / 40 ] loss: 0.230
[ 15 / 40 ] loss: 0.308
[ 16 / 40 ] loss: 0.310
[ 17 / 40 ] loss: 0.196
[ 18 / 40 ] loss: 0.514
[ 19 / 40 ] loss: 0.306
[ 20 / 40 ] loss: 0.468
[ 21 / 40 ] loss: 0.113
[ 22 / 40 ] loss: 0.274
[ 23 / 40 ] loss: 0.274
[ 24 / 40 ] loss: 0.082
[ 25 / 40 ] loss: 0.305
[ 26 / 40 ] loss: 0.368
[ 27 / 40 ] loss: 0.195
[ 28 / 40 ] loss: 0.365
[ 29 / 40 ] loss: 0.141
[ 30 / 40 ] loss: 0.531
[ 31 / 40 ] loss: 0.361
[ 32 / 40 ] loss: 0.443
[ 33 / 40 ] loss: 0.123
[ 34 / 40 ] loss: 0.236
[ 35 / 40 ] loss: 0.361
[ 36 / 40 ] loss: 0.338
[ 37 / 40 ] loss: 0.131
[ 38 / 40 ] loss: 0.427
[ 39 / 40 ] loss: 0.371
[ 40 / 40 ] loss: 0.195
0.3022351384162903
Accuracy: 0.737288 -- Precision: 0.744526 -- Recall: 0.790698 -- F1: 0.766917 -- AUC: 0.795262
========= epoch: 8 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.192
[ 2 / 40 ] loss: 0.298
[ 3 / 40 ] loss: 0.263
[ 4 / 40 ] loss: 0.137
[ 5 / 40 ] loss: 0.064
[ 6 / 40 ] loss: 0.193
[ 7 / 40 ] loss: 0.184
[ 8 / 40 ] loss: 0.174
[ 9 / 40 ] loss: 0.295
[ 10 / 40 ] loss: 0.447
[ 11 / 40 ] loss: 0.179
[ 12 / 40 ] loss: 0.522
[ 13 / 40 ] loss: 0.186
[ 14 / 40 ] loss: 0.201
[ 15 / 40 ] loss: 0.512
[ 16 / 40 ] loss: 0.190
[ 17 / 40 ] loss: 0.307
[ 18 / 40 ] loss: 0.110
[ 19 / 40 ] loss: 0.223
[ 20 / 40 ] loss: 0.296
[ 21 / 40 ] loss: 0.105
[ 22 / 40 ] loss: 0.174
[ 23 / 40 ] loss: 0.450
[ 24 / 40 ] loss: 0.353
[ 25 / 40 ] loss: 0.174
[ 26 / 40 ] loss: 0.325
[ 27 / 40 ] loss: 0.178
[ 28 / 40 ] loss: 0.675
[ 29 / 40 ] loss: 0.119
[ 30 / 40 ] loss: 0.293
[ 31 / 40 ] loss: 0.180
[ 32 / 40 ] loss: 0.249
[ 33 / 40 ] loss: 0.258
[ 34 / 40 ] loss: 0.237
[ 35 / 40 ] loss: 0.399
[ 36 / 40 ] loss: 0.403
[ 37 / 40 ] loss: 0.199
[ 38 / 40 ] loss: 0.191
[ 39 / 40 ] loss: 0.234
[ 40 / 40 ] loss: 0.318
0.2621105656027794
Accuracy: 0.707627 -- Precision: 0.727273 -- Recall: 0.744186 -- F1: 0.735632 -- AUC: 0.790553
========= epoch: 9 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.141
[ 2 / 40 ] loss: 0.146
[ 3 / 40 ] loss: 0.232
[ 4 / 40 ] loss: 0.178
[ 5 / 40 ] loss: 0.526
[ 6 / 40 ] loss: 0.423
[ 7 / 40 ] loss: 0.074
[ 8 / 40 ] loss: 0.249
[ 9 / 40 ] loss: 0.195
[ 10 / 40 ] loss: 0.280
[ 11 / 40 ] loss: 0.171
[ 12 / 40 ] loss: 0.069
[ 13 / 40 ] loss: 0.209
[ 14 / 40 ] loss: 0.083
[ 15 / 40 ] loss: 0.049
[ 16 / 40 ] loss: 0.261
[ 17 / 40 ] loss: 0.182
[ 18 / 40 ] loss: 0.188
[ 19 / 40 ] loss: 0.240
[ 20 / 40 ] loss: 0.188
[ 21 / 40 ] loss: 0.227
[ 22 / 40 ] loss: 0.125
[ 23 / 40 ] loss: 0.057
[ 24 / 40 ] loss: 0.284
[ 25 / 40 ] loss: 0.089
[ 26 / 40 ] loss: 0.474
[ 27 / 40 ] loss: 0.053
[ 28 / 40 ] loss: 0.079
[ 29 / 40 ] loss: 0.099
[ 30 / 40 ] loss: 0.465
[ 31 / 40 ] loss: 0.634
[ 32 / 40 ] loss: 0.376
[ 33 / 40 ] loss: 0.404
[ 34 / 40 ] loss: 0.096
[ 35 / 40 ] loss: 0.284
[ 36 / 40 ] loss: 0.288
[ 37 / 40 ] loss: 0.477
[ 38 / 40 ] loss: 0.403
[ 39 / 40 ] loss: 0.319
[ 40 / 40 ] loss: 0.126
0.23610401675105094
Accuracy: 0.762712 -- Precision: 0.748299 -- Recall: 0.852713 -- F1: 0.797101 -- AUC: 0.791567
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[200, 33, 220, 58, 107, 167, 129, 99, 216, 7, 147, 204, 17, 212, 188, 235, 34, 133, 65, 93, 123, 67, 95, 56, 142, 119, 55, 113, 112, 100, 40, 207, 81, 105, 218, 217, 194, 185, 101, 205, 172, 86, 130, 70, 173, 29, 45, 21, 38, 44, 181, 199, 60, 154, 206, 110, 28, 85, 158, 149, 229, 134, 10, 19, 126, 48, 32, 49, 223, 109, 159, 108, 63, 96, 1, 182, 90, 187, 227, 156, 189, 219, 16, 203, 171, 53, 27, 59, 151, 84, 152, 157, 120, 69, 128, 166, 25, 175, 228, 104, 39, 215, 74, 115, 51, 111, 20, 79, 162, 57, 226, 179, 31, 92, 68, 195, 50, 11, 5, 26, 87, 76, 23, 43, 197, 139, 170, 82, 163, 47, 2, 178, 8, 77, 91, 230, 37, 61, 18, 97, 66, 231, 127, 12, 4, 224, 148, 174, 136, 83, 15, 168, 103, 114, 234, 118, 211, 137, 106, 192, 88, 94, 155, 73, 121, 42, 41, 46, 22, 150, 209, 222, 146, 145, 221, 144, 116, 177, 190, 193, 208, 36, 214, 213, 54, 186, 132, 196, 210, 201, 35, 122, 78, 233, 3, 138, 52, 14, 143, 236, 169, 24, 72, 202, 180, 183, 102, 131, 161, 160, 184, 62, 125, 64, 135, 9, 30, 98, 176, 75, 165, 232, 117, 6, 124, 140, 141, 13, 80, 164, 89, 71, 191, 225, 198, 153]
[1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1]
[1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 10 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.166
[ 2 / 40 ] loss: 0.153
[ 3 / 40 ] loss: 0.070
[ 4 / 40 ] loss: 0.388
[ 5 / 40 ] loss: 0.206
[ 6 / 40 ] loss: 0.086
[ 7 / 40 ] loss: 0.114
[ 8 / 40 ] loss: 0.108
[ 9 / 40 ] loss: 0.312
[ 10 / 40 ] loss: 0.353
[ 11 / 40 ] loss: 0.334
[ 12 / 40 ] loss: 0.136
[ 13 / 40 ] loss: 0.504
[ 14 / 40 ] loss: 0.100
[ 15 / 40 ] loss: 0.282
[ 16 / 40 ] loss: 0.188
[ 17 / 40 ] loss: 0.080
[ 18 / 40 ] loss: 0.313
[ 19 / 40 ] loss: 0.426
[ 20 / 40 ] loss: 0.065
[ 21 / 40 ] loss: 0.142
[ 22 / 40 ] loss: 0.127
[ 23 / 40 ] loss: 0.255
[ 24 / 40 ] loss: 0.446
[ 25 / 40 ] loss: 0.147
[ 26 / 40 ] loss: 0.056
[ 27 / 40 ] loss: 0.317
[ 28 / 40 ] loss: 0.598
[ 29 / 40 ] loss: 0.475
[ 30 / 40 ] loss: 0.379
[ 31 / 40 ] loss: 0.072
[ 32 / 40 ] loss: 0.468
[ 33 / 40 ] loss: 0.097
[ 34 / 40 ] loss: 0.132
[ 35 / 40 ] loss: 0.152
[ 36 / 40 ] loss: 0.221
[ 37 / 40 ] loss: 0.232
[ 38 / 40 ] loss: 0.209
[ 39 / 40 ] loss: 0.131
[ 40 / 40 ] loss: 0.162
0.23007195256650448
Accuracy: 0.750000 -- Precision: 0.739726 -- Recall: 0.837209 -- F1: 0.785455 -- AUC: 0.793596
========= epoch: 11 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.164
[ 2 / 40 ] loss: 0.254
[ 3 / 40 ] loss: 0.107
[ 4 / 40 ] loss: 0.084
[ 5 / 40 ] loss: 0.219
[ 6 / 40 ] loss: 0.167
[ 7 / 40 ] loss: 0.276
[ 8 / 40 ] loss: 0.283
[ 9 / 40 ] loss: 0.076
[ 10 / 40 ] loss: 0.216
[ 11 / 40 ] loss: 0.177
[ 12 / 40 ] loss: 0.228
[ 13 / 40 ] loss: 0.219
[ 14 / 40 ] loss: 0.226
[ 15 / 40 ] loss: 0.098
[ 16 / 40 ] loss: 0.062
[ 17 / 40 ] loss: 0.421
[ 18 / 40 ] loss: 0.060
[ 19 / 40 ] loss: 0.152
[ 20 / 40 ] loss: 0.268
[ 21 / 40 ] loss: 0.072
[ 22 / 40 ] loss: 0.263
[ 23 / 40 ] loss: 0.100
[ 24 / 40 ] loss: 0.333
[ 25 / 40 ] loss: 0.185
[ 26 / 40 ] loss: 0.097
[ 27 / 40 ] loss: 0.393
[ 28 / 40 ] loss: 0.211
[ 29 / 40 ] loss: 0.092
[ 30 / 40 ] loss: 0.047
[ 31 / 40 ] loss: 0.186
[ 32 / 40 ] loss: 0.213
[ 33 / 40 ] loss: 0.124
[ 34 / 40 ] loss: 0.090
[ 35 / 40 ] loss: 0.056
[ 36 / 40 ] loss: 0.226
[ 37 / 40 ] loss: 0.249
[ 38 / 40 ] loss: 0.250
[ 39 / 40 ] loss: 0.421
[ 40 / 40 ] loss: 0.505
0.1966718058101833
Accuracy: 0.737288 -- Precision: 0.716129 -- Recall: 0.860465 -- F1: 0.781690 -- AUC: 0.765268
========= epoch: 12 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.243
[ 2 / 40 ] loss: 0.435
[ 3 / 40 ] loss: 0.044
[ 4 / 40 ] loss: 0.557
[ 5 / 40 ] loss: 0.116
[ 6 / 40 ] loss: 0.049
[ 7 / 40 ] loss: 0.373
[ 8 / 40 ] loss: 0.097
[ 9 / 40 ] loss: 0.097
[ 10 / 40 ] loss: 0.093
[ 11 / 40 ] loss: 0.132
[ 12 / 40 ] loss: 0.353
[ 13 / 40 ] loss: 0.124
[ 14 / 40 ] loss: 0.206
[ 15 / 40 ] loss: 0.094
[ 16 / 40 ] loss: 0.060
[ 17 / 40 ] loss: 0.333
[ 18 / 40 ] loss: 0.193
[ 19 / 40 ] loss: 0.080
[ 20 / 40 ] loss: 0.477
[ 21 / 40 ] loss: 0.206
[ 22 / 40 ] loss: 0.064
[ 23 / 40 ] loss: 0.051
[ 24 / 40 ] loss: 0.180
[ 25 / 40 ] loss: 0.352
[ 26 / 40 ] loss: 0.191
[ 27 / 40 ] loss: 0.098
[ 28 / 40 ] loss: 0.191
[ 29 / 40 ] loss: 0.226
[ 30 / 40 ] loss: 0.329
[ 31 / 40 ] loss: 0.072
[ 32 / 40 ] loss: 0.036
[ 33 / 40 ] loss: 0.249
[ 34 / 40 ] loss: 0.093
[ 35 / 40 ] loss: 0.059
[ 36 / 40 ] loss: 0.255
[ 37 / 40 ] loss: 0.153
[ 38 / 40 ] loss: 0.244
[ 39 / 40 ] loss: 0.334
[ 40 / 40 ] loss: 0.078
0.19036955880001188
Accuracy: 0.720339 -- Precision: 0.760331 -- Recall: 0.713178 -- F1: 0.736000 -- AUC: 0.782583
========= epoch: 13 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.432
[ 2 / 40 ] loss: 0.112
[ 3 / 40 ] loss: 0.147
[ 4 / 40 ] loss: 0.207
[ 5 / 40 ] loss: 0.202
[ 6 / 40 ] loss: 0.111
[ 7 / 40 ] loss: 0.093
[ 8 / 40 ] loss: 0.113
[ 9 / 40 ] loss: 0.083
[ 10 / 40 ] loss: 0.220
[ 11 / 40 ] loss: 0.151
[ 12 / 40 ] loss: 0.052
[ 13 / 40 ] loss: 0.146
[ 14 / 40 ] loss: 0.154
[ 15 / 40 ] loss: 0.054
[ 16 / 40 ] loss: 0.113
[ 17 / 40 ] loss: 0.085
[ 18 / 40 ] loss: 0.108
[ 19 / 40 ] loss: 0.128
[ 20 / 40 ] loss: 0.227
[ 21 / 40 ] loss: 0.225
[ 22 / 40 ] loss: 0.227
[ 23 / 40 ] loss: 0.054
[ 24 / 40 ] loss: 0.421
[ 25 / 40 ] loss: 0.055
[ 26 / 40 ] loss: 0.088
[ 27 / 40 ] loss: 0.143
[ 28 / 40 ] loss: 0.229
[ 29 / 40 ] loss: 0.067
[ 30 / 40 ] loss: 0.048
[ 31 / 40 ] loss: 0.174
[ 32 / 40 ] loss: 0.259
[ 33 / 40 ] loss: 0.206
[ 34 / 40 ] loss: 0.048
[ 35 / 40 ] loss: 0.355
[ 36 / 40 ] loss: 0.476
[ 37 / 40 ] loss: 0.191
[ 38 / 40 ] loss: 0.160
[ 39 / 40 ] loss: 0.074
[ 40 / 40 ] loss: 0.073
0.16272335033863783
Accuracy: 0.745763 -- Precision: 0.737931 -- Recall: 0.829457 -- F1: 0.781022 -- AUC: 0.792944
========= epoch: 14 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.121
[ 2 / 40 ] loss: 0.132
[ 3 / 40 ] loss: 0.190
[ 4 / 40 ] loss: 0.154
[ 5 / 40 ] loss: 0.134
[ 6 / 40 ] loss: 0.108
[ 7 / 40 ] loss: 0.101
[ 8 / 40 ] loss: 0.159
[ 9 / 40 ] loss: 0.215
[ 10 / 40 ] loss: 0.149
[ 11 / 40 ] loss: 0.115
[ 12 / 40 ] loss: 0.212
[ 13 / 40 ] loss: 0.096
[ 14 / 40 ] loss: 0.051
[ 15 / 40 ] loss: 0.116
[ 16 / 40 ] loss: 0.049
[ 17 / 40 ] loss: 0.106
[ 18 / 40 ] loss: 0.060
[ 19 / 40 ] loss: 0.096
[ 20 / 40 ] loss: 0.176
[ 21 / 40 ] loss: 0.242
[ 22 / 40 ] loss: 0.112
[ 23 / 40 ] loss: 0.052
[ 24 / 40 ] loss: 0.345
[ 25 / 40 ] loss: 0.160
[ 26 / 40 ] loss: 0.235
[ 27 / 40 ] loss: 0.053
[ 28 / 40 ] loss: 0.067
[ 29 / 40 ] loss: 0.055
[ 30 / 40 ] loss: 0.230
[ 31 / 40 ] loss: 0.027
[ 32 / 40 ] loss: 0.212
[ 33 / 40 ] loss: 0.209
[ 34 / 40 ] loss: 0.251
[ 35 / 40 ] loss: 0.170
[ 36 / 40 ] loss: 0.078
[ 37 / 40 ] loss: 0.312
[ 38 / 40 ] loss: 0.559
[ 39 / 40 ] loss: 0.329
[ 40 / 40 ] loss: 0.124
0.15904062474146485
Accuracy: 0.762712 -- Precision: 0.847619 -- Recall: 0.689922 -- F1: 0.760684 -- AUC: 0.818518
========= epoch: 15 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.033
[ 2 / 40 ] loss: 0.105
[ 3 / 40 ] loss: 0.263
[ 4 / 40 ] loss: 0.053
[ 5 / 40 ] loss: 0.071
[ 6 / 40 ] loss: 0.157
[ 7 / 40 ] loss: 0.185
[ 8 / 40 ] loss: 0.120
[ 9 / 40 ] loss: 0.038
[ 10 / 40 ] loss: 0.345
[ 11 / 40 ] loss: 0.080
[ 12 / 40 ] loss: 0.088
[ 13 / 40 ] loss: 0.065
[ 14 / 40 ] loss: 0.139
[ 15 / 40 ] loss: 0.051
[ 16 / 40 ] loss: 0.110
[ 17 / 40 ] loss: 0.188
[ 18 / 40 ] loss: 0.026
[ 19 / 40 ] loss: 0.266
[ 20 / 40 ] loss: 0.152
[ 21 / 40 ] loss: 0.411
[ 22 / 40 ] loss: 0.059
[ 23 / 40 ] loss: 0.523
[ 24 / 40 ] loss: 0.033
[ 25 / 40 ] loss: 0.271
[ 26 / 40 ] loss: 0.183
[ 27 / 40 ] loss: 0.161
[ 28 / 40 ] loss: 0.276
[ 29 / 40 ] loss: 0.093
[ 30 / 40 ] loss: 0.214
[ 31 / 40 ] loss: 0.114
[ 32 / 40 ] loss: 0.097
[ 33 / 40 ] loss: 0.171
[ 34 / 40 ] loss: 0.116
[ 35 / 40 ] loss: 0.442
[ 36 / 40 ] loss: 0.126
[ 37 / 40 ] loss: 0.136
[ 38 / 40 ] loss: 0.136
[ 39 / 40 ] loss: 0.226
[ 40 / 40 ] loss: 0.064
0.1597059522755444
Accuracy: 0.703390 -- Precision: 0.687898 -- Recall: 0.837209 -- F1: 0.755245 -- AUC: 0.794175
========= epoch: 16 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.120
[ 2 / 40 ] loss: 0.160
[ 3 / 40 ] loss: 0.239
[ 4 / 40 ] loss: 0.045
[ 5 / 40 ] loss: 0.113
[ 6 / 40 ] loss: 0.105
[ 7 / 40 ] loss: 0.072
[ 8 / 40 ] loss: 0.076
[ 9 / 40 ] loss: 0.067
[ 10 / 40 ] loss: 0.182
[ 11 / 40 ] loss: 0.136
[ 12 / 40 ] loss: 0.078
[ 13 / 40 ] loss: 0.037
[ 14 / 40 ] loss: 0.041
[ 15 / 40 ] loss: 0.041
[ 16 / 40 ] loss: 0.031
[ 17 / 40 ] loss: 0.197
[ 18 / 40 ] loss: 0.118
[ 19 / 40 ] loss: 0.079
[ 20 / 40 ] loss: 0.093
[ 21 / 40 ] loss: 0.384
[ 22 / 40 ] loss: 0.219
[ 23 / 40 ] loss: 0.020
[ 24 / 40 ] loss: 0.076
[ 25 / 40 ] loss: 0.301
[ 26 / 40 ] loss: 0.117
[ 27 / 40 ] loss: 0.176
[ 28 / 40 ] loss: 0.247
[ 29 / 40 ] loss: 0.317
[ 30 / 40 ] loss: 0.272
[ 31 / 40 ] loss: 0.151
[ 32 / 40 ] loss: 0.111
[ 33 / 40 ] loss: 0.078
[ 34 / 40 ] loss: 0.113
[ 35 / 40 ] loss: 0.427
[ 36 / 40 ] loss: 0.372
[ 37 / 40 ] loss: 0.120
[ 38 / 40 ] loss: 0.180
[ 39 / 40 ] loss: 0.104
[ 40 / 40 ] loss: 0.024
0.14601253238506615
Accuracy: 0.758475 -- Precision: 0.753521 -- Recall: 0.829457 -- F1: 0.789668 -- AUC: 0.838368
========= epoch: 17 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.027
[ 2 / 40 ] loss: 0.044
[ 3 / 40 ] loss: 0.136
[ 4 / 40 ] loss: 0.150
[ 5 / 40 ] loss: 0.051
[ 6 / 40 ] loss: 0.023
[ 7 / 40 ] loss: 0.162
[ 8 / 40 ] loss: 0.203
[ 9 / 40 ] loss: 0.057
[ 10 / 40 ] loss: 0.027
[ 11 / 40 ] loss: 0.035
[ 12 / 40 ] loss: 0.033
[ 13 / 40 ] loss: 0.100
[ 14 / 40 ] loss: 0.041
[ 15 / 40 ] loss: 0.079
[ 16 / 40 ] loss: 0.100
[ 17 / 40 ] loss: 0.085
[ 18 / 40 ] loss: 0.031
[ 19 / 40 ] loss: 0.239
[ 20 / 40 ] loss: 0.021
[ 21 / 40 ] loss: 0.106
[ 22 / 40 ] loss: 0.119
[ 23 / 40 ] loss: 0.066
[ 24 / 40 ] loss: 0.026
[ 25 / 40 ] loss: 0.295
[ 26 / 40 ] loss: 0.028
[ 27 / 40 ] loss: 0.336
[ 28 / 40 ] loss: 0.157
[ 29 / 40 ] loss: 0.106
[ 30 / 40 ] loss: 0.149
[ 31 / 40 ] loss: 0.232
[ 32 / 40 ] loss: 0.221
[ 33 / 40 ] loss: 0.214
[ 34 / 40 ] loss: 0.054
[ 35 / 40 ] loss: 0.041
[ 36 / 40 ] loss: 0.190
[ 37 / 40 ] loss: 0.038
[ 38 / 40 ] loss: 0.027
[ 39 / 40 ] loss: 0.077
[ 40 / 40 ] loss: 0.390
0.11286392589099706
Accuracy: 0.783898 -- Precision: 0.819672 -- Recall: 0.775194 -- F1: 0.796813 -- AUC: 0.824350
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[43, 66, 31, 61, 79, 6, 91, 123, 142, 149, 14, 16, 139, 13, 146, 25, 102, 159, 154, 1, 162, 214, 118, 145, 160, 78, 202, 46, 168, 83, 7, 76, 74, 148, 189, 198, 193, 152, 73, 126, 167, 228, 100, 32, 49, 101, 17, 203, 94, 2, 135, 12, 28, 93, 80, 151, 55, 9, 69, 10, 23, 4, 110, 63, 134, 223, 40, 217, 153, 220, 177, 75, 171, 89, 172, 150, 53, 85, 54, 119, 191, 200, 58, 60, 197, 144, 233, 206, 199, 48, 51, 179, 33, 112, 137, 44, 155, 38, 117, 207, 125, 129, 96, 41, 115, 127, 138, 52, 68, 156, 227, 11, 182, 34, 208, 224, 181, 136, 18, 124, 57, 20, 107, 84, 163, 98, 205, 131, 183, 215, 82, 166, 174, 175, 201, 86, 113, 30, 196, 62, 111, 164, 141, 39, 221, 21, 212, 27, 165, 99, 92, 173, 211, 186, 187, 170, 133, 232, 22, 226, 230, 37, 147, 176, 114, 87, 70, 97, 24, 190, 222, 178, 122, 219, 130, 109, 67, 180, 120, 56, 65, 45, 210, 42, 105, 88, 225, 231, 104, 185, 71, 3, 132, 209, 64, 216, 236, 59, 106, 90, 213, 35, 184, 121, 95, 188, 143, 194, 81, 26, 116, 218, 19, 195, 103, 169, 235, 192, 140, 77, 5, 36, 47, 234, 15, 161, 72, 50, 157, 204, 108, 29, 8, 128, 158, 229]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 18 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.064
[ 2 / 40 ] loss: 0.044
[ 3 / 40 ] loss: 0.395
[ 4 / 40 ] loss: 0.041
[ 5 / 40 ] loss: 0.211
[ 6 / 40 ] loss: 0.104
[ 7 / 40 ] loss: 0.034
[ 8 / 40 ] loss: 0.075
[ 9 / 40 ] loss: 0.189
[ 10 / 40 ] loss: 0.122
[ 11 / 40 ] loss: 0.313
[ 12 / 40 ] loss: 0.063
[ 13 / 40 ] loss: 0.019
[ 14 / 40 ] loss: 0.027
[ 15 / 40 ] loss: 0.068
[ 16 / 40 ] loss: 0.195
[ 17 / 40 ] loss: 0.026
[ 18 / 40 ] loss: 0.018
[ 19 / 40 ] loss: 0.183
[ 20 / 40 ] loss: 0.036
[ 21 / 40 ] loss: 0.142
[ 22 / 40 ] loss: 0.126
[ 23 / 40 ] loss: 0.024
[ 24 / 40 ] loss: 0.023
[ 25 / 40 ] loss: 0.051
[ 26 / 40 ] loss: 0.513
[ 27 / 40 ] loss: 0.273
[ 28 / 40 ] loss: 0.091
[ 29 / 40 ] loss: 0.162
[ 30 / 40 ] loss: 0.036
[ 31 / 40 ] loss: 0.026
[ 32 / 40 ] loss: 0.016
[ 33 / 40 ] loss: 0.021
[ 34 / 40 ] loss: 0.183
[ 35 / 40 ] loss: 0.260
[ 36 / 40 ] loss: 0.037
[ 37 / 40 ] loss: 0.058
[ 38 / 40 ] loss: 0.174
[ 39 / 40 ] loss: 0.267
[ 40 / 40 ] loss: 0.067
0.11934739369899035
Accuracy: 0.754237 -- Precision: 0.808696 -- Recall: 0.720930 -- F1: 0.762295 -- AUC: 0.803231
========= epoch: 19 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.287
[ 2 / 40 ] loss: 0.205
[ 3 / 40 ] loss: 0.059
[ 4 / 40 ] loss: 0.202
[ 5 / 40 ] loss: 0.177
[ 6 / 40 ] loss: 0.183
[ 7 / 40 ] loss: 0.395
[ 8 / 40 ] loss: 0.025
[ 9 / 40 ] loss: 0.152
[ 10 / 40 ] loss: 0.099
[ 11 / 40 ] loss: 0.395
[ 12 / 40 ] loss: 0.146
[ 13 / 40 ] loss: 0.055
[ 14 / 40 ] loss: 0.223
[ 15 / 40 ] loss: 0.031
[ 16 / 40 ] loss: 0.042
[ 17 / 40 ] loss: 0.197
[ 18 / 40 ] loss: 0.169
[ 19 / 40 ] loss: 0.040
[ 20 / 40 ] loss: 0.043
[ 21 / 40 ] loss: 0.028
[ 22 / 40 ] loss: 0.052
[ 23 / 40 ] loss: 0.280
[ 24 / 40 ] loss: 0.202
[ 25 / 40 ] loss: 0.039
[ 26 / 40 ] loss: 0.133
[ 27 / 40 ] loss: 0.377
[ 28 / 40 ] loss: 0.168
[ 29 / 40 ] loss: 0.086
[ 30 / 40 ] loss: 0.219
[ 31 / 40 ] loss: 0.045
[ 32 / 40 ] loss: 0.089
[ 33 / 40 ] loss: 0.257
[ 34 / 40 ] loss: 0.050
[ 35 / 40 ] loss: 0.127
[ 36 / 40 ] loss: 0.070
[ 37 / 40 ] loss: 0.055
[ 38 / 40 ] loss: 0.195
[ 39 / 40 ] loss: 0.021
[ 40 / 40 ] loss: 0.075
0.14226846122182907
Accuracy: 0.792373 -- Precision: 0.798507 -- Recall: 0.829457 -- F1: 0.813688 -- AUC: 0.807433
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
[53, 76, 116, 166, 147, 228, 162, 118, 151, 113, 120, 78, 15, 173, 72, 102, 24, 119, 214, 54, 172, 8, 153, 70, 127, 10, 31, 158, 85, 68, 143, 229, 170, 137, 161, 106, 164, 193, 167, 62, 92, 69, 96, 6, 33, 21, 136, 135, 105, 61, 134, 71, 146, 221, 80, 91, 74, 219, 75, 25, 225, 125, 81, 163, 199, 109, 34, 114, 123, 97, 203, 129, 121, 101, 12, 35, 191, 2, 148, 55, 236, 201, 156, 22, 19, 32, 115, 36, 217, 126, 132, 41, 157, 174, 178, 133, 95, 98, 79, 168, 194, 57, 139, 67, 171, 222, 226, 52, 28, 100, 122, 183, 49, 131, 124, 7, 108, 218, 182, 200, 38, 216, 51, 231, 155, 17, 16, 46, 84, 60, 196, 187, 205, 220, 149, 90, 77, 234, 64, 177, 223, 227, 189, 103, 50, 179, 160, 104, 138, 190, 154, 63, 11, 110, 128, 211, 180, 198, 44, 86, 192, 209, 188, 195, 66, 232, 88, 47, 40, 42, 93, 175, 29, 58, 26, 145, 13, 111, 65, 89, 43, 23, 202, 230, 224, 152, 5, 107, 14, 186, 206, 56, 144, 83, 37, 159, 142, 212, 204, 233, 141, 45, 150, 185, 18, 140, 73, 4, 82, 39, 30, 208, 27, 197, 213, 3, 112, 48, 59, 207, 176, 9, 87, 117, 20, 184, 94, 210, 181, 99, 1, 165, 215, 130, 169, 235]
[0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]
[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
保存模型参数
========= epoch: 20 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.105
[ 2 / 40 ] loss: 0.032
[ 3 / 40 ] loss: 0.034
[ 4 / 40 ] loss: 0.022
[ 5 / 40 ] loss: 0.020
[ 6 / 40 ] loss: 0.018
[ 7 / 40 ] loss: 0.209
[ 8 / 40 ] loss: 0.200
[ 9 / 40 ] loss: 0.022
[ 10 / 40 ] loss: 0.016
[ 11 / 40 ] loss: 0.018
[ 12 / 40 ] loss: 0.026
[ 13 / 40 ] loss: 0.030
[ 14 / 40 ] loss: 0.016
[ 15 / 40 ] loss: 0.264
[ 16 / 40 ] loss: 0.023
[ 17 / 40 ] loss: 0.154
[ 18 / 40 ] loss: 0.141
[ 19 / 40 ] loss: 0.030
[ 20 / 40 ] loss: 0.354
[ 21 / 40 ] loss: 0.329
[ 22 / 40 ] loss: 0.218
[ 23 / 40 ] loss: 0.449
[ 24 / 40 ] loss: 0.343
[ 25 / 40 ] loss: 0.097
[ 26 / 40 ] loss: 0.128
[ 27 / 40 ] loss: 0.309
[ 28 / 40 ] loss: 0.164
[ 29 / 40 ] loss: 0.061
[ 30 / 40 ] loss: 0.043
[ 31 / 40 ] loss: 0.147
[ 32 / 40 ] loss: 0.503
[ 33 / 40 ] loss: 0.074
[ 34 / 40 ] loss: 0.079
[ 35 / 40 ] loss: 0.048
[ 36 / 40 ] loss: 0.124
[ 37 / 40 ] loss: 0.291
[ 38 / 40 ] loss: 0.032
[ 39 / 40 ] loss: 0.078
[ 40 / 40 ] loss: 0.357
0.140281085902825
Accuracy: 0.788136 -- Precision: 0.788321 -- Recall: 0.837209 -- F1: 0.812030 -- AUC: 0.826052
========= epoch: 21 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.038
[ 2 / 40 ] loss: 0.026
[ 3 / 40 ] loss: 0.159
[ 4 / 40 ] loss: 0.045
[ 5 / 40 ] loss: 0.038
[ 6 / 40 ] loss: 0.344
[ 7 / 40 ] loss: 0.221
[ 8 / 40 ] loss: 0.043
[ 9 / 40 ] loss: 0.142
[ 10 / 40 ] loss: 0.074
[ 11 / 40 ] loss: 0.169
[ 12 / 40 ] loss: 0.055
[ 13 / 40 ] loss: 0.261
[ 14 / 40 ] loss: 0.029
[ 15 / 40 ] loss: 0.054
[ 16 / 40 ] loss: 0.095
[ 17 / 40 ] loss: 0.027
[ 18 / 40 ] loss: 0.297
[ 19 / 40 ] loss: 0.170
[ 20 / 40 ] loss: 0.092
[ 21 / 40 ] loss: 0.154
[ 22 / 40 ] loss: 0.171
[ 23 / 40 ] loss: 0.026
[ 24 / 40 ] loss: 0.092
[ 25 / 40 ] loss: 0.200
[ 26 / 40 ] loss: 0.169
[ 27 / 40 ] loss: 0.033
[ 28 / 40 ] loss: 0.169
[ 29 / 40 ] loss: 0.229
[ 30 / 40 ] loss: 0.148
[ 31 / 40 ] loss: 0.025
[ 32 / 40 ] loss: 0.260
[ 33 / 40 ] loss: 0.209
[ 34 / 40 ] loss: 0.025
[ 35 / 40 ] loss: 0.071
[ 36 / 40 ] loss: 0.149
[ 37 / 40 ] loss: 0.038
[ 38 / 40 ] loss: 0.070
[ 39 / 40 ] loss: 0.109
[ 40 / 40 ] loss: 0.090
0.12049839519895614
Accuracy: 0.758475 -- Precision: 0.764706 -- Recall: 0.806202 -- F1: 0.784906 -- AUC: 0.830182
========= epoch: 22 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.226
[ 2 / 40 ] loss: 0.153
[ 3 / 40 ] loss: 0.039
[ 4 / 40 ] loss: 0.078
[ 5 / 40 ] loss: 0.029
[ 6 / 40 ] loss: 0.039
[ 7 / 40 ] loss: 0.018
[ 8 / 40 ] loss: 0.203
[ 9 / 40 ] loss: 0.049
[ 10 / 40 ] loss: 0.020
[ 11 / 40 ] loss: 0.172
[ 12 / 40 ] loss: 0.025
[ 13 / 40 ] loss: 0.071
[ 14 / 40 ] loss: 0.411
[ 15 / 40 ] loss: 0.038
[ 16 / 40 ] loss: 0.017
[ 17 / 40 ] loss: 0.270
[ 18 / 40 ] loss: 0.026
[ 19 / 40 ] loss: 0.157
[ 20 / 40 ] loss: 0.135
[ 21 / 40 ] loss: 0.132
[ 22 / 40 ] loss: 0.184
[ 23 / 40 ] loss: 0.073
[ 24 / 40 ] loss: 0.190
[ 25 / 40 ] loss: 0.060
[ 26 / 40 ] loss: 0.066
[ 27 / 40 ] loss: 0.015
[ 28 / 40 ] loss: 0.312
[ 29 / 40 ] loss: 0.047
[ 30 / 40 ] loss: 0.046
[ 31 / 40 ] loss: 0.068
[ 32 / 40 ] loss: 0.037
[ 33 / 40 ] loss: 0.345
[ 34 / 40 ] loss: 0.197
[ 35 / 40 ] loss: 0.180
[ 36 / 40 ] loss: 0.099
[ 37 / 40 ] loss: 0.122
[ 38 / 40 ] loss: 0.069
[ 39 / 40 ] loss: 0.041
[ 40 / 40 ] loss: 0.065
0.11309089194983243
Accuracy: 0.750000 -- Precision: 0.777778 -- Recall: 0.759690 -- F1: 0.768627 -- AUC: 0.815402
========= epoch: 23 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.053
[ 2 / 40 ] loss: 0.065
[ 3 / 40 ] loss: 0.199
[ 4 / 40 ] loss: 0.059
[ 5 / 40 ] loss: 0.037
[ 6 / 40 ] loss: 0.043
[ 7 / 40 ] loss: 0.160
[ 8 / 40 ] loss: 0.031
[ 9 / 40 ] loss: 0.224
[ 10 / 40 ] loss: 0.216
[ 11 / 40 ] loss: 0.129
[ 12 / 40 ] loss: 0.016
[ 13 / 40 ] loss: 0.031
[ 14 / 40 ] loss: 0.057
[ 15 / 40 ] loss: 0.030
[ 16 / 40 ] loss: 0.169
[ 17 / 40 ] loss: 0.089
[ 18 / 40 ] loss: 0.204
[ 19 / 40 ] loss: 0.183
[ 20 / 40 ] loss: 0.022
[ 21 / 40 ] loss: 0.064
[ 22 / 40 ] loss: 0.198
[ 23 / 40 ] loss: 0.031
[ 24 / 40 ] loss: 0.093
[ 25 / 40 ] loss: 0.030
[ 26 / 40 ] loss: 0.325
[ 27 / 40 ] loss: 0.036
[ 28 / 40 ] loss: 0.033
[ 29 / 40 ] loss: 0.016
[ 30 / 40 ] loss: 0.033
[ 31 / 40 ] loss: 0.030
[ 32 / 40 ] loss: 0.016
[ 33 / 40 ] loss: 0.039
[ 34 / 40 ] loss: 0.104
[ 35 / 40 ] loss: 0.014
[ 36 / 40 ] loss: 0.160
[ 37 / 40 ] loss: 0.020
[ 38 / 40 ] loss: 0.034
[ 39 / 40 ] loss: 0.110
[ 40 / 40 ] loss: 0.050
0.08633225883822888
Accuracy: 0.783898 -- Precision: 0.782609 -- Recall: 0.837209 -- F1: 0.808989 -- AUC: 0.811635
========= epoch: 24 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.035
[ 2 / 40 ] loss: 0.027
[ 3 / 40 ] loss: 0.017
[ 4 / 40 ] loss: 0.019
[ 5 / 40 ] loss: 0.013
[ 6 / 40 ] loss: 0.070
[ 7 / 40 ] loss: 0.113
[ 8 / 40 ] loss: 0.014
[ 9 / 40 ] loss: 0.009
[ 10 / 40 ] loss: 0.062
[ 11 / 40 ] loss: 0.430
[ 12 / 40 ] loss: 0.018
[ 13 / 40 ] loss: 0.057
[ 14 / 40 ] loss: 0.114
[ 15 / 40 ] loss: 0.113
[ 16 / 40 ] loss: 0.089
[ 17 / 40 ] loss: 0.126
[ 18 / 40 ] loss: 0.052
[ 19 / 40 ] loss: 0.074
[ 20 / 40 ] loss: 0.018
[ 21 / 40 ] loss: 0.071
[ 22 / 40 ] loss: 0.106
[ 23 / 40 ] loss: 0.029
[ 24 / 40 ] loss: 0.037
[ 25 / 40 ] loss: 0.034
[ 26 / 40 ] loss: 0.033
[ 27 / 40 ] loss: 0.223
[ 28 / 40 ] loss: 0.020
[ 29 / 40 ] loss: 0.222
[ 30 / 40 ] loss: 0.014
[ 31 / 40 ] loss: 0.239
[ 32 / 40 ] loss: 0.019
[ 33 / 40 ] loss: 0.075
[ 34 / 40 ] loss: 0.200
[ 35 / 40 ] loss: 0.108
[ 36 / 40 ] loss: 0.101
[ 37 / 40 ] loss: 0.015
[ 38 / 40 ] loss: 0.063
[ 39 / 40 ] loss: 0.016
[ 40 / 40 ] loss: 0.019
0.07775121231097729
Accuracy: 0.728814 -- Precision: 0.715232 -- Recall: 0.837209 -- F1: 0.771429 -- AUC: 0.793596
========= epoch: 25 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.009
[ 2 / 40 ] loss: 0.184
[ 3 / 40 ] loss: 0.027
[ 4 / 40 ] loss: 0.021
[ 5 / 40 ] loss: 0.018
[ 6 / 40 ] loss: 0.013
[ 7 / 40 ] loss: 0.087
[ 8 / 40 ] loss: 0.068
[ 9 / 40 ] loss: 0.140
[ 10 / 40 ] loss: 0.286
[ 11 / 40 ] loss: 0.011
[ 12 / 40 ] loss: 0.255
[ 13 / 40 ] loss: 0.023
[ 14 / 40 ] loss: 0.012
[ 15 / 40 ] loss: 0.140
[ 16 / 40 ] loss: 0.023
[ 17 / 40 ] loss: 0.291
[ 18 / 40 ] loss: 0.067
[ 19 / 40 ] loss: 0.048
[ 20 / 40 ] loss: 0.172
[ 21 / 40 ] loss: 0.123
[ 22 / 40 ] loss: 0.070
[ 23 / 40 ] loss: 0.076
[ 24 / 40 ] loss: 0.157
[ 25 / 40 ] loss: 0.165
[ 26 / 40 ] loss: 0.061
[ 27 / 40 ] loss: 0.034
[ 28 / 40 ] loss: 0.105
[ 29 / 40 ] loss: 0.023
[ 30 / 40 ] loss: 0.111
[ 31 / 40 ] loss: 0.030
[ 32 / 40 ] loss: 0.069
[ 33 / 40 ] loss: 0.020
[ 34 / 40 ] loss: 0.031
[ 35 / 40 ] loss: 0.023
[ 36 / 40 ] loss: 0.050
[ 37 / 40 ] loss: 0.034
[ 38 / 40 ] loss: 0.130
[ 39 / 40 ] loss: 0.036
[ 40 / 40 ] loss: 0.022
0.08167251863051206
Accuracy: 0.733051 -- Precision: 0.746269 -- Recall: 0.775194 -- F1: 0.760456 -- AUC: 0.787293
========= epoch: 26 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.016
[ 2 / 40 ] loss: 0.019
[ 3 / 40 ] loss: 0.018
[ 4 / 40 ] loss: 0.019
[ 5 / 40 ] loss: 0.147
[ 6 / 40 ] loss: 0.124
[ 7 / 40 ] loss: 0.134
[ 8 / 40 ] loss: 0.099
[ 9 / 40 ] loss: 0.010
[ 10 / 40 ] loss: 0.032
[ 11 / 40 ] loss: 0.014
[ 12 / 40 ] loss: 0.084
[ 13 / 40 ] loss: 0.024
[ 14 / 40 ] loss: 0.015
[ 15 / 40 ] loss: 0.012
[ 16 / 40 ] loss: 0.224
[ 17 / 40 ] loss: 0.118
[ 18 / 40 ] loss: 0.033
[ 19 / 40 ] loss: 0.011
[ 20 / 40 ] loss: 0.031
[ 21 / 40 ] loss: 0.018
[ 22 / 40 ] loss: 0.064
[ 23 / 40 ] loss: 0.016
[ 24 / 40 ] loss: 0.037
[ 25 / 40 ] loss: 0.121
[ 26 / 40 ] loss: 0.213
[ 27 / 40 ] loss: 0.028
[ 28 / 40 ] loss: 0.032
[ 29 / 40 ] loss: 0.018
[ 30 / 40 ] loss: 0.010
[ 31 / 40 ] loss: 0.039
[ 32 / 40 ] loss: 0.013
[ 33 / 40 ] loss: 0.024
[ 34 / 40 ] loss: 0.189
[ 35 / 40 ] loss: 0.083
[ 36 / 40 ] loss: 0.013
[ 37 / 40 ] loss: 0.133
[ 38 / 40 ] loss: 0.023
[ 39 / 40 ] loss: 0.010
[ 40 / 40 ] loss: 0.006
0.05679850217420608
Accuracy: 0.766949 -- Precision: 0.764286 -- Recall: 0.829457 -- F1: 0.795539 -- AUC: 0.789466
========= epoch: 27 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.018
[ 2 / 40 ] loss: 0.012
[ 3 / 40 ] loss: 0.108
[ 4 / 40 ] loss: 0.015
[ 5 / 40 ] loss: 0.010
[ 6 / 40 ] loss: 0.006
[ 7 / 40 ] loss: 0.019
[ 8 / 40 ] loss: 0.012
[ 9 / 40 ] loss: 0.023
[ 10 / 40 ] loss: 0.039
[ 11 / 40 ] loss: 0.007
[ 12 / 40 ] loss: 0.128
[ 13 / 40 ] loss: 0.012
[ 14 / 40 ] loss: 0.005
[ 15 / 40 ] loss: 0.014
[ 16 / 40 ] loss: 0.006
[ 17 / 40 ] loss: 0.005
[ 18 / 40 ] loss: 0.011
[ 19 / 40 ] loss: 0.006
[ 20 / 40 ] loss: 0.009
[ 21 / 40 ] loss: 0.008
[ 22 / 40 ] loss: 0.006
[ 23 / 40 ] loss: 0.015
[ 24 / 40 ] loss: 0.045
[ 25 / 40 ] loss: 0.009
[ 26 / 40 ] loss: 0.009
[ 27 / 40 ] loss: 0.024
[ 28 / 40 ] loss: 0.043
[ 29 / 40 ] loss: 0.015
[ 30 / 40 ] loss: 0.005
[ 31 / 40 ] loss: 0.006
[ 32 / 40 ] loss: 0.011
[ 33 / 40 ] loss: 0.013
[ 34 / 40 ] loss: 0.310
[ 35 / 40 ] loss: 0.153
[ 36 / 40 ] loss: 0.250
[ 37 / 40 ] loss: 0.097
[ 38 / 40 ] loss: 0.012
[ 39 / 40 ] loss: 0.195
[ 40 / 40 ] loss: 0.005
0.04235662169521674
Accuracy: 0.762712 -- Precision: 0.744966 -- Recall: 0.860465 -- F1: 0.798561 -- AUC: 0.809824
========= epoch: 28 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.006
[ 2 / 40 ] loss: 0.194
[ 3 / 40 ] loss: 0.008
[ 4 / 40 ] loss: 0.018
[ 5 / 40 ] loss: 0.006
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.014
[ 8 / 40 ] loss: 0.418
[ 9 / 40 ] loss: 0.020
[ 10 / 40 ] loss: 0.007
[ 11 / 40 ] loss: 0.114
[ 12 / 40 ] loss: 0.077
[ 13 / 40 ] loss: 0.102
[ 14 / 40 ] loss: 0.021
[ 15 / 40 ] loss: 0.044
[ 16 / 40 ] loss: 0.030
[ 17 / 40 ] loss: 0.018
[ 18 / 40 ] loss: 0.077
[ 19 / 40 ] loss: 0.052
[ 20 / 40 ] loss: 0.011
[ 21 / 40 ] loss: 0.033
[ 22 / 40 ] loss: 0.087
[ 23 / 40 ] loss: 0.010
[ 24 / 40 ] loss: 0.135
[ 25 / 40 ] loss: 0.044
[ 26 / 40 ] loss: 0.009
[ 27 / 40 ] loss: 0.010
[ 28 / 40 ] loss: 0.080
[ 29 / 40 ] loss: 0.105
[ 30 / 40 ] loss: 0.015
[ 31 / 40 ] loss: 0.015
[ 32 / 40 ] loss: 0.125
[ 33 / 40 ] loss: 0.133
[ 34 / 40 ] loss: 0.015
[ 35 / 40 ] loss: 0.135
[ 36 / 40 ] loss: 0.063
[ 37 / 40 ] loss: 0.107
[ 38 / 40 ] loss: 0.007
[ 39 / 40 ] loss: 0.007
[ 40 / 40 ] loss: 0.005
0.05952096233377233
Accuracy: 0.745763 -- Precision: 0.789916 -- Recall: 0.728682 -- F1: 0.758065 -- AUC: 0.789611
========= epoch: 29 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.017
[ 2 / 40 ] loss: 0.021
[ 3 / 40 ] loss: 0.007
[ 4 / 40 ] loss: 0.257
[ 5 / 40 ] loss: 0.006
[ 6 / 40 ] loss: 0.152
[ 7 / 40 ] loss: 0.120
[ 8 / 40 ] loss: 0.026
[ 9 / 40 ] loss: 0.091
[ 10 / 40 ] loss: 0.022
[ 11 / 40 ] loss: 0.007
[ 12 / 40 ] loss: 0.005
[ 13 / 40 ] loss: 0.098
[ 14 / 40 ] loss: 0.062
[ 15 / 40 ] loss: 0.274
[ 16 / 40 ] loss: 0.016
[ 17 / 40 ] loss: 0.041
[ 18 / 40 ] loss: 0.004
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.016
[ 21 / 40 ] loss: 0.006
[ 22 / 40 ] loss: 0.020
[ 23 / 40 ] loss: 0.007
[ 24 / 40 ] loss: 0.131
[ 25 / 40 ] loss: 0.284
[ 26 / 40 ] loss: 0.023
[ 27 / 40 ] loss: 0.035
[ 28 / 40 ] loss: 0.044
[ 29 / 40 ] loss: 0.014
[ 30 / 40 ] loss: 0.017
[ 31 / 40 ] loss: 0.010
[ 32 / 40 ] loss: 0.083
[ 33 / 40 ] loss: 0.005
[ 34 / 40 ] loss: 0.020
[ 35 / 40 ] loss: 0.030
[ 36 / 40 ] loss: 0.061
[ 37 / 40 ] loss: 0.111
[ 38 / 40 ] loss: 0.006
[ 39 / 40 ] loss: 0.014
[ 40 / 40 ] loss: 0.019
0.05461792142596096
Accuracy: 0.728814 -- Precision: 0.712418 -- Recall: 0.844961 -- F1: 0.773050 -- AUC: 0.775846
========= epoch: 30 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.176
[ 2 / 40 ] loss: 0.098
[ 3 / 40 ] loss: 0.038
[ 4 / 40 ] loss: 0.004
[ 5 / 40 ] loss: 0.006
[ 6 / 40 ] loss: 0.096
[ 7 / 40 ] loss: 0.005
[ 8 / 40 ] loss: 0.013
[ 9 / 40 ] loss: 0.124
[ 10 / 40 ] loss: 0.173
[ 11 / 40 ] loss: 0.209
[ 12 / 40 ] loss: 0.118
[ 13 / 40 ] loss: 0.117
[ 14 / 40 ] loss: 0.007
[ 15 / 40 ] loss: 0.215
[ 16 / 40 ] loss: 0.057
[ 17 / 40 ] loss: 0.080
[ 18 / 40 ] loss: 0.018
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.156
[ 21 / 40 ] loss: 0.036
[ 22 / 40 ] loss: 0.042
[ 23 / 40 ] loss: 0.475
[ 24 / 40 ] loss: 0.015
[ 25 / 40 ] loss: 0.009
[ 26 / 40 ] loss: 0.011
[ 27 / 40 ] loss: 0.066
[ 28 / 40 ] loss: 0.077
[ 29 / 40 ] loss: 0.042
[ 30 / 40 ] loss: 0.387
[ 31 / 40 ] loss: 0.238
[ 32 / 40 ] loss: 0.518
[ 33 / 40 ] loss: 0.811
[ 34 / 40 ] loss: 0.529
[ 35 / 40 ] loss: 0.091
[ 36 / 40 ] loss: 0.127
[ 37 / 40 ] loss: 0.068
[ 38 / 40 ] loss: 0.075
[ 39 / 40 ] loss: 0.201
[ 40 / 40 ] loss: 0.027
0.13900489596417173
Accuracy: 0.783898 -- Precision: 0.760000 -- Recall: 0.883721 -- F1: 0.817204 -- AUC: 0.823263
========= epoch: 31 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.183
[ 2 / 40 ] loss: 0.091
[ 3 / 40 ] loss: 0.034
[ 4 / 40 ] loss: 0.095
[ 5 / 40 ] loss: 0.057
[ 6 / 40 ] loss: 0.025
[ 7 / 40 ] loss: 0.067
[ 8 / 40 ] loss: 0.373
[ 9 / 40 ] loss: 0.145
[ 10 / 40 ] loss: 0.021
[ 11 / 40 ] loss: 0.036
[ 12 / 40 ] loss: 0.027
[ 13 / 40 ] loss: 0.053
[ 14 / 40 ] loss: 0.042
[ 15 / 40 ] loss: 0.035
[ 16 / 40 ] loss: 0.127
[ 17 / 40 ] loss: 0.012
[ 18 / 40 ] loss: 0.095
[ 19 / 40 ] loss: 0.128
[ 20 / 40 ] loss: 0.131
[ 21 / 40 ] loss: 0.043
[ 22 / 40 ] loss: 0.099
[ 23 / 40 ] loss: 0.014
[ 24 / 40 ] loss: 0.087
[ 25 / 40 ] loss: 0.118
[ 26 / 40 ] loss: 0.029
[ 27 / 40 ] loss: 0.251
[ 28 / 40 ] loss: 0.081
[ 29 / 40 ] loss: 0.460
[ 30 / 40 ] loss: 0.086
[ 31 / 40 ] loss: 0.166
[ 32 / 40 ] loss: 0.150
[ 33 / 40 ] loss: 0.148
[ 34 / 40 ] loss: 0.136
[ 35 / 40 ] loss: 0.046
[ 36 / 40 ] loss: 0.038
[ 37 / 40 ] loss: 0.096
[ 38 / 40 ] loss: 0.020
[ 39 / 40 ] loss: 0.075
[ 40 / 40 ] loss: 0.032
0.09883687561377882
Accuracy: 0.741525 -- Precision: 0.750000 -- Recall: 0.790698 -- F1: 0.769811 -- AUC: 0.805912
========= epoch: 32 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.062
[ 2 / 40 ] loss: 0.046
[ 3 / 40 ] loss: 0.056
[ 4 / 40 ] loss: 0.036
[ 5 / 40 ] loss: 0.086
[ 6 / 40 ] loss: 0.080
[ 7 / 40 ] loss: 0.032
[ 8 / 40 ] loss: 0.087
[ 9 / 40 ] loss: 0.068
[ 10 / 40 ] loss: 0.117
[ 11 / 40 ] loss: 0.022
[ 12 / 40 ] loss: 0.023
[ 13 / 40 ] loss: 0.251
[ 14 / 40 ] loss: 0.029
[ 15 / 40 ] loss: 0.032
[ 16 / 40 ] loss: 0.008
[ 17 / 40 ] loss: 0.024
[ 18 / 40 ] loss: 0.006
[ 19 / 40 ] loss: 0.008
[ 20 / 40 ] loss: 0.016
[ 21 / 40 ] loss: 0.019
[ 22 / 40 ] loss: 0.029
[ 23 / 40 ] loss: 0.103
[ 24 / 40 ] loss: 0.191
[ 25 / 40 ] loss: 0.008
[ 26 / 40 ] loss: 0.008
[ 27 / 40 ] loss: 0.019
[ 28 / 40 ] loss: 0.149
[ 29 / 40 ] loss: 0.011
[ 30 / 40 ] loss: 0.122
[ 31 / 40 ] loss: 0.027
[ 32 / 40 ] loss: 0.209
[ 33 / 40 ] loss: 0.036
[ 34 / 40 ] loss: 0.226
[ 35 / 40 ] loss: 0.009
[ 36 / 40 ] loss: 0.041
[ 37 / 40 ] loss: 0.231
[ 38 / 40 ] loss: 0.070
[ 39 / 40 ] loss: 0.036
[ 40 / 40 ] loss: 0.010
0.06606107710395008
Accuracy: 0.745763 -- Precision: 0.755556 -- Recall: 0.790698 -- F1: 0.772727 -- AUC: 0.796204
========= epoch: 33 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.018
[ 2 / 40 ] loss: 0.120
[ 3 / 40 ] loss: 0.014
[ 4 / 40 ] loss: 0.012
[ 5 / 40 ] loss: 0.008
[ 6 / 40 ] loss: 0.008
[ 7 / 40 ] loss: 0.008
[ 8 / 40 ] loss: 0.021
[ 9 / 40 ] loss: 0.307
[ 10 / 40 ] loss: 0.015
[ 11 / 40 ] loss: 0.019
[ 12 / 40 ] loss: 0.017
[ 13 / 40 ] loss: 0.007
[ 14 / 40 ] loss: 0.015
[ 15 / 40 ] loss: 0.028
[ 16 / 40 ] loss: 0.030
[ 17 / 40 ] loss: 0.011
[ 18 / 40 ] loss: 0.012
[ 19 / 40 ] loss: 0.012
[ 20 / 40 ] loss: 0.130
[ 21 / 40 ] loss: 0.012
[ 22 / 40 ] loss: 0.026
[ 23 / 40 ] loss: 0.005
[ 24 / 40 ] loss: 0.037
[ 25 / 40 ] loss: 0.105
[ 26 / 40 ] loss: 0.166
[ 27 / 40 ] loss: 0.006
[ 28 / 40 ] loss: 0.018
[ 29 / 40 ] loss: 0.011
[ 30 / 40 ] loss: 0.022
[ 31 / 40 ] loss: 0.021
[ 32 / 40 ] loss: 0.184
[ 33 / 40 ] loss: 0.007
[ 34 / 40 ] loss: 0.016
[ 35 / 40 ] loss: 0.136
[ 36 / 40 ] loss: 0.071
[ 37 / 40 ] loss: 0.160
[ 38 / 40 ] loss: 0.469
[ 39 / 40 ] loss: 0.022
[ 40 / 40 ] loss: 0.018
0.058220219379290936
Accuracy: 0.754237 -- Precision: 0.798319 -- Recall: 0.736434 -- F1: 0.766129 -- AUC: 0.797435
========= epoch: 34 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.172
[ 2 / 40 ] loss: 0.288
[ 3 / 40 ] loss: 0.060
[ 4 / 40 ] loss: 0.207
[ 5 / 40 ] loss: 0.230
[ 6 / 40 ] loss: 0.182
[ 7 / 40 ] loss: 0.043
[ 8 / 40 ] loss: 0.134
[ 9 / 40 ] loss: 0.157
[ 10 / 40 ] loss: 0.073
[ 11 / 40 ] loss: 0.227
[ 12 / 40 ] loss: 0.223
[ 13 / 40 ] loss: 0.200
[ 14 / 40 ] loss: 0.032
[ 15 / 40 ] loss: 0.031
[ 16 / 40 ] loss: 0.087
[ 17 / 40 ] loss: 0.019
[ 18 / 40 ] loss: 0.015
[ 19 / 40 ] loss: 0.076
[ 20 / 40 ] loss: 0.229
[ 21 / 40 ] loss: 0.015
[ 22 / 40 ] loss: 0.026
[ 23 / 40 ] loss: 0.016
[ 24 / 40 ] loss: 0.074
[ 25 / 40 ] loss: 0.028
[ 26 / 40 ] loss: 0.011
[ 27 / 40 ] loss: 0.321
[ 28 / 40 ] loss: 0.011
[ 29 / 40 ] loss: 0.110
[ 30 / 40 ] loss: 0.012
[ 31 / 40 ] loss: 0.006
[ 32 / 40 ] loss: 0.010
[ 33 / 40 ] loss: 0.009
[ 34 / 40 ] loss: 0.016
[ 35 / 40 ] loss: 0.043
[ 36 / 40 ] loss: 0.009
[ 37 / 40 ] loss: 0.095
[ 38 / 40 ] loss: 0.036
[ 39 / 40 ] loss: 0.060
[ 40 / 40 ] loss: 0.011
0.09009788929251954
Accuracy: 0.745763 -- Precision: 0.748201 -- Recall: 0.806202 -- F1: 0.776119 -- AUC: 0.784684
========= epoch: 35 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.149
[ 2 / 40 ] loss: 0.008
[ 3 / 40 ] loss: 0.017
[ 4 / 40 ] loss: 0.025
[ 5 / 40 ] loss: 0.061
[ 6 / 40 ] loss: 0.099
[ 7 / 40 ] loss: 0.044
[ 8 / 40 ] loss: 0.173
[ 9 / 40 ] loss: 0.068
[ 10 / 40 ] loss: 0.020
[ 11 / 40 ] loss: 0.060
[ 12 / 40 ] loss: 0.037
[ 13 / 40 ] loss: 0.073
[ 14 / 40 ] loss: 0.048
[ 15 / 40 ] loss: 0.021
[ 16 / 40 ] loss: 0.021
[ 17 / 40 ] loss: 0.053
[ 18 / 40 ] loss: 0.012
[ 19 / 40 ] loss: 0.087
[ 20 / 40 ] loss: 0.040
[ 21 / 40 ] loss: 0.013
[ 22 / 40 ] loss: 0.010
[ 23 / 40 ] loss: 0.084
[ 24 / 40 ] loss: 0.013
[ 25 / 40 ] loss: 0.039
[ 26 / 40 ] loss: 0.199
[ 27 / 40 ] loss: 0.008
[ 28 / 40 ] loss: 0.016
[ 29 / 40 ] loss: 0.122
[ 30 / 40 ] loss: 0.235
[ 31 / 40 ] loss: 0.021
[ 32 / 40 ] loss: 0.076
[ 33 / 40 ] loss: 0.020
[ 34 / 40 ] loss: 0.022
[ 35 / 40 ] loss: 0.005
[ 36 / 40 ] loss: 0.012
[ 37 / 40 ] loss: 0.037
[ 38 / 40 ] loss: 0.055
[ 39 / 40 ] loss: 0.012
[ 40 / 40 ] loss: 0.168
0.05699307887116447
Accuracy: 0.771186 -- Precision: 0.758621 -- Recall: 0.852713 -- F1: 0.802920 -- AUC: 0.805332
========= epoch: 36 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.065
[ 2 / 40 ] loss: 0.072
[ 3 / 40 ] loss: 0.007
[ 4 / 40 ] loss: 0.027
[ 5 / 40 ] loss: 0.051
[ 6 / 40 ] loss: 0.330
[ 7 / 40 ] loss: 0.006
[ 8 / 40 ] loss: 0.006
[ 9 / 40 ] loss: 0.007
[ 10 / 40 ] loss: 0.096
[ 11 / 40 ] loss: 0.019
[ 12 / 40 ] loss: 0.025
[ 13 / 40 ] loss: 0.043
[ 14 / 40 ] loss: 0.010
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.007
[ 17 / 40 ] loss: 0.134
[ 18 / 40 ] loss: 0.006
[ 19 / 40 ] loss: 0.005
[ 20 / 40 ] loss: 0.009
[ 21 / 40 ] loss: 0.005
[ 22 / 40 ] loss: 0.041
[ 23 / 40 ] loss: 0.007
[ 24 / 40 ] loss: 0.014
[ 25 / 40 ] loss: 0.060
[ 26 / 40 ] loss: 0.080
[ 27 / 40 ] loss: 0.209
[ 28 / 40 ] loss: 0.008
[ 29 / 40 ] loss: 0.034
[ 30 / 40 ] loss: 0.007
[ 31 / 40 ] loss: 0.026
[ 32 / 40 ] loss: 0.050
[ 33 / 40 ] loss: 0.116
[ 34 / 40 ] loss: 0.065
[ 35 / 40 ] loss: 0.006
[ 36 / 40 ] loss: 0.104
[ 37 / 40 ] loss: 0.043
[ 38 / 40 ] loss: 0.028
[ 39 / 40 ] loss: 0.126
[ 40 / 40 ] loss: 0.006
0.049200112430844455
Accuracy: 0.758475 -- Precision: 0.750000 -- Recall: 0.837209 -- F1: 0.791209 -- AUC: 0.800043
========= epoch: 37 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.031
[ 2 / 40 ] loss: 0.009
[ 3 / 40 ] loss: 0.054
[ 4 / 40 ] loss: 0.018
[ 5 / 40 ] loss: 0.007
[ 6 / 40 ] loss: 0.016
[ 7 / 40 ] loss: 0.019
[ 8 / 40 ] loss: 0.007
[ 9 / 40 ] loss: 0.328
[ 10 / 40 ] loss: 0.016
[ 11 / 40 ] loss: 0.014
[ 12 / 40 ] loss: 0.020
[ 13 / 40 ] loss: 0.029
[ 14 / 40 ] loss: 0.006
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.014
[ 17 / 40 ] loss: 0.056
[ 18 / 40 ] loss: 0.036
[ 19 / 40 ] loss: 0.177
[ 20 / 40 ] loss: 0.023
[ 21 / 40 ] loss: 0.127
[ 22 / 40 ] loss: 0.021
[ 23 / 40 ] loss: 0.005
[ 24 / 40 ] loss: 0.007
[ 25 / 40 ] loss: 0.004
[ 26 / 40 ] loss: 0.030
[ 27 / 40 ] loss: 0.004
[ 28 / 40 ] loss: 0.034
[ 29 / 40 ] loss: 0.056
[ 30 / 40 ] loss: 0.005
[ 31 / 40 ] loss: 0.021
[ 32 / 40 ] loss: 0.015
[ 33 / 40 ] loss: 0.004
[ 34 / 40 ] loss: 0.005
[ 35 / 40 ] loss: 0.005
[ 36 / 40 ] loss: 0.032
[ 37 / 40 ] loss: 0.022
[ 38 / 40 ] loss: 0.158
[ 39 / 40 ] loss: 0.032
[ 40 / 40 ] loss: 0.003
0.03702436838648282
Accuracy: 0.766949 -- Precision: 0.780303 -- Recall: 0.798450 -- F1: 0.789272 -- AUC: 0.783851
========= epoch: 38 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.003
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.004
[ 4 / 40 ] loss: 0.047
[ 5 / 40 ] loss: 0.004
[ 6 / 40 ] loss: 0.015
[ 7 / 40 ] loss: 0.011
[ 8 / 40 ] loss: 0.099
[ 9 / 40 ] loss: 0.023
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.059
[ 12 / 40 ] loss: 0.022
[ 13 / 40 ] loss: 0.004
[ 14 / 40 ] loss: 0.115
[ 15 / 40 ] loss: 0.004
[ 16 / 40 ] loss: 0.005
[ 17 / 40 ] loss: 0.004
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.274
[ 20 / 40 ] loss: 0.027
[ 21 / 40 ] loss: 0.005
[ 22 / 40 ] loss: 0.178
[ 23 / 40 ] loss: 0.005
[ 24 / 40 ] loss: 0.025
[ 25 / 40 ] loss: 0.006
[ 26 / 40 ] loss: 0.010
[ 27 / 40 ] loss: 0.004
[ 28 / 40 ] loss: 0.044
[ 29 / 40 ] loss: 0.005
[ 30 / 40 ] loss: 0.006
[ 31 / 40 ] loss: 0.029
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.005
[ 34 / 40 ] loss: 0.256
[ 35 / 40 ] loss: 0.012
[ 36 / 40 ] loss: 0.022
[ 37 / 40 ] loss: 0.040
[ 38 / 40 ] loss: 0.017
[ 39 / 40 ] loss: 0.006
[ 40 / 40 ] loss: 0.003
0.03529477315605618
Accuracy: 0.745763 -- Precision: 0.748201 -- Recall: 0.806202 -- F1: 0.776119 -- AUC: 0.790987
========= epoch: 39 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.021
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.004
[ 4 / 40 ] loss: 0.005
[ 5 / 40 ] loss: 0.008
[ 6 / 40 ] loss: 0.012
[ 7 / 40 ] loss: 0.010
[ 8 / 40 ] loss: 0.003
[ 9 / 40 ] loss: 0.003
[ 10 / 40 ] loss: 0.015
[ 11 / 40 ] loss: 0.003
[ 12 / 40 ] loss: 0.015
[ 13 / 40 ] loss: 0.003
[ 14 / 40 ] loss: 0.007
[ 15 / 40 ] loss: 0.338
[ 16 / 40 ] loss: 0.004
[ 17 / 40 ] loss: 0.003
[ 18 / 40 ] loss: 0.004
[ 19 / 40 ] loss: 0.003
[ 20 / 40 ] loss: 0.104
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.019
[ 23 / 40 ] loss: 0.013
[ 24 / 40 ] loss: 0.006
[ 25 / 40 ] loss: 0.041
[ 26 / 40 ] loss: 0.004
[ 27 / 40 ] loss: 0.011
[ 28 / 40 ] loss: 0.234
[ 29 / 40 ] loss: 0.291
[ 30 / 40 ] loss: 0.109
[ 31 / 40 ] loss: 0.006
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.022
[ 34 / 40 ] loss: 0.004
[ 35 / 40 ] loss: 0.014
[ 36 / 40 ] loss: 0.005
[ 37 / 40 ] loss: 0.004
[ 38 / 40 ] loss: 0.067
[ 39 / 40 ] loss: 0.082
[ 40 / 40 ] loss: 0.015
0.0380423512251582
Accuracy: 0.737288 -- Precision: 0.755725 -- Recall: 0.767442 -- F1: 0.761538 -- AUC: 0.793378
========= epoch: 40 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.077
[ 2 / 40 ] loss: 0.028
[ 3 / 40 ] loss: 0.008
[ 4 / 40 ] loss: 0.030
[ 5 / 40 ] loss: 0.013
[ 6 / 40 ] loss: 0.104
[ 7 / 40 ] loss: 0.039
[ 8 / 40 ] loss: 0.005
[ 9 / 40 ] loss: 0.004
[ 10 / 40 ] loss: 0.122
[ 11 / 40 ] loss: 0.012
[ 12 / 40 ] loss: 0.003
[ 13 / 40 ] loss: 0.136
[ 14 / 40 ] loss: 0.004
[ 15 / 40 ] loss: 0.003
[ 16 / 40 ] loss: 0.013
[ 17 / 40 ] loss: 0.014
[ 18 / 40 ] loss: 0.189
[ 19 / 40 ] loss: 0.004
[ 20 / 40 ] loss: 0.048
[ 21 / 40 ] loss: 0.004
[ 22 / 40 ] loss: 0.004
[ 23 / 40 ] loss: 0.003
[ 24 / 40 ] loss: 0.070
[ 25 / 40 ] loss: 0.004
[ 26 / 40 ] loss: 0.004
[ 27 / 40 ] loss: 0.016
[ 28 / 40 ] loss: 0.040
[ 29 / 40 ] loss: 0.003
[ 30 / 40 ] loss: 0.025
[ 31 / 40 ] loss: 0.022
[ 32 / 40 ] loss: 0.031
[ 33 / 40 ] loss: 0.004
[ 34 / 40 ] loss: 0.210
[ 35 / 40 ] loss: 0.003
[ 36 / 40 ] loss: 0.014
[ 37 / 40 ] loss: 0.014
[ 38 / 40 ] loss: 0.003
[ 39 / 40 ] loss: 0.007
[ 40 / 40 ] loss: 0.011
0.033816578797996044
Accuracy: 0.745763 -- Precision: 0.734694 -- Recall: 0.837209 -- F1: 0.782609 -- AUC: 0.815547
========= epoch: 41 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.006
[ 2 / 40 ] loss: 0.038
[ 3 / 40 ] loss: 0.004
[ 4 / 40 ] loss: 0.004
[ 5 / 40 ] loss: 0.003
[ 6 / 40 ] loss: 0.005
[ 7 / 40 ] loss: 0.004
[ 8 / 40 ] loss: 0.014
[ 9 / 40 ] loss: 0.003
[ 10 / 40 ] loss: 0.024
[ 11 / 40 ] loss: 0.105
[ 12 / 40 ] loss: 0.016
[ 13 / 40 ] loss: 0.015
[ 14 / 40 ] loss: 0.013
[ 15 / 40 ] loss: 0.003
[ 16 / 40 ] loss: 0.003
[ 17 / 40 ] loss: 0.020
[ 18 / 40 ] loss: 0.007
[ 19 / 40 ] loss: 0.008
[ 20 / 40 ] loss: 0.067
[ 21 / 40 ] loss: 0.400
[ 22 / 40 ] loss: 0.009
[ 23 / 40 ] loss: 0.030
[ 24 / 40 ] loss: 0.002
[ 25 / 40 ] loss: 0.008
[ 26 / 40 ] loss: 0.032
[ 27 / 40 ] loss: 0.256
[ 28 / 40 ] loss: 0.195
[ 29 / 40 ] loss: 0.004
[ 30 / 40 ] loss: 0.004
[ 31 / 40 ] loss: 0.004
[ 32 / 40 ] loss: 0.152
[ 33 / 40 ] loss: 0.006
[ 34 / 40 ] loss: 0.007
[ 35 / 40 ] loss: 0.004
[ 36 / 40 ] loss: 0.095
[ 37 / 40 ] loss: 0.030
[ 38 / 40 ] loss: 0.018
[ 39 / 40 ] loss: 0.058
[ 40 / 40 ] loss: 0.190
0.046701852220576254
Accuracy: 0.758475 -- Precision: 0.753521 -- Recall: 0.829457 -- F1: 0.789668 -- AUC: 0.805405
========= epoch: 42 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.015
[ 2 / 40 ] loss: 0.038
[ 3 / 40 ] loss: 0.007
[ 4 / 40 ] loss: 0.044
[ 5 / 40 ] loss: 0.038
[ 6 / 40 ] loss: 0.041
[ 7 / 40 ] loss: 0.046
[ 8 / 40 ] loss: 0.049
[ 9 / 40 ] loss: 0.153
[ 10 / 40 ] loss: 0.020
[ 11 / 40 ] loss: 0.024
[ 12 / 40 ] loss: 0.047
[ 13 / 40 ] loss: 0.004
[ 14 / 40 ] loss: 0.211
[ 15 / 40 ] loss: 0.009
[ 16 / 40 ] loss: 0.009
[ 17 / 40 ] loss: 0.011
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.015
[ 20 / 40 ] loss: 0.020
[ 21 / 40 ] loss: 0.003
[ 22 / 40 ] loss: 0.084
[ 23 / 40 ] loss: 0.021
[ 24 / 40 ] loss: 0.016
[ 25 / 40 ] loss: 0.005
[ 26 / 40 ] loss: 0.004
[ 27 / 40 ] loss: 0.070
[ 28 / 40 ] loss: 0.003
[ 29 / 40 ] loss: 0.268
[ 30 / 40 ] loss: 0.153
[ 31 / 40 ] loss: 0.003
[ 32 / 40 ] loss: 0.005
[ 33 / 40 ] loss: 0.101
[ 34 / 40 ] loss: 0.023
[ 35 / 40 ] loss: 0.064
[ 36 / 40 ] loss: 0.019
[ 37 / 40 ] loss: 0.020
[ 38 / 40 ] loss: 0.018
[ 39 / 40 ] loss: 0.136
[ 40 / 40 ] loss: 0.013
0.0458209341391921
Accuracy: 0.758475 -- Precision: 0.772727 -- Recall: 0.790698 -- F1: 0.781609 -- AUC: 0.793233
========= epoch: 43 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.035
[ 2 / 40 ] loss: 0.088
[ 3 / 40 ] loss: 0.017
[ 4 / 40 ] loss: 0.091
[ 5 / 40 ] loss: 0.021
[ 6 / 40 ] loss: 0.029
[ 7 / 40 ] loss: 0.055
[ 8 / 40 ] loss: 0.036
[ 9 / 40 ] loss: 0.053
[ 10 / 40 ] loss: 0.066
[ 11 / 40 ] loss: 0.008
[ 12 / 40 ] loss: 0.017
[ 13 / 40 ] loss: 0.033
[ 14 / 40 ] loss: 0.105
[ 15 / 40 ] loss: 0.066
[ 16 / 40 ] loss: 0.003
[ 17 / 40 ] loss: 0.016
[ 18 / 40 ] loss: 0.085
[ 19 / 40 ] loss: 0.041
[ 20 / 40 ] loss: 0.041
[ 21 / 40 ] loss: 0.012
[ 22 / 40 ] loss: 0.032
[ 23 / 40 ] loss: 0.054
[ 24 / 40 ] loss: 0.171
[ 25 / 40 ] loss: 0.015
[ 26 / 40 ] loss: 0.076
[ 27 / 40 ] loss: 0.007
[ 28 / 40 ] loss: 0.004
[ 29 / 40 ] loss: 0.075
[ 30 / 40 ] loss: 0.002
[ 31 / 40 ] loss: 0.097
[ 32 / 40 ] loss: 0.010
[ 33 / 40 ] loss: 0.063
[ 34 / 40 ] loss: 0.026
[ 35 / 40 ] loss: 0.011
[ 36 / 40 ] loss: 0.256
[ 37 / 40 ] loss: 0.004
[ 38 / 40 ] loss: 0.042
[ 39 / 40 ] loss: 0.249
[ 40 / 40 ] loss: 0.002
0.052861090161604805
Accuracy: 0.741525 -- Precision: 0.778689 -- Recall: 0.736434 -- F1: 0.756972 -- AUC: 0.791277
========= epoch: 44 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.056
[ 2 / 40 ] loss: 0.005
[ 3 / 40 ] loss: 0.041
[ 4 / 40 ] loss: 0.022
[ 5 / 40 ] loss: 0.020
[ 6 / 40 ] loss: 0.368
[ 7 / 40 ] loss: 0.036
[ 8 / 40 ] loss: 0.048
[ 9 / 40 ] loss: 0.004
[ 10 / 40 ] loss: 0.029
[ 11 / 40 ] loss: 0.027
[ 12 / 40 ] loss: 0.038
[ 13 / 40 ] loss: 0.044
[ 14 / 40 ] loss: 0.006
[ 15 / 40 ] loss: 0.053
[ 16 / 40 ] loss: 0.024
[ 17 / 40 ] loss: 0.011
[ 18 / 40 ] loss: 0.034
[ 19 / 40 ] loss: 0.066
[ 20 / 40 ] loss: 0.275
[ 21 / 40 ] loss: 0.178
[ 22 / 40 ] loss: 0.071
[ 23 / 40 ] loss: 0.027
[ 24 / 40 ] loss: 0.042
[ 25 / 40 ] loss: 0.062
[ 26 / 40 ] loss: 0.034
[ 27 / 40 ] loss: 0.012
[ 28 / 40 ] loss: 0.083
[ 29 / 40 ] loss: 0.074
[ 30 / 40 ] loss: 0.085
[ 31 / 40 ] loss: 0.155
[ 32 / 40 ] loss: 0.004
[ 33 / 40 ] loss: 0.022
[ 34 / 40 ] loss: 0.091
[ 35 / 40 ] loss: 0.196
[ 36 / 40 ] loss: 0.109
[ 37 / 40 ] loss: 0.029
[ 38 / 40 ] loss: 0.040
[ 39 / 40 ] loss: 0.005
[ 40 / 40 ] loss: 0.005
0.06323241034988314
Accuracy: 0.724576 -- Precision: 0.707792 -- Recall: 0.844961 -- F1: 0.770318 -- AUC: 0.809172
========= epoch: 45 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.022
[ 2 / 40 ] loss: 0.042
[ 3 / 40 ] loss: 0.039
[ 4 / 40 ] loss: 0.266
[ 5 / 40 ] loss: 0.007
[ 6 / 40 ] loss: 0.005
[ 7 / 40 ] loss: 0.037
[ 8 / 40 ] loss: 0.005
[ 9 / 40 ] loss: 0.029
[ 10 / 40 ] loss: 0.007
[ 11 / 40 ] loss: 0.006
[ 12 / 40 ] loss: 0.018
[ 13 / 40 ] loss: 0.079
[ 14 / 40 ] loss: 0.015
[ 15 / 40 ] loss: 0.057
[ 16 / 40 ] loss: 0.006
[ 17 / 40 ] loss: 0.011
[ 18 / 40 ] loss: 0.005
[ 19 / 40 ] loss: 0.108
[ 20 / 40 ] loss: 0.013
[ 21 / 40 ] loss: 0.029
[ 22 / 40 ] loss: 0.010
[ 23 / 40 ] loss: 0.121
[ 24 / 40 ] loss: 0.078
[ 25 / 40 ] loss: 0.029
[ 26 / 40 ] loss: 0.021
[ 27 / 40 ] loss: 0.029
[ 28 / 40 ] loss: 0.004
[ 29 / 40 ] loss: 0.034
[ 30 / 40 ] loss: 0.011
[ 31 / 40 ] loss: 0.029
[ 32 / 40 ] loss: 0.013
[ 33 / 40 ] loss: 0.034
[ 34 / 40 ] loss: 0.004
[ 35 / 40 ] loss: 0.016
[ 36 / 40 ] loss: 0.030
[ 37 / 40 ] loss: 0.201
[ 38 / 40 ] loss: 0.019
[ 39 / 40 ] loss: 0.004
[ 40 / 40 ] loss: 0.005
0.03746390613960102
Accuracy: 0.792373 -- Precision: 0.807692 -- Recall: 0.813953 -- F1: 0.810811 -- AUC: 0.789249
========= epoch: 46 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.012
[ 2 / 40 ] loss: 0.002
[ 3 / 40 ] loss: 0.003
[ 4 / 40 ] loss: 0.048
[ 5 / 40 ] loss: 0.033
[ 6 / 40 ] loss: 0.019
[ 7 / 40 ] loss: 0.053
[ 8 / 40 ] loss: 0.107
[ 9 / 40 ] loss: 0.008
[ 10 / 40 ] loss: 0.003
[ 11 / 40 ] loss: 0.079
[ 12 / 40 ] loss: 0.003
[ 13 / 40 ] loss: 0.020
[ 14 / 40 ] loss: 0.044
[ 15 / 40 ] loss: 0.003
[ 16 / 40 ] loss: 0.018
[ 17 / 40 ] loss: 0.062
[ 18 / 40 ] loss: 0.002
[ 19 / 40 ] loss: 0.003
[ 20 / 40 ] loss: 0.295
[ 21 / 40 ] loss: 0.174
[ 22 / 40 ] loss: 0.002
[ 23 / 40 ] loss: 0.020
[ 24 / 40 ] loss: 0.044
[ 25 / 40 ] loss: 0.022
[ 26 / 40 ] loss: 0.009
[ 27 / 40 ] loss: 0.036
[ 28 / 40 ] loss: 0.007
[ 29 / 40 ] loss: 0.042
[ 30 / 40 ] loss: 0.033
[ 31 / 40 ] loss: 0.002
[ 32 / 40 ] loss: 0.002
[ 33 / 40 ] loss: 0.020
[ 34 / 40 ] loss: 0.003
[ 35 / 40 ] loss: 0.008
[ 36 / 40 ] loss: 0.024
[ 37 / 40 ] loss: 0.027
[ 38 / 40 ] loss: 0.005
[ 39 / 40 ] loss: 0.209
[ 40 / 40 ] loss: 0.003
0.03773129271576181
Accuracy: 0.754237 -- Precision: 0.762963 -- Recall: 0.798450 -- F1: 0.780303 -- AUC: 0.789394
========= epoch: 47 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.002
[ 2 / 40 ] loss: 0.009
[ 3 / 40 ] loss: 0.120
[ 4 / 40 ] loss: 0.004
[ 5 / 40 ] loss: 0.076
[ 6 / 40 ] loss: 0.002
[ 7 / 40 ] loss: 0.011
[ 8 / 40 ] loss: 0.021
[ 9 / 40 ] loss: 0.055
[ 10 / 40 ] loss: 0.115
[ 11 / 40 ] loss: 0.011
[ 12 / 40 ] loss: 0.003
[ 13 / 40 ] loss: 0.005
[ 14 / 40 ] loss: 0.052
[ 15 / 40 ] loss: 0.013
[ 16 / 40 ] loss: 0.032
[ 17 / 40 ] loss: 0.269
[ 18 / 40 ] loss: 0.002
[ 19 / 40 ] loss: 0.003
[ 20 / 40 ] loss: 0.070
[ 21 / 40 ] loss: 0.031
[ 22 / 40 ] loss: 0.016
[ 23 / 40 ] loss: 0.003
[ 24 / 40 ] loss: 0.004
[ 25 / 40 ] loss: 0.021
[ 26 / 40 ] loss: 0.005
[ 27 / 40 ] loss: 0.004
[ 28 / 40 ] loss: 0.003
[ 29 / 40 ] loss: 0.004
[ 30 / 40 ] loss: 0.074
[ 31 / 40 ] loss: 0.043
[ 32 / 40 ] loss: 0.003
[ 33 / 40 ] loss: 0.012
[ 34 / 40 ] loss: 0.009
[ 35 / 40 ] loss: 0.023
[ 36 / 40 ] loss: 0.003
[ 37 / 40 ] loss: 0.264
[ 38 / 40 ] loss: 0.021
[ 39 / 40 ] loss: 0.020
[ 40 / 40 ] loss: 0.003
0.036032685992540794
Accuracy: 0.741525 -- Precision: 0.742857 -- Recall: 0.806202 -- F1: 0.773234 -- AUC: 0.793523
========= epoch: 48 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.004
[ 2 / 40 ] loss: 0.007
[ 3 / 40 ] loss: 0.003
[ 4 / 40 ] loss: 0.013
[ 5 / 40 ] loss: 0.073
[ 6 / 40 ] loss: 0.003
[ 7 / 40 ] loss: 0.003
[ 8 / 40 ] loss: 0.014
[ 9 / 40 ] loss: 0.004
[ 10 / 40 ] loss: 0.009
[ 11 / 40 ] loss: 0.008
[ 12 / 40 ] loss: 0.100
[ 13 / 40 ] loss: 0.003
[ 14 / 40 ] loss: 0.003
[ 15 / 40 ] loss: 0.023
[ 16 / 40 ] loss: 0.053
[ 17 / 40 ] loss: 0.002
[ 18 / 40 ] loss: 0.073
[ 19 / 40 ] loss: 0.003
[ 20 / 40 ] loss: 0.013
[ 21 / 40 ] loss: 0.002
[ 22 / 40 ] loss: 0.123
[ 23 / 40 ] loss: 0.070
[ 24 / 40 ] loss: 0.024
[ 25 / 40 ] loss: 0.003
[ 26 / 40 ] loss: 0.040
[ 27 / 40 ] loss: 0.005
[ 28 / 40 ] loss: 0.003
[ 29 / 40 ] loss: 0.015
[ 30 / 40 ] loss: 0.015
[ 31 / 40 ] loss: 0.033
[ 32 / 40 ] loss: 0.003
[ 33 / 40 ] loss: 0.010
[ 34 / 40 ] loss: 0.006
[ 35 / 40 ] loss: 0.238
[ 36 / 40 ] loss: 0.010
[ 37 / 40 ] loss: 0.005
[ 38 / 40 ] loss: 0.002
[ 39 / 40 ] loss: 0.003
[ 40 / 40 ] loss: 0.027
0.026230602580471895
Accuracy: 0.741525 -- Precision: 0.742857 -- Recall: 0.806202 -- F1: 0.773234 -- AUC: 0.796820
========= epoch: 49 ==============
/root/miniconda3/envs/evapatch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)
  self.dropout, self.training, self.bidirectional, self.batch_first)
[ 1 / 40 ] loss: 0.012
[ 2 / 40 ] loss: 0.003
[ 3 / 40 ] loss: 0.002
[ 4 / 40 ] loss: 0.003
[ 5 / 40 ] loss: 0.091
[ 6 / 40 ] loss: 0.009
[ 7 / 40 ] loss: 0.002
[ 8 / 40 ] loss: 0.005
[ 9 / 40 ] loss: 0.095
[ 10 / 40 ] loss: 0.004
[ 11 / 40 ] loss: 0.003
[ 12 / 40 ] loss: 0.090
[ 13 / 40 ] loss: 0.011
[ 14 / 40 ] loss: 0.003
[ 15 / 40 ] loss: 0.005
[ 16 / 40 ] loss: 0.002
[ 17 / 40 ] loss: 0.015
[ 18 / 40 ] loss: 0.003
[ 19 / 40 ] loss: 0.233
[ 20 / 40 ] loss: 0.004
[ 21 / 40 ] loss: 0.031
[ 22 / 40 ] loss: 0.027
[ 23 / 40 ] loss: 0.085
[ 24 / 40 ] loss: 0.019
[ 25 / 40 ] loss: 0.040
[ 26 / 40 ] loss: 0.012
[ 27 / 40 ] loss: 0.002
[ 28 / 40 ] loss: 0.018
[ 29 / 40 ] loss: 0.003
[ 30 / 40 ] loss: 0.002
[ 31 / 40 ] loss: 0.002
[ 32 / 40 ] loss: 0.002
[ 33 / 40 ] loss: 0.003
[ 34 / 40 ] loss: 0.003
[ 35 / 40 ] loss: 0.054
[ 36 / 40 ] loss: 0.002
[ 37 / 40 ] loss: 0.032
[ 38 / 40 ] loss: 0.063
[ 39 / 40 ] loss: 0.056
[ 40 / 40 ] loss: 0.032
0.026999555635848084
Accuracy: 0.750000 -- Precision: 0.739726 -- Recall: 0.837209 -- F1: 0.785455 -- AUC: 0.799754
0.8061546163198169 0.8248505568048933 0.8256171735241502 0.8246270863755233 0.8356264803080713
[113, 82, 39, 57, 189, 58, 178, 218, 172, 152, 66, 188, 185, 181, 195, 51, 91, 146, 40, 215, 63, 72, 119, 184, 101, 194, 74, 23, 227, 201, 200, 60, 95, 233, 192, 123, 202, 237, 186, 31, 7, 18, 187, 171, 132, 204, 214, 125, 45, 16, 22, 124, 109, 34, 232, 41, 169, 167, 225, 234, 166, 55, 128, 61, 32, 68, 219, 2, 149, 76, 207, 35, 43, 209, 62, 158, 196, 73, 168, 11, 230, 115, 93, 129, 59, 220, 154, 121, 142, 208, 231, 116, 131, 42, 147, 37, 136, 87, 191, 112, 213, 173, 175, 150, 10, 139, 221, 75, 183, 15, 199, 24, 193, 143, 47, 176, 103, 38, 54, 81, 228, 56, 9, 8, 97, 96, 78, 27, 25, 85, 212, 50, 44, 110, 30, 71, 177, 14, 159, 84, 86, 46, 65, 100, 216, 26, 190, 67, 236, 134, 52, 117, 89, 222, 211, 210, 144, 137, 153, 94, 6, 140, 79, 223, 48, 12, 205, 120, 53, 17, 226, 19, 170, 36, 155, 174, 164, 3, 151, 69, 217, 180, 88, 13, 92, 99, 165, 163, 162, 64, 138, 28, 29, 98, 203, 130, 77, 104, 229, 107, 108, 21, 197, 156, 157, 114, 126, 127, 161, 179, 118, 224, 141, 105, 90, 70, 33, 182, 5, 106, 83, 133, 235, 20, 206, 135, 122, 145, 80, 198, 49, 148, 102, 111, 4, 160, 1, 156, 127, 91, 61, 15, 124, 44, 130, 216, 235, 170, 66, 53, 67, 12, 14, 154, 196, 126, 176, 168, 76, 54, 88, 129, 201, 166, 94, 161, 200, 158, 78, 199, 211, 81, 29, 73, 184, 237, 26, 179, 45, 32, 134, 116, 75, 164, 20, 56, 24, 84, 8, 132, 183, 59, 57, 58, 202, 1, 192, 46, 42, 121, 34, 206, 217, 48, 155, 207, 13, 223, 18, 185, 140, 23, 107, 37, 138, 52, 172, 30, 106, 22, 227, 149, 47, 82, 125, 112, 114, 95, 188, 175, 115, 69, 16, 70, 25, 225, 177, 79, 195, 74, 109, 146, 186, 232, 43, 137, 80, 77, 3, 27, 105, 83, 60, 190, 220, 101, 10, 113, 210, 40, 122, 193, 39, 141, 108, 98, 221, 49, 197, 171, 150, 87, 119, 50, 62, 139, 99, 167, 93, 21, 209, 55, 173, 65, 38, 194, 204, 2, 189, 41, 102, 152, 36, 213, 131, 203, 228, 162, 198, 9, 143, 218, 11, 157, 219, 144, 230, 234, 224, 111, 96, 5, 120, 85, 51, 71, 136, 180, 117, 169, 229, 7, 208, 222, 19, 214, 226, 178, 147, 103, 133, 165, 236, 31, 187, 4, 104, 72, 148, 68, 215, 63, 35, 151, 191, 182, 110, 174, 145, 64, 89, 123, 142, 17, 90, 100, 233, 6, 159, 135, 231, 160, 128, 28, 212, 97, 118, 163, 153, 92, 181, 205, 33, 86, 147, 229, 219, 41, 92, 34, 134, 45, 200, 225, 195, 85, 210, 173, 8, 136, 54, 207, 35, 95, 108, 230, 90, 149, 227, 53, 129, 56, 26, 181, 28, 148, 143, 197, 20, 228, 166, 86, 9, 25, 224, 128, 96, 3, 87, 170, 161, 97, 174, 47, 142, 124, 62, 160, 198, 183, 145, 205, 138, 126, 152, 27, 164, 6, 191, 75, 12, 139, 175, 55, 84, 21, 201, 153, 22, 235, 192, 18, 154, 119, 93, 60, 51, 67, 237, 33, 24, 44, 104, 232, 151, 186, 133, 63, 109, 68, 176, 52, 13, 32, 203, 30, 71, 19, 169, 162, 80, 77, 39, 46, 188, 231, 66, 144, 167, 215, 59, 172, 193, 211, 209, 1, 40, 4, 184, 132, 146, 36, 204, 218, 212, 165, 31, 206, 48, 117, 214, 69, 82, 15, 141, 116, 114, 49, 23, 118, 182, 178, 81, 106, 185, 2, 158, 123, 159, 196, 64, 43, 157, 233, 125, 131, 220, 111, 213, 38, 208, 107, 190, 177, 171, 199, 105, 103, 127, 5, 102, 101, 155, 121, 29, 10, 76, 236, 83, 226, 78, 58, 217, 11, 57, 88, 194, 98, 50, 113, 137, 61, 110, 17, 74, 120, 202, 221, 7, 79, 65, 89, 222, 112, 150, 91, 168, 100, 156, 180, 14, 16, 99, 70, 73, 130, 42, 135, 122, 140, 216, 72, 189, 234, 223, 94, 179, 37, 187, 115, 163, 199, 113, 231, 51, 109, 187, 232, 98, 208, 163, 198, 105, 13, 45, 116, 128, 190, 3, 117, 92, 82, 209, 115, 218, 58, 143, 213, 188, 183, 130, 123, 55, 147, 205, 118, 154, 204, 67, 77, 216, 185, 156, 68, 146, 2, 110, 155, 132, 167, 40, 181, 223, 235, 236, 150, 191, 162, 49, 229, 65, 189, 101, 157, 212, 148, 220, 122, 7, 86, 12, 186, 170, 182, 230, 194, 57, 94, 184, 211, 19, 164, 106, 151, 121, 95, 207, 90, 100, 25, 14, 62, 50, 174, 16, 165, 73, 54, 41, 166, 234, 168, 176, 124, 9, 26, 203, 15, 206, 159, 120, 233, 224, 66, 192, 63, 17, 80, 85, 145, 175, 36, 141, 71, 125, 43, 52, 20, 46, 129, 81, 178, 22, 201, 72, 152, 134, 219, 87, 112, 227, 99, 138, 48, 38, 75, 221, 172, 177, 4, 210, 64, 215, 88, 47, 173, 21, 217, 74, 200, 37, 93, 1, 89, 214, 228, 222, 135, 61, 70, 226, 127, 29, 31, 179, 39, 53, 69, 136, 11, 153, 171, 102, 27, 79, 225, 202, 139, 197, 195, 103, 142, 35, 6, 28, 59, 83, 97, 18, 193, 30, 96, 23, 180, 111, 160, 119, 114, 42, 149, 44, 8, 10, 32, 137, 158, 76, 133, 169, 104, 91, 34, 78, 33, 24, 56, 161, 140, 5, 144, 131, 84, 107, 60, 196, 126, 108, 53, 76, 116, 166, 147, 228, 162, 118, 151, 113, 120, 78, 15, 173, 72, 102, 24, 119, 214, 54, 172, 8, 153, 70, 127, 10, 31, 158, 85, 68, 143, 229, 170, 137, 161, 106, 164, 193, 167, 62, 92, 69, 96, 6, 33, 21, 136, 135, 105, 61, 134, 71, 146, 221, 80, 91, 74, 219, 75, 25, 225, 125, 81, 163, 199, 109, 34, 114, 123, 97, 203, 129, 121, 101, 12, 35, 191, 2, 148, 55, 236, 201, 156, 22, 19, 32, 115, 36, 217, 126, 132, 41, 157, 174, 178, 133, 95, 98, 79, 168, 194, 57, 139, 67, 171, 222, 226, 52, 28, 100, 122, 183, 49, 131, 124, 7, 108, 218, 182, 200, 38, 216, 51, 231, 155, 17, 16, 46, 84, 60, 196, 187, 205, 220, 149, 90, 77, 234, 64, 177, 223, 227, 189, 103, 50, 179, 160, 104, 138, 190, 154, 63, 11, 110, 128, 211, 180, 198, 44, 86, 192, 209, 188, 195, 66, 232, 88, 47, 40, 42, 93, 175, 29, 58, 26, 145, 13, 111, 65, 89, 43, 23, 202, 230, 224, 152, 5, 107, 14, 186, 206, 56, 144, 83, 37, 159, 142, 212, 204, 233, 141, 45, 150, 185, 18, 140, 73, 4, 82, 39, 30, 208, 27, 197, 213, 3, 112, 48, 59, 207, 176, 9, 87, 117, 20, 184, 94, 210, 181, 99, 1, 165, 215, 130, 169, 235]
[1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]
[1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]
